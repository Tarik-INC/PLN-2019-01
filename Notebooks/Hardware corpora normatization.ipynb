{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/corpora/hardware/https:--www_techspot_com-reviews-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1799-geforce-gtx-1660-mega-benchmark-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1823-fortnite-battle-royale-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1789-amd-radeon-vii-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1813-geforce-gtx-1660-mega-benchmark-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1744-core-i9-9900k-round-two-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1815-msi-gs75-stealth-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1776-das-keyboard-4q-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1808-geforce-gtx-1660-ti-vs-rtx-2060-vs-gtx-980-ti-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1790-viotek-gft27db-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1816-asrock-deskmini-a300-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1812-geforce-rtx-2080-max-q-laptop-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1804-geforce-rtx-2060-laptop-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1782-intel-whiskey-lake-core-i7-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1800-anthem-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1730-intel-core-i9-9900k-core-i7-9700k-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1730-intel-core-i9-9900k-core-i7-9700k-page5_html.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1701-geforce-rtx-2080-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1781-geforce-rtx-2060-mega-benchmark-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1817-asus-zenbook-pro-14-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1791-amd-radeon-vii-mega-benchmark-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1797-nvidia-geforce-gtx-1060-ti-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1819-geforce-gtx-1660-vs-rtx-2060-vs-gtx-960-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1730-intel-core-i9-9900k-core-i7-9700k-page2_html.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1822-geforce-gtx-970-vs-radeon-r9-290-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1783-razer-blade-stealth-2019-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1730-intel-core-i9-9900k-core-i7-9700k-page3_html.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1780-geforce-1060-vs-radeon-580-vs-radeon-570-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1806-apex-legends-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1820-alienware-m15-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1777-geforce-1050ti-vs-radeon-570-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1730-intel-core-i9-9900k-core-i7-9700k-page6_html.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1818-sekiro-shadows-die-twice-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1796-best-rtx-2060-graphics-cards-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1807-asus-rog-strix-scar2-rtx-laptop-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1795-metro-exodus-benchmarks-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1825-ryzen-2600x-vs-1700-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1811-nvidia-geforce-gtx-1060-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1798-gigabyte-aero-15-x9-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1784-resident-evil-2-benchmarks-.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1730-intel-core-i9-9900k-core-i7-9700k-page4_html.txt', '../Data/corpora/hardware/https:--www_techspot_com-review-1792-geforce-rtx-2070-max-q-laptop-.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "corpora_path = '../Data/corpora/'\n",
    "corpora_dirs = os.listdir(corpora_path)\n",
    "corpora_dirs = [d for d in corpora_dirs if d not in '.DS_Store']\n",
    "# print(corpora_dirs)\n",
    "\n",
    "corpora = {}\n",
    "for corpus in corpora_dirs:\n",
    "    if corpus != 'CSTNews 6.0':\n",
    "        files = [os.path.join(corpora_path + corpus, f) \n",
    "                for f in os.listdir(corpora_path + corpus) \n",
    "                if os.path.isfile(os.path.join(corpora_path + corpus, f))]\n",
    "        if corpus == 'hardware':\n",
    "            print(files)\n",
    "        corpora[corpus] = {'raw': [], 'tokenized': [], 'words': []}\n",
    "        for file in files:\n",
    "            with open(file, 'r', encoding='iso-8859-1') as txt_file:\n",
    "                lines = txt_file.readlines()\n",
    "                corpora[corpus]['raw'].append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': [['Latest ReviewsWhen we recently updated our Best CPUs feature, we noticed that access to affordable first-gen Ryzen processors remains an attractive option for many. The Ryzen 7 1700 is a standout option for an 8-core/16-thread part selling for $160, meaning you can either buy the R7 1700 or the R5 2600 at the same price.It\\'s hard to say how Fortnite is turning out because it keeps turning into something else. Skins and storylines come and go. Landmarks appear and disappear. Weapons are added and removed. Recently, a mysterious excavation site appeared, but it was dug up and abandoned by the time I got there the next day. There\\'s never a perfect time to say what Fortnite is.Recently we\\'ve looked back at the GeForce GTX 980 Ti and the GTX 960, both popular GPUs from yesteryear. Those features have been warmly welcomed, but besides the overall positive responses what we noticed in common in your feedback was the request to test the GeForce GTX 970, which was the performance/value offering of the time and a GPU some of you are still rocking in today\\'s games with some success.When we recently tested the new GeForce GTX 1660 we noted that Nvidia was making a bold claim in the review guide saying that the 1660 was a whopping 113% faster than the GTX 960, making it a perfect upgrade option for owners of the old mid-range Maxwell GPU.Today we\\'re taking a look at another gaming laptop, the brand new Alienware m15, which is the company\\'s portable laptop offering to rival systems like the MSI GS65 Stealth, Gigabyte Aero 15 and Razer Blade. Historically we haven\\'t been huge fans of Alienware\\'s chunkier laptop designs, but the m15 is one of the best we\\'ve seen from the company so far.In Sekiro: Shadows Die Twice, the thread between life and death is tenuous. As the One-Armed Wolf, a loyal shinobi seeking to save a young noble with a cursed bloodline, you traverse a feudal Japan so saturated with the remnants of war that the idea of mortality becomes fickle.To say there are a ton of laptops out there is an understatement. As is the case with smartphones, within a given segment or budget often there isn\\'t much that differentiates them---other than components and small design changes. But with Asus\\' latest ZenBook Pro, there\\'s one element that really makes it stand out from the crowd: a touchpad that\\'s also a touchscreen.The Asrock DeskMini A300 is a tiny PC that takes advantage of Ryzen processors. Almost every custom designed mini PC that we\\'ve seen to date has used Intel inside and while Intel CPUs are very good, they aren\\'t the best choice for this kind of system. At least if you want to game or do any kind of 3D work, for that AMD\\'s Ryzen APUs are unrivaled.This is our second look at the new GeForce GTX 1660. Not to be confused with the 1660 Ti that was released a month earlier, both GPUs offer great value at mid-range prices of $220 for the GTX 1660 and $280 for the Ti version.The latest member of the Turing GTX family is making its debut in the form of the Nvidia GeForce GTX 1660. An anticipated release after the launch of the GTX 1660 Ti, which proved to be a great buy, now the vanilla GTX 1660 has been set at $220 and it looks to provide great value for your money.Today we\\'re revisiting the GeForce GTX 980 Ti to see how it stacks up to the newly released RTX 2060 and GTX 1660 Ti, particularly in more recent titles such as Apex Legends, Resident Evil 2 and Far Cry New Dawn. The GTX 980 Ti is now four years old, so you\\'d expect new GPUs around half the price to deliver a similar level of performance... or do they?The Asus ROG Strix Scar II GL504GV is a fine laptop with a good design, a good trackpad and keyboard, and generous I/O. It\\'s not overly portable, it\\'s not overly chunky. It\\'s got a pretty good display, upgradeability options, and the internal hardware and performance is good, too.Apex Legends is a tasting menu of battle royale moments, rather than the potato chip jump-die-restart of my Fortnite experiences. There are countless moments to surprise or disappoint yourself. The hero aspect of the game is a change for battle royales, but other than that, the basics are standard for the genre.Every once in a while, a game comes along that that does something surprising, different, memorable. Anthem is not one of those games. Anthem\\'s core idea of \"jetpacks plus guns\" works excellently on its own, but nothing else in the game quite lives up to it.We\\'re following up to our GeForce GTX 1660 Ti review with an even more ample 33 game benchmark test. The day-one review looked at more recent games such as Resident Evil 2, Metro Exodus, Apex Legends, and many others. Now we\\'re keen to see how the 1660 Ti stacks up in a much wider range of games.Today we are reviewing the Gigabyte Aero 15 X9, the first Nvidia RTX laptop we tested and used for our RTX 2070 Max-Q feature earlier this month. It\\'s a cool gaming laptop, pretty similar to the Aero 15X v8 we looked at last year, but with a few upgrades that we\\'ll walk you through here.Nvidia\\'s latest attempt to excite gamers arrives in the form of a new mid-range GPU with no RTX features on board. The new GeForce GTX 1660 Ti comes as no surprise as it\\'s been widely rumored for some time: a cut down Turing GPU that trims off the fat and offers better value for less than $300.The GeForce RTX 2060 offers a middle ground for great performance at a price that remains within reach of most enthusiasts. If you\\'re convinced this is the best GPU for you, you will still have to choose among dozens of different models. Here come our picks.Metro Exodus is about to receive our usual GPU benchmark treatment with 36 graphics cards on the bench getting a full performance comparison. The game has generally been well received although it\\'s been out for just a few days.Nvidia unveiled RTX laptops early 2019 and the first models sporting the GPUs are making it to market just now. Today we\\'re checking out Nvidia\\'s new GeForce RTX GPUs for laptops, starting with the RTX 2070 Max-Q variant, thanks to Gigabyte who sent in their latest Aero 15 X9 for testing.Following up to our best value FreeSync monitors feature, we though this review would be ideal. Today we\\'re looking at the Viotek GFT27DB which is a brand new display release, sporting a new TN panel (ideal for gamers) that promises a lot on paper. Having reviewed several Viotek displays in the last year, we\\'ve been consistently impressed with the value proposition, so we are hoping nothing changes here in that respect.The Radeon VII is AMD\\'s latest GPU offering aimed at gamers. CEO Lisa Su has stated that the Radeon VII performs competitively with Nvidia\\'s RTX 2080 and as such will go for the same $700. It\\'s believed AMD\\'s not turning a profit on these things, even at this price. But is it any good?It\\'s time for another mega benchmark and the subject of today\\'s GPU onslaught is Resident Evil 2. A classic survival horror game developed and published by Capcom that it\\'s also a remake of the original Resident Evil 2 released for the PlayStation way back in 1998.The new Razer Blade Stealth uses an all-new design with new hardware. Powered by a Core i7-8565U processor, the 13.3\" ultraportable offers models with and without discrete graphics, 8 or 16GB of RAM and two performance levels of 256GB SSDs running on a 53 Wh battery.Intel unveiled a new series of U-series laptop processors last year designed for ultraportables. These CPUs are codenamed Whiskey Lake, and they\\'re still 8th-gen parts that are not radically different from Kaby Lake Refresh. The focus of this review will be the Core i7-8565U, which is essentially the new flagship 15W CPU in Intel\\'s line-up.The Das Keyboard 4Q, as the name implies, combines the proven hardware design of the Das Keyboard 4 we know and love, while adding the smarts of the 5Q as well as per-key RGB backlighting. Are the new IoT features useful enough to make the 4Q a better buy than the 5Q or even the standard Das 4? Let\\'s find out.'],\n",
       "  [\"As we had anticipated, we are following up to our GeForce GTX 1660 Ti review with an even more ample 33 game benchmark test. The day-one review looked at a dozen of the more recently released games such as Resident Evil 2, Metro Exodus, Apex Legends, and many others.We were keen to see how the 1660 Ti stacked up in a much wider range of games, so today weâ\\x80\\x99re doing that. The performance summary and breakdown section includes the 12 games we already looked at along with the new 21 titles, and for 12 of those 21 titles weâ\\x80\\x99ll analyze and comment the results more closely below.BenchmarksFirst up we have Assassin's Creed: Odyssey and despite being an AMD sponsored title it does prefer Nvidia hardware and we see that here as the GTX 1660 Ti is able to match Vega 64. It was also 42% faster than the RX 590. When compared to other GeForce parts it matched the GTX 1070 and was just 11% slower than the RTX 2060.Next up we have Strange Brigade, another AMD sponsored title. This time it plays nice with AMD hardware which makes better sense. Interestingly, whereas the RTX 2070 is a good bit faster than the GTX 1080, the GTX 1660 Ti is actually a little slower than the GTX 1070. Still overall performance at 1440p was good and the new affordable Turing GPU also edged out the RX 590.In Star Wars Battlefront II the GTX 1660 Ti demonstrates how itâ\\x80\\x99s clearly a class ahead of the GTX 1060 6GB and RX 590, offering over 40% more performance. In this title weâ\\x80\\x99re looking at GTX 1070 or Vega 56-like performance with a healthy 68 fps on average.DiRT 4 using CMAA is an AMD special, but even so the GTX 1660 Ti was able to match the RX 590, providing the same experience. The 1660 Ti also roughly matched the GTX 1070 Ti and was just 5% slower than the GTX 1080.Next we have For Honor, here we see the GTX 1660 Ti creeping in just behind the GTX 1070, which meant it was also a few frames down on Vega 56. Even so it was still almost 30% faster than the RX 590 and GTX 1060 6GB.Sniper Elite 4 is one of the few games that supports DirectX 12 and actually runs better using DX12. That said, the GTX 1660 Ti did trail the GTX 1070 by a small margin in this one. It was also 12% slower than Vega 56. While not a bad result, it wasn't the card's best showing either.In The Witcher 3 the 1660 Ti matches the GTX 1070 as it often does. The GPU also matched Vega 56 and was 26% faster than the Radeon RX 590, but most crucially with the game's visual settings just about maxed out it provided a very playable 54 fps on average at 1440p. This enabled a much better experience than either the RX 590 or 6GB GTX 1060.F1 fans will appreciate what the GTX 1660 Ti has to offer for less than $300. Youâ\\x80\\x99re looking at performance somewhere between a GTX 1070 and 1070 Ti. The Radeon GPUs perform very well in this title, too, but even so the 1660 Ti dusted the RX 590 by a 23% margin.Testing with Warhammer Vermintide 2 sees the GTX 1660 Ti match Vega 56, and please note we're using the DirectX 11 API for testing. A few Vega users complained that we were favoring Nvidia by using DX11, thatâ\\x80\\x99s not the intention but last we checked DX11 offered better frame time performance. We plan to retest these GPUs with DX12 just to make sure. Plus, in case those Vega owners didn't get the memo, Turing is pretty handy in DX12 and Vulkan titles now anyway.Performance in Prey is a little down on what you might be expecting from the GTX 1660 Ti, though with 86 fps on average it was still well ahead of the RX 590 and GTX 1060 6GB. The 1% low result was very solid, too, though it lagged behind the 1070 by a 9% margin which was a little surprising.Kingdom Come Deliverance has the 1660 Ti sitting between the 1070 and 1070 Ti, making this new mid-range GPU ~14-15% slower than the more expensive RTX 2060 and Vega 56. It also wasnâ\\x80\\x99t that much faster than the RX 590... well, it was 22% faster, but given weâ\\x80\\x99ve seen it up around 40% at times, a 22% margin looks modest in this instance.Weâ\\x80\\x99ve already seen how Warframe runs better with Pascal GPUs. For example, the GTX 1080 is 8% faster than the RTX 2070. Here the GTX 1070 was 16% faster than the 1660 Ti, in fact, the new budget Turing offering was only 9% faster than the GTX 1060 6GB, so a fairly disappointing result to end on for fans of Warframe.Performance SummaryGTX 1660 Ti vs. GTX 1060In our day-one review the GTX 1660 Ti came out 40% faster on average when compared to the GTX 1060 6GB at 1440p. That figure has dropped quite a bit in the 33 game comparison, now the 1660 Ti is just 34% faster, though still a decent margin.The reason for this is down to the more limited game selection that purely by chance were more favorable to the Turing GPU. The focus was on new titles such as Shadow of the Tomb Raider, Apex Legends, Resident Evil 2 and Hitman 2, all of which heavily favored the 1660 Ti.However now we have included games such as Warframe, Shadow of War and half a dozen other titles that saw the 1660 Ti beat the 1060 by less than 30%, bringing that average down and making the GTX 1660 Ti 34% faster on average against the GTX 1060 6G, which in itself is not a bad result given it costs just 12% more.GTX 1660 Ti vs. GTX 1070In our day-one coverage the 1660 Ti was ~2% faster on average against the GTX 1070 and because of that we noted performance to be basically the same overall. Now in our 33 game test the 1660 Ti came out exactly level with the 1070. This graph gives us a good idea of where Turing is more efficient than Pascal and, of course, Turing is faster under all conditions. Itâ\\x80\\x99s just that in this case weâ\\x80\\x99re pitting a 1920 CUDA core Pascal part against a 1536 CUDA core Turing part.GTX 1660 Ti vs. RTX 2060For the RTX 2060 comparison we see similar margins to our original review. Previously the 1660 Ti was 12% slower, now with 33 games that margin is increased ever so slightly to 14%. Weâ\\x80\\x99ve got to retest Forza Horizon 4 to try and work out whatâ\\x80\\x99s going on there, but other than that the only other major outliers include Warframe, Metro Exodus and World of Tanks.GTX 1660 Ti vs. Radeon RX 590The performance margin remains the same for the Radeon RX 590 comparison.\\n\",\n",
       "   \"The 1660 Ti is 24% faster here.GTX 1660 Ti vs. Vega 56When put against the Radeon Vega 56, the GTX 1660 Ti is 8% slower on average, though with more titles at hand we see a few examples where the GTX 1660 Ti was significantly slower: Warframe, Strange Brigade, Battlefield V, DiRT 4 and Resident Evil 2, for example. Meanwhile, the GTX 1660 Ti was slightly faster or a match for Vega 56 in Assassinâ\\x80\\x99s Creed Odyssey, Fortnite, Vermintide 2 and Apex Legends.Cost per Frame and Closing ThoughtsA quick update to our cost per frame graph. If you read our GTX 1660 Ti review from days prior, the change here is very small at ~2-4 fps for the most part. Only the Radeon VII saw a significant change in the 33 game sample, and here it stacks up much better than on the previous 12-game comparison. But for the purpose of this article, weâ\\x80\\x99re not interested in Radeon VII, rather Vega 56, RX 590 and RX 580 play more consequential roles in this comparison.Overall little has changed... the GTX 1660 Ti is only 9% more expensive than the RX 580 per frame, but 12% cheaper than the RX 590 and 22% cheaper than Vega 56, at the more typical $400 asking price. Then compared to the RTX 2060, the cost per frame of the 1660 Ti is improved by 7%, so a solid result for the new GTX graphics card.Thatâ\\x80\\x99s how the GeForce GTX 1660 Ti stacks up in a wide range of games and it was interesting to see where it offered big gains over GPUs such as the GTX 1060 6GB, and then others where the gains were less impressive.Although the GTX 1660 Ti looks less appealing for GTX 1060 6GB owners in this massive benchmark comparison, a 34% performance boost on average is still nothing to scoff at. If we were currently gaming with a GTX 1060 and were after something with a bit more punch, upgrading to a GTX 1660 Ti is worthy of consideration. The new GPU does offer next tier performance for a small price premium. Of course, if you have anything faster than a GTX 1060 or RX 580, such as Vega 56 or the GTX 1070, thereâ\\x80\\x99s still no worthwhile upgrade choice if you want best value for your money.For those coming from much older mid-range GPUs, your top choices are either the GTX 1660 Ti or Vega 56. The RX 590 simply isnâ\\x80\\x99t fast enough for the price, though we are finally starting to see some changes with some models dropping as low as $240, which does nudge closer in terms of cost per frame. Still, for that level of performance youâ\\x80\\x99re usually better off with the Radeon RX 580. We donâ\\x80\\x99t expect a true GTX 1660 Ti competitor from AMD until Navi drops.There are currently a number of 1660 Ti graphics cards available at the $280 MSRP.Zotac with their Gaming model, MSI has the Zentus, EVGA has the XC Black, Gigabyte is offering a base model OC card as well as a Mini-ITX version. So it'd appear getting a decent model for the MSRP is not an issue. We really like the look of the MSI Ventus and weâ\\x80\\x99ll try to get our hands on one very soon. As it stands today, the GTX 1660 Ti has no competition for best value and overall package for the price.\"],\n",
       "  [\"Itâ\\x80\\x99s hard to say how Fortnite is turning out because it keeps turning into something else. Skins and storylines come and go. Landmarks appear and disappear. Weapons are added and removed. Recently, a mysterious excavation site appeared, but it was dug up and abandoned by the time I got there the next day.As Iâ\\x80\\x99m writing this, players are angry about a gameplay change that will probably be replaced by something new to hate a month from now. Thereâ\\x80\\x99s never a perfect time to say what Fortnite is.But hereâ\\x80\\x99s what it is:My colleague Paul took this photo last week while out in New York City, where we live. Fortnite has nothing to do with New York, and New York has nothing to do with Fortnite, though that may change when the gameâ\\x80\\x99s $30 million World Cup descends on us in July. I laughed at this photo for a long time. â\\x80\\x9cYeah, OK,â\\x80\\x9d I said. â\\x80\\x9cThis makes sense.â\\x80\\x9dFortnite: Battle Royale launched in early access on September 26, 2017, a spin-off from developer Epicâ\\x80\\x99s co-op building game Fortnite: Save the World. More than a year and a half later, itâ\\x80\\x99s an unfinished game thatâ\\x80\\x99s nevertheless garnered 250 million players and made an estimated billions of dollars. Itâ\\x80\\x99s free to play on consoles, PC, and mobile, making it easy for basically anyone to access. Itâ\\x80\\x99s an unfinished game thatâ\\x80\\x99s ubiquitous, not just to those of us who play video games, but to parents and teachers and athletes and musicians. Itâ\\x80\\x99s seeped into our lives through Halloween costumes and contentious dances. Itâ\\x80\\x99s arguably inspired more games to add battle passes, a monetization strategy thatâ\\x80\\x99s overtaking increasingly gauche loot boxes. Its success helped launch the Epic Games Store, challenging Valveâ\\x80\\x99s long-standing dominance of the PC marketplace. It means so many things, is in so many places, that itâ\\x80\\x99s ended up on this nonsensical sweatshirt.There are tourists who will see this sweatshirt as a good gift for someone back home whoâ\\x80\\x99s mentioned Fortnite at some point, because someone probably has. Millions of people could be given this sweatshirt, and it would mean something, whether as an ironic â\\x80\\x9clook how ridiculous this isâ\\x80\\x9d or an earnestly special gift.Both the irony and the earnestness are right at home in Fortnite. Almost everyone who plays it can find something to like, whether thatâ\\x80\\x99s a cute skin, a cool thing they built, or a free way to hang out with their friends. They can also find a bunch of stuff they donâ\\x80\\x99t like: annoying or hateful teammates, an overpowered sword, or a constantly changing competitive scene. The game is stuffed with stuff, as full of ridiculous nonsense as it is real moments of victory, excitement, and connection.In some ways, Fortnite is a lot like any other battle royale. One hundred playersâ\\x80\\x94alone, in duos, or in squads of fourâ\\x80\\x94leap from a flying bus onto an island. The mapâ\\x80\\x99s usable area shrinks as a brightly colored storm, which damages players, closes in. Players scavenge for guns, ammo, explosives, and shield potions, which they use to destroy each other or outlive destruction. Only one team can win.Weapons are divided into rarities, depicted by color, that dictate their power. Thereâ\\x80\\x99s none of PUBGâ\\x80\\x99s or Apex Legendsâ\\x80\\x99 attachments; that gun you picked up is as good as itâ\\x80\\x99s going to get. This absence makes gearing up for the fight tighter than in other battle royales, though the actual gunplay can feel floaty and imprecise. Apart from guns, there are ridiculous weapons that come and go regularly: grenades that make enemies dance or turn their feet to blocks of ice, launchers that fire rockets you can ride on, boomboxes that destroy buildings. Your favorite strategy one week might be useless the next. But for players like me who donâ\\x80\\x99t enjoy forming opinions on a gameâ\\x80\\x99s best gun, Fortniteâ\\x80\\x99s copious items give me fun or silly things to do that can still contribute to winning a match. The more frivolous stuff, when dropped into Fortniteâ\\x80\\x99s bright, gore-free world, gives the game a lighthearted vibe that papers over the disturbing brutality of its core dictate, which is to ruthlessly murder everyone you meet.Sometimes these items are sequestered into their own modes, which also change regularly. There are 50-player team battles, modes where you have to capture a dance floor or defend a food mascot, and modes where lava gradually fills the map. Sometimes there are events: a rocket launch, a cube exploding, a concert. These modes and events, like the different items, give players a change of pace. They can encourage working together, trying out new tactics, or just doing something absurd.But what really separates Fortnite from other murder-to-win games is the unique building mechanic. Using your starting weapon, a pickaxe, you can harvest almost all of the game worldâ\\x80\\x99s features for materials. Trees and fences will get you wood, walls and rocks will get you stone, and staircases and cars will get you metal. Even if you canâ\\x80\\x99t find a weapon, you always have something. You might not be able to outshoot another player, but you can outbuild them. You can knock down a skyscraper and create your own in its place. You can turn an empty field into a maze of ramps and walls and traps.In other battle royales, you can tell a place has already been looted by open doors or empty chests. In Fortnite, a visited place is a frantic wreckage of destroyed infrastructure, of ramps snaking up hillsides, of zigzagging towers flecked with loot that a downed playerâ\\x80\\x99s killers didn't want. The architecture of desire built over a placeâ\\x80\\x99s original design tells a story of what might have happened: the exact path someone took, the ebb and flow of a build fight. Players take the space Fortniteâ\\x80\\x99s developers created and change it, make it their own. Then itâ\\x80\\x99s destroyed, or the match ends, and everyone starts over again.Recently, I jumped into Fortniteâ\\x80\\x99s Playground mode, which lets you explore the map or practice skills without the pressure of an actual match. I walked the entirety of the map, or at least the current oneâ\\x80\\x94places and landscapes get renamed or replaced often. Iâ\\x80\\x99d never spent much time considering how the security cameras at Snobby Shores make the town feel like a home for the rich and paranoid, or taken the time to admire the charming â\\x80\\x9850s nostalgia of Paradise Palms. Fortniteâ\\x80\\x99s world is lovely when you aren't getting shot at or desperately hunting for loot.But it didn't feel right. Nothing seemed to be where I thought it was, and it wasn't just due to map changes. Iâ\\x80\\x99d pictured Lonely Lodge less to the east, Pleasant Park closer to the center. A lot of my sense of the map was created by where the battle bus cuts across at the start of each multiplayer round. Learning the map and picking the right landing spot is part of the gameâ\\x80\\x99s strategy, but I hadn't realized how situational my sense of Fortniteâ\\x80\\x99s world really was until I traversed it outside of a match. Without the actions of 99 other players directing my movements, the gameâ\\x80\\x99s world just wasn't the same.In his 1979 article â\\x80\\x9cSpace and Place: Humanistic Perspective,â\\x80\\x9d geographer Yi-Fu Tuan writes about the ways in which humans give a space character and qualities through being in and interacting with them:A place that evokes affection has personality in the same sense that an old raincoat can be said to have character. The character of the raincoat is imparted by the person who wears it and grows fond of itâ\\x80¦People demonstrate their sense of place when they apply their moral and aesthetic discernment to sites and locations.When I picture Fortnite, my places aren't the named landmarks. Theyâ\\x80\\x99re an empty stretch where I got ambushed by another squad, a tree I fell out of to my death when I forgot Epic had removed glider redeploy, the shack I ducked into to heal when I just managed to outrun the storm. Tuan calls such random places â\\x80\\x9cfields of care,â\\x80\\x9d places that are created by personal knowledge rather than universal or prescribed importance. (Think your local bar versus Stonehenge.) He writes that â\\x80\\x9cfields of care...lack visual identity. Outsiders find [them] difficult to recognize and delimit.â\\x80\\x9dFortniteâ\\x80\\x99s map doesnâ\\x80\\x99t really stand out from other battle royales. Cute names aside, it has a lot of empty space that can make parts of the game feel tedious. But the game excels at giving players fields of care. The building mechanic lets you make a space your own, if even just for a match. Challengesâ\\x80\\x94changing tasks that players complete to level up their battle passâ\\x80\\x94make areas weâ\\x80\\x99d never go to memorable. The Block area of the map hosts a changing roster of player creations, all of which mean something unique to the creator and their friends. The gameâ\\x80\\x99s massive player base creates a dizzying range of possibilities for its world. Its unique space is created not just by so many people having their own experiences there, but somehow forging group ones, like landing at Tilted to prove yourself or thanking the bus driver when you jump off the battle bus. All of its colorful locales would be graveyards, all of its wacky items trash, if there weren't so many people forming the gameâ\\x80\\x99s world alongside you.One afternoon, a small voice began chattering as soon as I loaded into a game. â\\x80\\x9cGuess how old I am!â\\x80\\x9d they kept saying. The third member of our lopsided squad finally guessed, â\\x80\\x9cUmm...13?â\\x80\\x9d in the bright voice of an adult talking to a kid. â\\x80\\x9cSeven!â\\x80\\x9d the kid responded proudly. As we looted our way through the castle at Polar Peak, I admired the careful effort my adult squadmate made to explain what we were doing to a kid who was mostly interested in firing ammo into the air and tumbling off the peak in a hamster ball vehicle.When the storm forced us to move, the adult and I positioned ourselves strategically around the kid, scanning the horizon and trying to keep them from getting too far ahead. We got in a firefight and survived, barely, thanks to the adultâ\\x80\\x99s skill and the kidâ\\x80\\x99s spray-and-pray strategy. Our looting for desperately needed gear got cut short by the storm catching up with us again. We had a long, long run to its new perimeter. â\\x80\\x9cI have a campfire, should I drop it?â\\x80\\x9d the kid kept asking, and my teammate patiently explained why the immobile healing item would be better used later. â\\x80\\x9cI think we can make it!â\\x80\\x9d I shouted, and the adult chuckled incredulously, â\\x80\\x9cReally?â\\x80\\x9d But it kept our spirits up as our health ticked down. We just barely made it alive, only to be promptly decimated by another squad taking advantage of our single-digit health bars. The kid was upset, but the adult praised them up and down, recounting our epic run through the storm and all the fun weâ\\x80\\x99d had. Afterward, the kid sent me a friend request I still havenâ\\x80\\x99t accepted.Fortnite, against my expectations, is full of these weird, positive moments with teammates. There were the players who wanted to conga and, when we got separated, went to conga somewhere else until they both fell off a mountain, narrating the whole thing through their mics so I could picture it from afar. There was the polite kid with a Southern accent who left me to die when I got downed, only to suddenly cry, â\\x80\\x9cOh no, I forgot my teammate!â\\x80\\x9d and try, futilely, to get back to me before I died. More than half my matches were with Spanish speakers, and we tried largely unsuccessfully to communicate with each other, wasting precious seconds fumbling for the right words but mostly laughing through it all. Iâ\\x80\\x99m often matched with kids, like the young teammate I spectated who got all the way to the final two solely by hiding with a steely patience that I, an ex-Zen Buddhist, would have found impossible to muster. I love the sense of possibility I feel any time I jump into a match.In-game, Fortnite is populated by a whole lot of weirdos. Players customize their avatars through the skins and accoutrementsâ\\x80\\x94gliders, backpacks, emotes, and pickaxesâ\\x80\\x94that they buy from the store or earn through ranking up their battle pass. Currently, my avatar is a girl in a cropped hoodie who carries a black-and-white dog in a backpack. When she jumps off the battle bus, she floats to the ground on a palm tree-like umbrella that I earned for winning a match in Season 8. I donâ\\x80\\x99t like the umbrella as much as my previous glider, but I equipped it the moment I got it, because it told other players Iâ\\x80\\x99d won at least one match. It is, to date, the only match I've won this season, but no one needs to know that.The number of ways players can express themselves in Fortnite makes the game feel chaotic and sometimes crowded, but also lively. Your foes can be a phalanx of black-hooded Ice Kings cresting a ridge to murder you or a gaggle of pink panda Cuddle Team Leaders firing themselves at you from cannons. Theyâ\\x80\\x99re a creepy banana. Theyâ\\x80\\x99re maligned soccer players. In past seasons, high-level players would wear the default skins everyone started with to trick others into thinking they were new. In Season 3, a skin of Keanu Reeves as action movie character John Wick (the skin is actually called The Reaper) was the top-tier reward for the battle pass. Iâ\\x80\\x99d actually get scared if I saw one.Never knowing who youâ\\x80\\x99ll encounter cuts both ways. There were adults who berated my every decision. There was a kid who screamed into their mic the whole match, a squadmate who played their television into their mic at ear-splitting volume seemingly on purpose. There were the usual slurs and threats and shitty talk. There were a few teams I quit the moment someone spoke. But the size of the player base and the gameâ\\x80\\x99s popularity mean thereâ\\x80\\x99s always a new squad to join, new people to find. If I donâ\\x80\\x99t like who Iâ\\x80\\x99m matched with, I can find someone new.In almost all of the Fortnite matches I've played, at least one person has left their mic open. Mixed in with the gameâ\\x80\\x99s sounds of explosions and clanging pickaxes are dogs barking, a bird chattering. The rattle of a gunfight is accented with the sizzle of a frying pan in someoneâ\\x80\\x99s kitchen or the sharp tone of a loved one demanding someone stop playing to eat. This background busyness feels right at home with all the chaos and stuff going on inside the game.Itâ\\x80\\x99s this fullness, this being stuffed to the gills with constantly changing stuff, that makes that â\\x80\\x9cFortnite New Yorkâ\\x80\\x9d sweatshirt make sense. Itâ\\x80\\x99s also what makes Fortnite what it is, what keeps people coming back. We want one new game, one new squad, one new experience. Fortnite players loved Kevin the cube, but we wanted to know what he would do next. Season 8â\\x80\\x99s volcano is cool, but is it going to erupt? We want all the stuff, even when we hate it or it makes no sense. There are so many people, so many places, so many things, that Fortnite risks veering off into meaninglessness, into the same cacophony that fills my headphones as I play or the same confusion I felt when Paul sent me the picture of that sweatshirt. But Fortnite wouldn't be Fortnite without it.\"],\n",
       "  [\"The Radeon VII is AMD's latest GPU offering aimed at gamers, but what you may not realize is that Radeon VII is based on an existing compute product that's been re-purposed for gaming and content creation. Based on the Radeon Instinct MI50, the Radeon VII is essentially just gimped Vega 20 silicon, though itâ\\x80\\x99s not gimped in a way thatâ\\x80\\x99s meaningful to gamers.So whatâ\\x80\\x99s been modified then? Double-precision floating-point (FP64) has been disabled, reducing performance from 6.7 TFLOPS down to just 1.7 TFLOPS, but this has no impact on gaming. AMD has also disabled PCIe 4.0, limiting the Radeon VII to the more commonly used PCIe 3.0 interface, which is not an issue either.When Radeon VII was announced at CES a month ago, it wasnâ\\x80\\x99t met with a lot of fanfare. Everyone was hoping for AMDâ\\x80\\x99s new GPU architecture Navi. So a repurposed compute product wasnâ\\x80\\x99t exactly what everyone was clambering up the walls for, and certainly not for $700.Yes, this is a new 7nm graphics card, but itâ\\x80\\x99s based on 5th-gen GCN architecture, the same used by Vega 56 and Vega 64. The move from GlobalFoundries' 14nm process to TSMC's 7nm FinFET process has allowed AMD to shrink the die size by 32% while also packing in 6% more transistors. Technically Radeon VII should be called Vega 60 or something similar given the specs. What we have here is 60 compute units for a total of 3840 stream processors, or 6% fewer cores than Vega 64 even though you can expect it to be faster.The cores are clocked at least 13% higher and we have some serious memory upgrades and Iâ\\x80\\x99m not just talking about the insanely unnecessary (at least for gaming) 16GB VRAM capacity. In addition to having as much memory as your entire system, the Radeon VII also gets a massive 4096-bit wide memory bus, allowing for an insane 1TB/s memory bandwidth, double that of Vega 64.CEO Lisa Su has stated that the Radeon VII performs competitively with Nvidiaâ\\x80\\x99s GeForce RTX 2080 and as such will be available at the same $700 asking price. Itâ\\x80\\x99s believed AMDâ\\x80\\x99s not turning a profit on these things, even at this price, so we guess weâ\\x80\\x99re not going to see them on sale anytime soon.Our test system for this review was comprised of an Intel Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3400 memory. While testing the Radeon VII, we ran into numerous stability issues with the early drivers supplied by AMD and as we pieced this content together, a fix had yet to be issued.Instead of testing 30+ games we're sticking with a dozen titles that accurately represent how the Radeon VII performs across a wide range of titles. Once a stable driver has been provided a mega benchmark session will follow with 2 or 3 dozen games tested.BenchmarksFirst up we have Fortnite and I have to say this is not a good start for Radeon VII as it trails the RTX 2080 by 20%. Thankfully, this isnâ\\x80\\x99t the norm as you're about to see. But we do have a good mix of games that I believe paints a pretty accurate picture of the overall performance.Fortnite is certainly a worst case scenario as the new Radeon GPU was only able to match the RTX 2070, and it was only 22% faster than Vega 64. Itâ\\x80\\x99s also a far cry from the now 2 year old GTX 1080 Ti.Thatâ\\x80\\x99s a bit brutal so letâ\\x80\\x99s move onto some far more promising results.Battlefield V looks much better. The Radeon VII slayed in BF5 using the DirectX 11 API, pumping out an impressive 122 fps on average. The 1% low performance was a little lower than expected but we do see a similar thing with Vega 64 and 56. When compared to Vega 64 we do see a 31% boost to the average frame rate so thatâ\\x80\\x99s quite nice.Here we have another title where the Radeon VII is unable to keep up. I mean, 156 fps in World of Tanks is ample performanceâ\\x80¦ but it also meant the new Radeon GPU was 17% slower than the RTX 2080 and only slightly faster than the RTX 2070. So a poor result overall, though Vega never did well in this title so the result isnâ\\x80\\x99t entirely surprising. Vega 64 only just beats the GTX 1070 after all.A title where the Radeon GPUs shred is Strange Brigade, though like Battlefield V we do see slightly weaker than expected 1% low performance. Itâ\\x80\\x99s not horrible and there was no sign of stuttering but the 1% lows are still low relative to the GeForce GPUs. Still when comparing the average frame rate the Radeon VII was 10% faster than the RTX 2080.Are you starting to notice what weâ\\x80\\x99ve done hereâ\\x80¦?We're giving you bad news followed by some good news, just trying to let you guys down gently is all, I really do care. That said itâ\\x80\\x99s time for some bad news, Monster Hunter: World sees the Radeon VII fall well behind the RTX 2080, here itâ\\x80\\x99s 16% slower and just 7% faster than the RTX 2070, so letâ\\x80\\x99s move along.The Radeon VIIâ\\x80\\x99s back on track for Shadow of the Tomb Raider. Though if you were expecting or at least hoping for an RTX 2080-killer then these results wonâ\\x80\\x99t exactly send tingles through your spine. Itâ\\x80\\x99s a few frames faster on average while a few frames down for the 1% low result, so basically itâ\\x80\\x99s on par with the RTX 2080 in one of 2018â\\x80\\x99s best looking titles.Nvidia Turing architecture is well suited to Rainbow Six Siege and where the Radeon GPUs once enjoyed a strong performance advantage in this title, thatâ\\x80\\x99s no more. Here the Radeon VII struggled to keep pace with the RTX 2080 and in fact was just a whisker ahead of the RTX 2070.Far Cry 5 is a well optimized title and it also happens to be sponsored by AMD. I would say this is a fairly even playing field and as such the Radeon VII and GeForce RTX 2080 are basically neck and neck.Forza Horizon 4 is another well optimized title and while Vega gives the Pascal based GeForce 10 series GPUs a pounding, the new RTX models donâ\\x80\\x99t suffer from the same weakness and the RTX 2080 was just able to outpace the new Radeon VII.Resident Evil 2 is yet another well optimized title that scales well across a wide range of hardware. Here the RTX 2080 and Radeon VII are very evenly matched along with the 2 year old GTX 1080 Ti, another $700 part.We seriously want to drop ARMA 3 from the testing. Iâ\\x80\\x99ve put my foot down and deleted GTA V and how I see it ARMA 3â\\x80\\x99s days are also numbered. Itâ\\x80\\x99s basically a dual-core CPU benchmark, so itâ\\x80\\x99s going to skew the results somewhat due to the CPU bottleneck, but given some games are CPU limited itâ\\x80\\x99s not exactly unrealistic either.The performance in Hitman is even between the GTX 1080 Ti, RTX 2080 and Radeon VII, though this is a title thatâ\\x80\\x99s somewhat CPU limited at 1440p.Power ConsumptionWe all know AMDâ\\x80\\x99s at least a generation behind Nvidia when it comes to efficiency. The Radeon VII is slightly slower than the GTX 1080 Ti, and here we see it consumes a whisker more power that that GPU. Given weâ\\x80\\x99re comparing a two year old product using TSMCâ\\x80\\x99s 16nm process to a brand new product using TSMCâ\\x80\\x99s 7nm process, thatâ\\x80\\x99s pretty depressing.Granted the GTX 1080 Ti is a purpose-built gaming graphics card, but at the end of the day gamers donâ\\x80\\x99t care about that. The RTX 2080 is 65% larger and packs roughly the same amount of transistors, yet reduced total system consumption by 15%, for total system consumption thatâ\\x80\\x99s a significant number.Moving on to operating temperatures, the temperatures alone are okay, comparable to that of the RTX 2080 Founders Edition. The issue is the fan speed required to allow these temperatures...It's worth mentioning that we replaced the Hitachi HM-03 carbon fiber pads with Arcticâ\\x80\\x99s MX-4 thermal paste after tearing the card down in our unboxing a few days ago.Out of the box with an ambient room temp of 21 degrees the Radeon VII hit 74 degrees. However when we went back and re-tested using the thermal paste that temperature under the same conditions dropped to just 69 degrees and the fan speed went unchanged.In other words, the Radeon VII is bloody loud, even when running at just shy of 70 degrees. As soon as the card was placed under any kind of load, the three fans spun up to a vacuum cleaner like 2900 RPM. It was at this point that I realized, yeah this is an AMD reference card, that makes sense. I mean the card looks beautiful, but boy is it hard on the ears. Nvidia Founder Edition cards arenâ\\x80\\x99t exactly quiet, but the fans only spin at 2000 RPM and they are significantly quieter.Performance SummaryBefore we move on, please note we havenâ\\x80\\x99t tested with more games on our day-one review as we're not fully confident in the drivers AMDâ\\x80\\x99s provided. As mentioned earlier, we are finding them unstable and weâ\\x80\\x99ve confirmed this with other fellow publications. The performance is accurate nonetheless, but before we go crazy on testing dozens of games, we rather wait for a more stable driver.AMD had hinted the Radeon VII is on par with the RTX 2080 and we guess that statement could be true if you cherry picked the games and settings to get them within a percent of one another. However across our fairly random selection of games -- our whitman's sampler if you will -- the Radeon VII was 4% slower on average. That's still fairly decent, standing out only in Battlefield V and Strange Brigade.Performance was similar in Shadow of the Tomb Raider, Far Cry 5, Resident Evil 2, Hitman 2 and ARMA 3. Then we saw the Radeon VII fall behind in Forza Horizon 4 and then well behind in Rainbow Six Siege, Monster Hunter World, World of Tanks and Fortnite. Given the Radeon VII arrives at the same $700 MSRP as the RTX 2080, weâ\\x80\\x99d say this is a disappointing result.Moreover, if you were disappointed that the GeForce RTX 2080 was only slightly faster than the two year old GTX 1080 Ti, then youâ\\x80\\x99re no doubt disappointed that the Radeon VII is slightly slower. Also, as we saw power consumption isnâ\\x80\\x99t improved. In fact, itâ\\x80\\x99s slightly worse, so Iâ\\x80\\x99m not sure what more we can say at this point...At least the Radeon VII is a full 31% faster than the RTX 2060. You just have to close your eyes and ignore the fact that it costs 100% more. Or perhaps relatively soon we'll see Radeon VII selling on the street for something closer to $600 where it's better value and makes somewhat better sense.Vega 64 comes at a $500 MSRP, making the Radeon VII 40% more expensive but it's only 23% faster. Thatâ\\x80\\x99s even worse than RTX boards. The only saving grace here is that AMD hasnâ\\x80\\x99t tried to sell us on a technology that wonâ\\x80\\x99t deliver, while effectively scamming customers with a month long pre-order periodâ\\x80¦ gotta look on the bright side guys.Cost per Frame and Closing ThoughtsWe're not even going to try and sugar coat it -- not that we ever do -- I guess Iâ\\x80\\x99m just getting over disappointing GPU releases. Looking at cost per frame data, naturally itâ\\x80\\x99s worse value than the RTX 2080 and despite being faster than Vega 56 and 64, itâ\\x80\\x99s way worse value.Frankly we would rather turn down a few quality settings and enjoy the almost 25% improvement in value with Vega 56, or just get an RTX 2060 given itâ\\x80\\x99s slightly faster and slightly cheaper, then I'd just grin as Radeon VII owners tell me 6GB of VRAM isnâ\\x80\\x99t enough.At this point you might be wondering where the overclocking performance is, but unfortunately we have nothing for you right now. AMDâ\\x80\\x99s WattMan is broken and it just made my already unstable card even more unstable. So this is something weâ\\x80\\x99ll have to revisit later on. Update:AMD released new Radeon VII drivers that address all stability issues on release day one. Performance is the same, but at least anyone who bought this card should enjoy a flawless experience. We've updated our review score as a result.Thereâ\\x80\\x99s also that massive 16GB frame buffer and insane 1TB/s memory bandwidth which could be beneficial for content creators and other GPU accelerated applications.Another issue for Radeon VII will be availability. The retailers weâ\\x80\\x99ve spoken to have said availabilityâ\\x80\\x99s much worse than what we saw with the RTX 2080 Ti upon release. So while we suspect demand for this new Radeon GPU will be low, production might be even lower. As such weâ\\x80\\x99re not sure if or when we'll see custom AIB models. AMD has told us AIBs are free to develop their own models, but we suspect few will. We saw this happen with Vega, there just wasnâ\\x80\\x99t enough volume and demand to warrant the investment from board partners.It's worth mentioning that AMD is offering an attractive game bundle with free copies of the Resident Evil 2 remake, The Division 2, and Devil May Cry 5 shipping with every Radeon VII card. Though it's hard to compare with other bundles since these change all the time, it's a good one AMD is offering at launch.Bottom line, Radeon VII looks to be nothing more than a stop gap to the now heavily delayed Navi. Itâ\\x80\\x99s a way for AMD to stick their hand up half way and say we're still here guys, donâ\\x80\\x99t forget about us. The only hope for the Radeon VII now is that production costs will come down over the coming months and they can start to edge down to $600. Itâ\\x80\\x99s a big ask, and even there it would become only slightly better value than Vega 64 and the RTX 2080.This is not great news for gamers wanting good value on high-end GPUs. AMD also needs a miracle with Navi. A lotâ\\x80\\x99s riding on that one, but given what theyâ\\x80\\x99ve achieved with Ryzen, we strongly believe anything is possible.Pros: Faster Radeon graphics, yay! AMD fans get a faster GPU but you're going to have to pay for it and it's not better than the competition. Massive 16GB frame buffer and insane 1TB/s memory bandwidth must be good for something.Cons: Radeon VII runs hot and loud. Value is worse than RTX cards, and you don't even get the benefit of the doubt of DLSS or ray tracing for the extra money. Early drivers could mean more performance down the line. If not, AMD will have to cut pricing by at least $100.\"],\n",
       "  [\"This is our second look at the new GeForce GTX 1660. Not to be confused with the 1660 Ti that was released a month earlier, both GPUs offer great value at mid-range prices of $220 for the GTX 1660 and $280 for the Ti version.For those of you who missed it, our day one review of the GTX 1660 featured a dozen games, many of which were recently released titles and there the new GPU came out looking like a champ. Today we're expanding the benchmark test to a grand total of 33 games to see how it stacks up to the likes of AMD's Radeon RX 580 and 590.While we donâ\\x80\\x99t expect this expanded benchmark test to change much, it should serve as a decent buying guide for those tossing up between the GTX 1660 and perhaps the Radeon RX 580. The fact that the new GeForce GPU is some 30% faster in Assassinâ\\x80\\x99s Creed Odyssey might not matter much if you predominantly play titles such as Strange Brigade, Battlefield V or Sniper Elite 4, to cite a few examples.To give you a better idea of how these two GPUs and more compare in a wide range of games, we put together this 33 game benchmark. The focus will be on 1080p, which is the most popular resolution for this kind of GPU, but we've also tested at 1440p making the entire analysis every bit as comprehensive as you may want.Our usual GPU test system was used, equipped with a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. Drivers used include the AMD Adrenalin 2019 Edition 19.2.3 for the Radeon GPUs and Game Ready 419.35 WHQL for the GeForce cards.BenchmarksThe first test in today's lineup is The Division 2. It's the first time we're featuring this brand new title in one of our benchmark features. Rather than using the canned benchmark weâ\\x80\\x99ve recorded performance in-game. We can observe that the GTX 1660 is good for 68 fps on average at 1080p using the ultra quality preset.That figure drops down to just 46 fps at 1440p and while playable youâ\\x80\\x99ll probably want to do some tweaking here to improve frame rates. As it stands the GTX 1660 was 15% faster than the GTX 1060, but 8% slower than the RX 590.Assassinâ\\x80\\x99s Creed: Odyssey is a bad title for AMD cards and here we see the GTX 1660 basically matching Vega 64. But donâ\\x80\\x99t expect to see that too often. The $220 GTX 1660 was just 12% faster than the GTX 1060 6GB, but 28% speedier than the Radeon RX 590.At 1440p we see a slight shift in the results. The 1660 maintained a reasonable lead over the 1060 and RX 590/580 GPUs, but was seen slipping behind the Vega GPUs by a slim margin.Next up we have Strange Brigade which is kind of the opposite of what we just saw. The GTX 1660 is only able to match the GTX 1060 6GB at 1080p, making it quite a bit slower than the Radeon competition. Much the same is seen at 1440p, with the GeForce 1660 running behind the Radeon RX 570, so obviously not a great result for Nvidia in this title.The GTX 1660 is back on top for our Star Wars Battlefront II testing, sitting between the RX 590 and GTX 1070, with 85 fps on average itâ\\x80\\x99s in a category of its own.Then at 1440p we find a similar situation, where the 1660 was 17% faster than the RX 590 and 22% faster than the 6GB GTX 1060. It was also 15% slower than the GTX 1070.Moving on to Monster Hunter: World, the GTX 1660 is able to match the RX 590. We see much the same story at 1440p, and this means the GTX 1660 was 20% slower than the Ti version.Warframe has the GTX 1660 offering a small performance gain over the GTX 1060 6GB as well as the RX 590. Every GPU tested in this title was good for over 100 fps on average, so it looks like the win is shared among all GPUs for this one.Things become a little more challenging at 1440p, but even so the GTX 1660 was good for a 1% low of 90 fps with an average frame rate of 109 fps. This made it 9% faster than the old Pascal-based GTX 1060.F1 fans will enjoy what the GTX 1660 has to offer. At 1080p it was 11% faster than the similarly priced RX 590 and 18% faster than the older 6GB 1060. It was also just 16% slower than the GeForce GTX 1070 and Radeon RX Vega 56.At 1440p it manages to close the gap to the GTX 1070, while Vega 56 gets a little further away. But overall performance was strong with 61 fps on average.The last game weâ\\x80\\x99re discussing the results for is The Witcher 3. Here the GTX 1660 is matching the RX 590 but surprisingly runs 3% slower than the old GTX 1060 6GB. Performance wasnâ\\x80\\x99t any better at 1440p, where the 1660 only matched the 1060, placing it roughly on par with the Radeon RX 590.Performance SummaryGTX 1660 vs. Radeon RX 590Here is the breakdown of all 33 games tested, comparing the GTX 1660 and Radeon RX 590 at 1080p. The GeForce GPU was on average 5% faster, down from an 8% win in our original 12 game test. So while the performance margin has shrunk a little, the new Turing GPU still came out on top and did so winning by a 5% margin or greater in 16 of the 33 games tested.Meanwhile the RX 590 was faster by a 5% margin or greater in just 5 of the games. Even though AMD has cut pricing to match the GTX 1660, theyâ\\x80\\x99ll need to do better than that for us to recommend the RX 590 over the GTX 1660.GTX 1660 vs. GTX 1060 6GBWe see a dramatic difference when comparing to the previous generation GTX 1060. The margin dropped from 21% in our initial 12 game sample (day one review) to just 14% with all 33 games. Thatâ\\x80\\x99s a big difference and as suspected the 1660 generally does much better in newer titles. Older games such as The Witcher 3, World of Tanks, Project Cars 2, and so on donâ\\x80\\x99t hand the Turing architecture much of an advantage over Pascal.GTX 1660 vs. GTX 1660 TiFinally when comparing head to head with the GTX 1660 Ti, the vanilla 1660 is 15% slower and thatâ\\x80\\x99s very much in line with the previous test which saw the non-Ti model come in 13% slower. Of course, we also saw times where the margin grew to 20% or more, and we saw that in half a dozen titles.Cost per Frame and Closing ThoughtsPerhaps one of the key findings from this review compared to what we saw a week ago is not simply the number of games tested, but because Nvidia clearly undercut the competition, a week has been enough time for AMD to react and adjust prices, so our cost per frame comparison has seen some changes as well. On the upside, changes are not dramatic, and GTX 1660 cards are indeed selling at the intended $220 MSRP already.The RX 580 has dropped down from $200 to $190 which is not a huge deal, but still makes the Radeon a viable alternative for some. Essentially you can save ~10% on the cost of the graphics card to have 10% of the performance shaved off. The RX 590 has seen a heftier price cut with numerous models now available at $220. This GPU is finally down to the price it should have launched at. Given itâ\\x80\\x99s a little slower than the 1660 but costs about the same, itâ\\x80\\x99s not quite as good in terms of value.The only clear cut winner for AMD right now is the RX 570, but thatâ\\x80\\x99s roughly 25% slower than the GTX 1660 so they arenâ\\x80\\x99t exactly in the same performance category. Meanwhile the RX 580 has gone from being the best value sub $250 option, to a somewhat tough sell.Of course, itâ\\x80\\x99s best to look at performance in the games youâ\\x80\\x99ll be playing. If you're primary playing DiRT 4 or Battlefield V, then the RX 590 is a better choice. Otherwise, as we just saw there are far more games where the GTX 1660 gets the better of the Radeon GPU.For those of you planning on buying and keeping your graphics card for 2-3 years and have no idea how things will look even just 6-12 months down the track, power consumption is worth keeping in mind. Not because weâ\\x80\\x99re worried about the power bill, but because a more efficient graphics card is a quieter graphics card, and itâ\\x80\\x99s also more friendly to other components in your case as itâ\\x80\\x99s not dumping as much heat on them.The GTX 1660 pushed our test system's consumption to 262 watts. Thatâ\\x80\\x99s a 12% saving compared to the RX 570 and almost 30% less than the RX 580 and 590. If we were looking at power consumption for the graphics cards alone those margins would increase significantly, making it clear Turing GPUs are much better in this regard.So in spite of a few pricing adjustments, our recommendation has not been altered from the original review. The biggest change has come from the sheer volume of games tested. The GTX 1660â\\x80\\x99s lead over the RX 590 shrunk, while the 590â\\x80\\x99s price also shrunk a little. So those two are more evenly matched now.It was pretty shocking to see the GTX 1660â\\x80\\x99s lead over the GTX 1060 6GB shrivel up from 21% to just 14%. Regardless, the 1660 was never intended to be an upgrade option for GTX 1060 owners, but perhaps it is a reasonable buy for those running older cards like the GTX 960, which we'll likely explore soon.\"],\n",
       "  ['Today we\\'re revisiting our original Core i9-9900K review and updating it with 95 watt TDP limited results, basically results based on the official Intel specification. For better context about this please read our opinion article from earlier this week titled â\\x80\\x9cDo We Need to Re-Review the Core i9-9900K?â\\x80\\x9d.The short version of this is that motherboard makers are currently getting blamed for running the 9900K out of spec, when in reality we strongly believe itâ\\x80\\x99s Intel whoâ\\x80\\x99s cheating their own spec and pushing board partners to run the 9900K at the default clock multiplier table, rather than at the official power spec.Whatever the case, out of the box the 9900K isnâ\\x80\\x99t running at the Intel spec, itâ\\x80\\x99s essentially overclocked and this has caused power and thermal results to go through the roof. So in todayâ\\x80\\x99s re-test we\\'ll be showing how the Core i9-9900K performs when adhering to the Intel specification and comparing that data to the current out of the box experience.It doesnâ\\x80\\x99t really matter where you stand on this, having a resource that shows how these configurations compare under the same test conditions is useful information in our opinion. For the unlimited testing the MSI MEG Z390 Godlike has been used and for the 95-watt limited testing I used the Asus ROG Maximus XI Hero, loaded up the \"xtreme\" memory profile and opted to use the Intel settings which enforces the 95-watt TDP. So letâ\\x80\\x99s get into the results...BenchmarksFirst up are Cinebench R15 multi-threaded scores. Previously we found the 9900K breaking the 2000 point barrier, however with the TDP limit in place the score is reduced by 14%, and that places it roughly on par with the Core i7-7820X and crucially, meant it was a few percent slower than the 2700X. Already you might be getting a sense of why Intel is happy for board partners to run out of spec.Next up we have the Blender short run test and here the 95-watt TDP limited configuration can only burst up to 120 watts for 10 seconds. In other words, for about half the test itâ\\x80\\x99s almost fully unleashed, and we were only seeing a 9% reduction in performance. Thatâ\\x80\\x99s still a reasonable drop, but itâ\\x80\\x99s not the full story.Professionals looking to invest in a rig for rendering will be running workloads that take much longer than 20-30 seconds. Generally weâ\\x80\\x99re talking hours of rendering work. Whereas we saw a 9% reduction in the short run test, here weâ\\x80\\x99re seeing a 14% reduction in a more realistic rendering workload. Thatâ\\x80\\x99s a pretty big drop off and it means the 9900K is now only keeping pace with the Ryzen 7 2700X.The Corona benchmark runs for over a minute and here we see a 13% decrease in performance when power limited. The 9900K was 25% faster than the 2700X when allowed to run without a power limit, but with the 95 watt TDP enforced itâ\\x80\\x99s just 9% faster. Itâ\\x80\\x99s still faster but the margin isnâ\\x80\\x99t as impressive anymore.Here we see a 15% reduction in performance for the 9900K when using the 95 watt limit and this meant it was just 4% faster than the 2700X, whereas we found it to be 23% faster previously.7-zip compression performance isnâ\\x80\\x99t impacted heavily by the 95-watt TDP limit as we see a minimal 3% decrease.For decompression the 9900K suffers a bit more, showing 7% slower results with the TDP limit enforced, enough to make it slower than the Ryzen 7 2700X. Itâ\\x80\\x99s not a significant difference dividing the two however.Excel is the perfect example of a short workload, at under 10 seconds the 9900K isnâ\\x80\\x99t impacted by the official spec and we see much the same performance with and without the TDP limit in place.Testing with HandBrake we see a 14% reduction in performance with the TDP limit enforced, so the 9900K is just 4% faster than the 8700K and 13% faster than the Ryzen 7 2700X. The 1st and 2nd generation Ryzen CPUs donâ\\x80\\x99t do that well with AVX workloads, so letâ\\x80\\x99s look at the margins in the H.264 test.Whereas we saw a 14% reduction in performance running H.265 encoding, we only see half that hit with a non-AVX encoding workload. However Ryzen is much better when using H.264 and looks more competitive here.The 9900K with the TDP limit in place dropped down to an all-core of 4 GHz in the H.265 test while it sustained 4.2 GHz in the H.264 test.Moving on, here we see another example where the 95-watt TDP sees the 9900K come in behind the 2700X by a small margin. Itâ\\x80\\x99s also interesting to note that in this Premiere Pro CC export the unlimited 9900K matched the 7820X, a 140 W part on the same process. Iâ\\x80\\x99ve said previously that the 9900K should have at least a 140w TDP rating and that does seem to fit with what we see here.The Premiere Warp stabilizer test doesnâ\\x80\\x99t max out all cores all the time, itâ\\x80\\x99s a typical editing workload, and here we see a 6% reduction in performance with the TDP limit in place. Still that was enough to see the 9900K come in behind the 8700K.Power ConsumptionThe following graph results explain a lot... during the Core i9-9900K\\'s launch week and first batch of reviews, we saw a few reviews that claimed the 9900K consumed less power than the 8700K, which doesnâ\\x80\\x99t really pass the common sense test.However, if you test the 9900K with a 95-watt TDP limit and the 8700K without a TDP limit you get this. Even with the TDP limit in place, the 8700K and its 6-cores wonâ\\x80\\x99t be impacted nearly as much as the 9900K and its 8-cores, for things to remain even the 9900K would need a TDP limit of at least 125 watts.What we see here is a 31% decrease in total system consumption as all 8-cores are wound down from 4.7 GHz to 4.0 GHz and as you can see that 15% reduction has a profound impact on system consumption as weâ\\x80\\x99re also taking a lot of voltage out of the chip at the lower clock speed.In Blender we see a 27% reduction in total system consumption and now the 9900K looks like a mighty efficient CPU. It was a few percent faster than the 2700X and here we see it reduced total system consumption by 12%. Previously it was 19% faster than the 2700X, but also pushed consumption 21% higher.Given what we saw from the total system power consumption results, these thermal numbers while shocking arenâ\\x80\\x99t that surprising. Using the Noctua NH-D15 and Corsair Hydro H100i Pro I found the 9900K to hit temperatures in the mid-80s when fully unleashed.However, using the 95-watt TDP spec the 9900K maxed out at just 64 degrees in our Blender stress test and that figure was dropped to just 58 degrees with our custom loop. So when operating all cores at 4 GHz the 9900K is as cool as a cucumber, but at 4.7 GHz it turns the CPU socket into a fiery pit of melting silicon. Ok... it\\'s not that bad, but itâ\\x80\\x99s bloody hot in comparison.Gaming BenchmarksWhen testing with Assassin\\'s Creed Odyssey we observe there is a measurable performance hit in CPU intensive titles, though this comes only under unrealistic conditions, for example, gaming at 1080p with an RTX 2080 Ti. At 1080p we see an 8% hit to frame time performance, then this margin is reduced to 3% at 1440p.Playing GPU bound titles like Forza Horizon 4 shows no impact and I expect this is how the 1080p results will look in most tiles. So keep that in mind as weâ\\x80\\x99re mostly focusing on CPU bound games in this review, Forza being the exception.Here we see a 7% hit to frame time performance when testing with Hitman at 1080p, a 6% hit at 1440p, and itâ\\x80\\x99s not until we reach 4K that the margin evaporates.Interestingly, we see no real impact in Project Cars 2 and this title is a bit odd in the sense that the 9900K is so much faster than the 8700K. Iâ\\x80\\x99m not sure why that is as the game doesnâ\\x80\\x99t require 8 cores. Other sources have confirmed these margins, so itâ\\x80\\x99s not some strange bug with our test system.Thereâ\\x80\\x99s no real margin to speak of when testing with Rainbow Six Siege either. Any modern CPU running at around 4 GHz seems to work well here.We do see a pretty hefty 15% performance hit to frame time performance at 1080p in Shadow of the Tomb Raider, though once we get to 1440p weâ\\x80\\x99re almost entirely GPU bound.Finally, we have Star Wars Battlefront II where we see a small performance drop off at 1080p, nothing extreme and by the time we hit 1440p the margins close up to nothing.ConclusionFirst things first, our original Core i9-9900K review is as valid today as the day we published it. In this second look, we\\'ve taken a deeper dive at why the 9900K runs so hot out of the box and form an explanation of why some reviews were showing more favorable power numbers, albeit at reduced raw performance output.Gamers don\\'t need to worry about the 95-watt TDP spec, it doesn\\'t make much difference either way and running overclocked out of the box on Z390 motherboards wonâ\\x80\\x99t lead to insane thermals and power consumption for gaming.But if you do both work and gaming, and your work involves intensive CPU workouts -- theyâ\\x80\\x99re about the only kind of workouts I seem to do these days -- then itâ\\x80\\x99s a different story. In our long-run Blender workload temperatures go through the roof when running the 9900K out of the TDP spec, we saw an increase of 20 degrees. Of course, it was a similar story for power consumption, we saw total system consumption climb by almost 40%. But if the 9900K was forced to run with Intelâ\\x80\\x99s TDP spec and abide by the power limits, it would be a mighty efficient 8-core processor. Youâ\\x80\\x99d get 2700X-like performance, while saving a little over 10% on power.But this is the problem for Intel, the 9900K was already a tough sell in the overclocked configuration used by all motherboard makers and well... Intel knew that would be the case. Itâ\\x80\\x99s a $500 8-core desktop CPU competing with a $300 8-core desktop CPU. As we just saw with the 95-watt limit, itâ\\x80\\x99s barely any faster than the Ryzen 7 2700X. In fact, in some tests itâ\\x80\\x99s slower, and thatâ\\x80\\x99s an awful result for a CPU that costs ~70% more.This is a big issue for Intel and theyâ\\x80\\x99ve painted themselves into a corner here. For the 9900K to make an ounce of sense, for anyone who isnâ\\x80\\x99t an extreme overclocker, it needs to run at around 70 C with a quality aftermarket cooler and for that the TDP canâ\\x80\\x99t really be any higher than about 105 watts.However, even at 105 watts itâ\\x80\\x99s barely any faster than the uncapped 8700K and only a whisker faster than the much cheaper 2700X, so you canâ\\x80\\x99t have that with a Core i9. For those of you wondering, a 105 watt limit sees the 9900K sustain a clock speed of 4.15 GHz in our Blender workload and run at 69 degrees using the Corsair H100i Pro. Thatâ\\x80\\x99s a 150 MHz increase over the 95 watt TDP limit and a 5 degree increase in operating temperature.Basically the 9900K is a really good overclocker, if you invest in proper cooling. The 2700X, on the other hand, is on a situation where what you see out of the box is pretty much what you get. You can squeeze a little more out of it and tuning memory sub timings really help, but won\\'t be able to overclock the snot out of the cores like you can with Intel CPUs.With motherboards technically overclocking the 9900K to the default clock multiplier table, 4.7 GHz as an all core for example, under those conditions there isnâ\\x80\\x99t much left in it. For most 5GHz will be the limit, good luck keeping it cool past that without a serious amount of time, effort and risk. So realistically youâ\\x80\\x99re talking about up to a 6% boost over whatâ\\x80\\x99s shown here when looking at the unlimited results, and we certainly found that to be true when attempting to overclock the 9900K in our day-one review.Looking ahead into future reviews, we plan to stick to show the typical out of the box experience, and if that means overclocked CPUs, well thatâ\\x80\\x99s what we will show. We\\'ll also continue to publish additional insights and features like this one when need be.'],\n",
       "  [\"Today weâ\\x80\\x99re checking out the top-of-the-line MSI GS75 Stealth 8SG, one of a number of new RTX-powered gaming laptops from MSI. As this beast fits into their Stealth line, itâ\\x80\\x99s designed to be a slim and light portable gaming system, with the GS75 branding indicating itâ\\x80\\x99s their 17-inch model. Previously we reviewed the GS65 and really liked it, so we were definitely keen to check out the larger model equipped with a new GPU.The GS75 Stealth 8SG model we were sent for testing is the most powerful unit available equipped with an RTX 2080 Max-Q GPU. There are also models available with the RTX 2070 Max-Q and the RTX 2060. Each variant of this laptop with a different GPU is then available with different RAM and storage configurations, in our case we received 32GB of dual-channel memory and a dual 512GB SSD setup in RAID 0.The rest of the hardware is shared among the configurations: you get an Intel Core i7-8750H processor, the standard for gaming laptops. Thereâ\\x80\\x99s also a 17.3-inch 1080p 144Hz â\\x80\\x9cIPS-levelâ\\x80\\x9d display, and an 82 Wh battery.The design of the new GS75 is nearly unchanged from MSIâ\\x80\\x99s previous versions, and thatâ\\x80\\x99s a good thing.It means weâ\\x80\\x99re still getting the excellent metal near-unibody build with beautiful copper gold highlights. This is one of the best looking gaming laptops on the market, so wholesale changes really arenâ\\x80\\x99t required. Itâ\\x80\\x99s a sleek laptop with a nice minimalist style.Weâ\\x80\\x99re still getting MSIâ\\x80\\x99s version of a slim-bezel laptop, even at 17-inches, with the tiny webcam positioned above the display. Itâ\\x80\\x99s not the slimmest bezels available on any laptop but it certainly feels like the display is taking up most of the room available to it, and it keeps the laptop compact overall. In fact, at just 19mm thick and 4.96 lbs heavy, itâ\\x80\\x99s a very portable chassis for a 17-inch beast, coming in slimmer and lighter than your typical 15-inch system.The keyboard is the same SteelSeries unit as previous MSI laptops. Itâ\\x80\\x99s not one of our favorites, the tactile response is spongy and not particularly satisfying, although travel distance is fine. It does come with a full numpad as weâ\\x80\\x99d expect for a 17-inch laptop, although the inclusion of full size arrow keys has truncated the right shift key which may annoy some users.The good news is you do get per-key RGB backlighting, which is becoming more widespread among high-end gaming laptops. You also get a sizable trackpad with excellent responsiveness.I/O is the usual affair: three USB 3.1 gen2 type-A ports, two USB-C ports, one is Thunderbolt 3 and the other USB 3.1 gen 1. Thereâ\\x80\\x99s Ethernet, a microSD card slot and HDMI as well plus dual 3.5mm audio jacks. Our only concern here is the amount of ports on the right side of the laptop, such as HDMI, which makes it a little awkward to have a mouse there if several things are plugged in.PerformanceLetâ\\x80\\x99s talk about performance now, and as a quick reminder, everything here is based on our experience with the RTX 2080 Max-Q model. With that said, we do have comprehensive benchmark reviews of the RTX 2070 Max-Q and RTX 2060 that should apply to this laptopâ\\x80\\x99s other GPU configurations.As with many gaming laptops, there are several performance modes and fan profiles to choose from. Throughout this review we found the top Turbo performance mode to perform the best, because it overclocks the GPU by 100 MHz compared to the default Sport mode. We also found that while there are four fan modes, the auto mode performed just as well as the other performance-oriented modes, with very little change in fan speeds when gaming. This is great to see because auto also quietens the fan significantly during less intensive tasks like web browsing.Throughout this review weâ\\x80\\x99ve used the combination of the Turbo mode and Auto fan mode.The laptop comes with one of Intelâ\\x80\\x99s six-core Core i7-8750H processors, which is widely used across gaming laptops, particularly in this form factor. Weâ\\x80\\x99ve tested this CPU many times before so weâ\\x80\\x99re not going to rehash a ton of benchmarks for this laptop in particular, if youâ\\x80\\x99re interested, go back and check out our original review of this mobile CPU.In terms of CPU performance, all weâ\\x80\\x99re looking for is to see whether the MSI GS75 Stealth performs especially well (or not) compared to the average result from all the Core i7-8750H laptops weâ\\x80\\x99ve tested so far. It does depend on the workload, but the GS75 is a little slower on average, especially in longer workloads like Handbrake and x264 encoding where itâ\\x80\\x99s about 6 percent behind. With that said, itâ\\x80\\x99s a little faster shorter tasks like 7-Zip and MATLAB so this isnâ\\x80\\x99t a bad result overall, itâ\\x80\\x99s just canâ\\x80\\x99t sustain as high performance for as long as some of the beefier laptops weâ\\x80\\x99ve tested.If you are coming from a quad-core laptop, something using a Core i7-7700HQ for example, you can expect the GS75 to deliver pretty sizable gains: 40% in workloads like x264 encoding, and over 50% in 7-Zip, with a handy 10% improvement in single-threaded tasks as well. Jumping up to six-cores is a good move, especially if you do a lot of multi-threaded work, and it can help alleviate a few bottlenecks in some games.Gaming PerformanceWeâ\\x80\\x99re sure most of you reading this review will be more interested in gaming performance though, given thatâ\\x80\\x99s the key selling point of getting a unit with a GPU as fast as the RTX 2080 Max-Q. If youâ\\x80\\x99re after a detailed performance breakdown of this GPU, please check out our full review that focuses on the RTX 2080 Max-Q, where youâ\\x80\\x99ll see some individual game breakdowns and discussion. Itâ\\x80\\x99s safe to say this GPU is very capable of gaming at 1080p at high frame rates, which makes use of this laptopâ\\x80\\x99s 144 Hz display.The GS75 Stealth uses the regular 80W configuration of the RTX 2080 Max-Q, not the faster 90W version, so the data to look at in our GPU review is for the standard configuration. But that data only refers to how this laptop performs in the default Sport performance mode, if you enable the Turbo mode, the GPU gets a 100 MHz factory overclock of sorts, which improves performance over the 'standard' RTX 2080 Max-Q configuration.Weâ\\x80\\x99re sure most gamers will want the best performance straight away, so this factory OC mode is something you should use to get the most out of the laptop. As far as thermals and noise are concerned, the Turbo mode didnâ\\x80\\x99t impact either significantly, so we donâ\\x80\\x99t see a reason not to use it for gaming.The best reason to use Turbo mode is that it improves gaming performance by 3 percent on average. Itâ\\x80\\x99s not enough to get the RTX 2080 Max-Q in its standard 80W config to reach the performance levels of the 90W version, but itâ\\x80\\x99s a handy boost over stock performance that you can essentially access with a single click. And depending on the game you play, you could see up to a 10 percent improvement, which is impressive.How does this Turbo performance stack up to other GPUs? Well, letâ\\x80\\x99s take a look. As we mentioned, weâ\\x80\\x99re sitting about three percent behind the 90W configuration of the RTX 2080 Max-Q, not a massive gap but laptops that do include the higher TDP variant deliver better performance. Unfortunately, thereâ\\x80\\x99s no easy way to tell which laptops are 90W models.Weâ\\x80\\x99re getting 12 percent more performance from the GS75 than a standard RTX 2070 Max-Q laptop without any factory overclocking. Some games are heavily constrained by the CPU or other hardware, like Hitman 2, while other such as Metro Exodus deliver 17 percent more performance. Youâ\\x80\\x99re not getting desktop-like margins here, but low double digit gains from the same cooler design isnâ\\x80\\x99t bad.How does 27 percent more performance than the GTX 1070 Max-Q sound? Thatâ\\x80\\x99s a very healthy improvement assisted by the small factory overclock provided by the GS75. Weâ\\x80\\x99re not comparing the same price bracket, but we suspect some people with a GTX 1070 Max-Q laptop might be considering spending a bit more cash on their next upgrade.Youâ\\x80\\x99ll also get 18% more performance than an RTX 2060 laptop. This is another GPU thatâ\\x80\\x99s available in the MSI GS75, so which GPU is best might come down to the pricing you can get for each variant. The RTX 2080 Max-Q is a decent amount faster.And finally a comparison to the Pascal GTX 1070 for laptops, which wasnâ\\x80\\x99t usually found in laptops of this size and weight, but still is a relevant comparison. The RTX 2080 Max-Q is only providing 9% more performance on average, and doesnâ\\x80\\x99t win in every title, though in some titles it is significantly faster. Itâ\\x80\\x99s not a bad upgrade, but itâ\\x80\\x99s not near the performance gain you can expect moving from a desktop GTX 1070 to a desktop RTX 2080.CoolingMSIâ\\x80\\x99s current-generation triple-fan cooling solution is very good, itâ\\x80\\x99s definitely one of the better designs weâ\\x80\\x99ve seen for this sort of portable gaming system. Even when using the overclocked Turbo mode, the GS75 is quieter than your average gaming laptop, at 44.5 dBA. And it does this while keeping the GPU around 75 degrees C, which is cooling performance youâ\\x80\\x99d normally expect from a chunkier laptop like the ROG Strix Scar II, not a slim system like this.In fact, the GS75 is significantly better it this regard than the GS65 with the GTX 1070 Max-Q, although it is slightly louder, itâ\\x80\\x99s also a larger system so thereâ\\x80\\x99s more room for heat dissipation, and the GPU is different. More impressively, itâ\\x80\\x99s quieter and cooler than the Gigabyte Aero 15 X9, which uses the slower RTX 2070 Max-Q. However, this does come at the cost of CPU cooling, which is right at its limit, hitting 91 degrees C under a gaming load.Surface temperatures as well were manageable, the laptop gets quite hot above the keyboard where there is some ventilation, however the keyboard area and wrist rests were comfortable to use during an extended gaming session. Just make sure you donâ\\x80\\x99t place anything that melts around the back or sides of this laptop, because the extensive venting holes do push out a fair volume of hot air.Storage PerformanceLooking at storage performance, the GS75 we received with 1TB of SSD space was configured with two Samsung PCIe NVMe drives in RAID 0, so naturally it delivered extremely good sequential speeds. Random performance is at the limit of these drives but is still excellent for a gaming laptop, and itâ\\x80\\x99s safe to say this SSD is much faster than what you need for game loading.When removing the bottom cover you get easy access to the two loaded SSDs as well as an additional third M.2 slot, which was free in our unit, so thereâ\\x80\\x99s further room to upgrade the storage in the future. This is also a better internal design than the last-gen GS65 we looked at, which had no access to storage without further deconstruction.However there are a few disappointing aspects to the internal layout here. Because MSI is still using a flipped motherboard, you canâ\\x80\\x99t access the RAM without taking the entire motherboard out, which is tricky and time consuming. Hopefully the models that come with 16GB of RAM include it in a dual-channel configuration for the best performance, because if itâ\\x80\\x99s single channel, it will be hard to fix.Thereâ\\x80\\x99s also no 2.5-inch drive bay so youâ\\x80\\x99re stuck with just M.2 slots for storage upgrades. Perhaps not the biggest deal given this is a high-end laptop and thereâ\\x80\\x99s a generous three M.2 slots, but it would have been nice given this is a 17-inch notebook.Battery Life & Display QualityBattery life is pretty good from the 82 Wh battery, despite the use of a 17-inch high refresh display. In our video playback test it performed around the same mark as other 15-inch laptops of this type, which Iâ\\x80\\x99d say is a good result.The display is 17.3-inches in size, it packs a 1080p resolution and it has a 144 Hz refresh rate. You donâ\\x80\\x99t get adaptive sync, which is a trade-off to allow better battery life through GPU switching, but the quality of this panel is great and the high refresh rate is a perfect pairing for this GPU that often pushes frame rates above 100 FPS at 1080p with Ultra settings.Brightness is very good, at over 400 nits, while the contrast ratio of 1000:1 is typical for a laptop IPS. And because itâ\\x80\\x99s IPS, you get great viewing angles with acceptable response times. But what we feel is the best aspect to this display, is MSIâ\\x80\\x99s color calibration. When using the sRGB mode available through MSI True Color, youâ\\x80\\x99re getting an accurate white point around 6500K, plus strong deltaE averages in our saturation and ColorChecker tests, where the display falls under 2.0. Itâ\\x80\\x99s not perfect due a greyscale deltaE average a little above 2.0, but otherwise this is an excellent result for a gaming laptop display.Closing RemarksOverall the MSI GS75 Stealth 8SG is an excellent portable gaming laptop. Thereâ\\x80\\x99s not a lot to complain about in regards to the hardware package. We absolutely love the metal chassis, the 15-inch display is very good, itâ\\x80\\x99s cooler and quieter than an average gaming laptop despite the size and powerful hardware inside, and thereâ\\x80\\x99s good I/O.Itâ\\x80\\x99s even better from a performance perspective. Surely weâ\\x80\\x99ve criticized the RTX 2080 Max-Q on a number of fronts like the naming confusion, how it stacks up to desktop parts and so on. But purely looking at its standalone performance, itâ\\x80\\x99s impressive what it can do in this sort of form factor.Itâ\\x80\\x99s great for 1080p high refresh gaming and very solid for higher resolutions using external displays, in the latest games at Ultra settings. The Core i7-8750H is also perfect for productivity tasks on the go like video editing. Right now, this is the fastest hardware you can get in this form factor.Most of our criticisms of this laptop feel like minor nitpicks relative to the overall quality of the system. The keyboard is a little spongy, the I/O layout isnâ\\x80\\x99t the best, and the 8750H is a little slower than average in longer workloads. But overall, looking just at the hardware, thereâ\\x80\\x99s no doubt this is a great gaming laptop and one weâ\\x80\\x99d recommend.Unfortunately, this hardware comes at a price, and itâ\\x80\\x99s a high one. To get top-end performance in this size, youâ\\x80\\x99re going to need to fork out $2,800, and thatâ\\x80\\x99s just for the GS75 with a 256GB SSD and 16GB of RAM. For the configuration we reviewed, with a 1TB SSD and 32GB of RAM, it's closer to $3,000.Itâ\\x80\\x99s hard to justify spending that sort of money on the RTX 2080 Max-Q, because youâ\\x80\\x99re paying a massive premium to get this flagship-level performance. The RTX 2070 Max-Q version of the same laptop is $400 cheaper, meaning it delivers about 10% less performance for a 14% lower price. The RTX 2060 model is even better relative value at $2,100, so 25% cheaper for 14% less performance. Whether spending an extra $700 is worth it to get 10 to 20 FPS more is up to you.The value situation is further muddied by Pascal laptops, which admittedly are getting harder to find as stock clears out. Right now you can get a GTX 1070 Max-Q system like the Asus ROG Zephyrus S for around $1,800, which is slightly better value than the RTX 2060 GS75, given the 1070 Max-Q is also slower.Itâ\\x80\\x99s an interesting market, because for regular laptops â\\x80\\x93 meaning, not the slim and light variants weâ\\x80\\x99ve been talking about so far â\\x80\\x93 we wouldnâ\\x80\\x99t recommend a more expensive RTX 2060 laptop over a cheaper GTX 1070 laptop right now. But for slim and light systems, the GTX 1070 Max-Q and RTX 2060 are fairly priced given their performance differences. What is the better option will be down to your budget, though we tend to like a lot the value offered by the RTX 2060 GS75 just because we love this MSI design and its 17 inches for playing games.\"],\n",
       "  ['Not long ago we took Das Keyboardâ\\x80\\x99s latest devices out for a spin: the long awaited Das Keyboard 5Q and X50Q. Our experiences with both keyboards were mostly positive, primarily due to their sturdy construction and tactile switches. However, high quality materials and switches are nothing new for Das.But when you\\'ve been building sturdy mechanical keyboards for years and have gone through various iterations of the same board to get it near perfection for your intended market, how else can you innovate? The Das Keyboard 5Q and X50Q introduced the concept of the â\\x80\\x9ccloud connectedâ\\x80\\x9d keyboard by streaming information from the web directly to your fingertips.As interesting as the idea is, I found that the way it was implemented was a bit gimmicky and impractical for the average user. Das is hoping to improve upon what we saw before, offering the same cloud features we tested last September, while adding smart IoT device control. These new software features have been made available to the 5Q, X50Q, and now on their latest 4Q model.The Das Keyboard 4Q, as the name implies, combines the proven hardware design of the Das Keyboard 4 we know and love, while adding the smarts of the 5Q as well as per-key RGB backlighting.Are the new IoT features useful enough to make the 4Q a better buy than the 5Q or even the standard Das 4? Letâ\\x80\\x99s find out.Build Quality and FeelIf youâ\\x80\\x99ve already read my review of the 5Q and X50Q, or if youâ\\x80\\x99re simply a fan of Das Keyboardâ\\x80\\x99s devices in general, it won\\'t come as a surprise for me to say the 4Q is one of the sturdiest keyboards I\\'ve ever used. It\\'s right up there with any other high-end keyboard, and it\\'s definitely on par with the 5Q and X50Q.Just like those devices, the 4Q has a solid aluminum top plate and firm plastic backing. Neither surface scratches easily, and the keyboard as a whole did not warp or bend even when I applied intense pressure.From a design perspective, there are a few key differences between the 4Q and the 5Q or X50Q. Namely, the 4Q does away with the two-in-one Q button/volume wheel system. Now, thereâ\\x80\\x99s a new dedicated Q button directly adjacent to the volume wheel. This is a small change, but it solves the problem of accidentally opening up the Q software when you only meant to adjust your systemâ\\x80\\x99s volume.A few features inherited from the standard Das 4 Professional, we see the inclusion of the â\\x80\\x9csleepâ\\x80\\x9d key, which puts the 4Q into a power-saving mode until you use it again. It also comes with a USB 2.0 hub with two ports towards the rear of the device. I didnâ\\x80\\x99t find myself using them all that often, but I can see them being a lifesaver for those with more peripherals and gadgets than I.With the good and neutral changes out of the way, let\\'s talk about some of the things I didn\\'t like about the 4Q\\'s design. First, unlike the 5Q and the X50Q, the 4Q does not ship with a wrist rest. Instead, you get an awkward stand-off bar (called a \"footbar\" by Das) that attaches to the bottom of the keyboard, presumably intended to improve ergonomics.Unfortunately, it doesnâ\\x80\\x99t serve that purpose very well, and that leads me to my second complaint: the keyboard is slightly uncomfortable to use. Even with the stand-off bar, the 4Q lies almost completely flat, which forces me to tilt my hands upwards at an uncomfortable angle to type or game.A final design-related difference compared to the 5Q and X50Q, there were two accent lights located on either side of the keyboards; they werenâ\\x80\\x99t all that visible during the day, but at night they cast a soft glow across my desk which I tend to like. Das has eliminated those accent lights on the 4Q.Moving on from general design, letâ\\x80\\x99s talk about what is arguably the most important part of any good mechanical keyboard: the keys themselves.In this regard, I wasnâ\\x80\\x99t disappointed. The keyboard feels fantastic to type on, but thatâ\\x80\\x99s really no surprise given the Cherry MX Brown switches that Das has included under the hood. Browns are generally considered the pleasant middle ground between Cherryâ\\x80\\x99s softer (and quieter) MX Reds and clickier MX Blues. In the Das 4Qâ\\x80\\x99s case, they manage to feel responsive and tactile without shattering your eardrums.While I hammered away at this article on the 4Q review unit, I found that my typing accuracy was just as good as it had been on the 5Q (my previous daily driver). As an added bonus, the 4Qâ\\x80\\x99s switches feature full per-key RGB backlighting, customizable through the \"Q\" software.Das Q SoftwareAs mentioned before, Das is trying to innovate through software and it\\'s making the Q software one of the main selling points of their cloud-connected keyboards. While we were writing this review we noticed Das had just released a January software update with improvements, there was another on December, so they\\'re actively developing the platform.Signals are customizable in-keyboard alerts that use two third party services: IFTTT and Zapier. This lets you connect the 4Q with various apps (such as Discord and Slack) and websites (such as Twitch or YouTube). Once connected, you can set custom lighting effects for any given key, which will only trigger when a specific criteria is filled.For example, you can set your T key to flash purple when your favorite Twitch streamer goes live, or you can have your ESC key flash yellow when you receive a message on Slack. You could even tell your keyboard to remind you to keep up with your laundry every hour.If you donâ\\x80\\x99t like the idea of signing up to multiple third-party services for even the most basic of alerts, thereâ\\x80\\x99s good news this time around. Alongside the 4Qâ\\x80\\x99s launch, Das rolled out a free Q software update containing the new â\\x80\\x9cQ Marketplace.â\\x80\\x9dThe Marketplace contains a few basic (free, despite its name) pre-set Signals that work on a plug-and-play basis - thereâ\\x80\\x99s very little set-up required, and you wonâ\\x80\\x99t have to sign up for IFTTT or Zapier to use them. You can set up Twitch notifications, periodic â\\x80\\x9cstand upâ\\x80\\x9d reminders, or keep track of the temperature outside, among other things.These features sound great in theory, but in practice I personally didnâ\\x80\\x99t find much of a use for them. Still, this is one aspect of the 4Q experience that is completely subjective, so itâ\\x80\\x99s up to you to decide how much use you think you could get out of the deviceâ\\x80\\x99s cloud features.The Das 4Qâ\\x80\\x99s RGB lighting system isnâ\\x80\\x99t much different than what youâ\\x80\\x99ve probably seen from other mechanical keyboard manufacturers, on the upside, it is one key feature than previous Das Professional models lacked.To adjust the 4Qâ\\x80\\x99s backlighting, all you have to do is click the â\\x80\\x9cRGB Profilesâ\\x80\\x9d tab within the â\\x80\\x9cConfigureâ\\x80\\x9d section of the Q interface. From there, you can create custom lighting profiles and add various effects to individual or groups of keys, such as breathing, rainbow, or ripple.IoT FunctionalitySince I reviewed the Das 5Q, the IoT feature has been added, so I began my testing with high hopes. I donâ\\x80\\x99t have many smart devices besides a few Philips Hue bulbs, but I still expected to see a wide variety of IoT device control options for other better-equipped users to take advantage of.Unfortunately, that wasn\\'t the case. Though this may improve in the future, as of writing, you can either control your smart home bulbs to a limited degree, or you can adjust your smart thermostat - thatâ\\x80\\x99s it. There are no options to use your keyboard to control smart door locks, cameras, or any other fancy gadgets.Limited options aside, what IoT features the 4Q does have are fairly simple to setup. You just have to take a trip to the Q softwareâ\\x80\\x99s Command Center, which is tucked within the Q Signal Center.Once youâ\\x80\\x99ve done so, merely follow the on-screen prompts to set up a specific action, such as toggling on or off your bedroom light. If you get stuck, Das has published a helpful tutorial video, but the process is fairly straightforward - just know that you will need an IFTTT account to use these features.As mentioned before, the lighting control options are pretty limited at the moment. The only actions available to me were various on/off toggles for my lights. Despite several hours of experimentation, I found no way to use my 4Q to change my Hue bulbsâ\\x80\\x99 color or adjust their brightness. The restrictive nature of the 4Qâ\\x80\\x99s IoT features donâ\\x80\\x99t end there. If you want to change something about an action (not that thereâ\\x80\\x99s much to change) after configuring it, youâ\\x80\\x99ll need to delete it entirely on the Q software side and re-create it, because thereâ\\x80\\x99s no edit button. Furthermore, it doesnâ\\x80\\x99t seem to be possible to change the shortcut key for an action; it defaults to 0 and increments up by 1 for every additional action you add.Wrap UpWith the Das Keyboard 4Q, the company has completed a new series of luxury-oriented keyboards. Starting at $250 with the 5Q and $200 for either the gaming-oriented X50Q or the Das 4Q. Meanwhile, the previous models will continue to be sold at the same prices, which to be honest were considered pretty premium already: the Das 4 Professional / Ultimate, Model S and Prime 13.Putting build quality questions aside -- no problems whatsoever there, coming from the Das 5Q, I did miss the wrist rest and I felt the ergonomics of the 4Q are not as good, so that\\'s something to consider. That, coupled with my already-established indifference toward the Signal system, leads me to feel that the 4Q just doesnâ\\x80\\x99t offer enough unique functionality to be worth picking up over competing products.If you happen to feel differently about Signals and consider them a good feature to have, Das\\' X50Q is likely a better alternative. It has the same $200 price tag, but it ships with a couple of this keyboardâ\\x80\\x99s missing features, such as a wrist rest, and accent lighting. Both the Das 5Q and X50Q ship with Gamma Zulu switches, while the 4Q and the rest of Das Keyboards use Cherry MX switches of the Brown or Blue variety, depending on the model. To be clear about differences, too, the Das 5Q and X50Q do not come with a USB hub, while the 4Q features a two-port USB 2.0 hub, and the older $150 Das 4 Professional comes with a two-port USB 3.0 hub. The downgrade in this instance is not at all welcome.The idea of adding notification capabilities to your keyboard is still new and it will take time for Das to determine the viability of more functionality.At the end of the day, the decision is up to you. Perhaps the idea of switching on and off your lights or adjusting your homeâ\\x80\\x99s temperature without having to deal with an app or a clumsy voice assistant is a major selling point for you -- or maybe you\\'re just in the market for a solid keyboard, and don\\'t particularly care about the extra features. On the latter case, Das delivers as always, though it remains an expensive affair for most.'],\n",
       "  ['Today weâ\\x80\\x99re revisiting the GeForce GTX 980 Ti to see how it stacks up to the newly released RTX 2060 and GTX 1660 Ti, particularly in more recent titles such as Apex Legends, Resident Evil 2 and Far Cry New Dawn. Of course, while the GTX 1660 Ti sells for $280 and the RTX 2060 is $350, the GTX 980 Ti was much more expensive at $650 at launch.The GTX 980 Ti is now four years old, so youâ\\x80\\x99d expect new GPUs around half the price to deliver a similar level of performance... or do they?Whereas the 980 Ti packs 2816 CUDA cores, the RTX 2060 has 32% fewer cores and the 1660 Ti 45% fewer cores. But those Turing cores are wider, allowing them to execute more instructions, and theyâ\\x80\\x99re also clocked higher. The Nvidia spec for the 980 Ti calls for a boost clock of 1075 MHz. This gives the RTX 2060 a 56% clock speed advantage, and the GTX 1660 Ti a 65% clock speed advantage.The Turing GPUs also feature higher clocked GDDR6 memory, so even though the RTX 2060 has a 192-bit memory bus opposed to the much wider 384-bit bus of the 980 Ti, the faster memory makes up for the difference, so both have the same 336 GB/s memory bandwidth. The 1660 Ti has been downgraded to 288 GB/s with its 12 Gbps memory, but as weâ\\x80\\x99ve learned already, this doesn\\'t cripple it much.Earlier this year in our \"Ultimate Guide to Buying a Used Graphics Card\" we found the average selling price of an used GTX 980 Ti was $235. We took a look again at completed eBay listings when we were working on this article and that price remains accurate.So that makes this comparison all the more interesting... if you would consider it, should you get the 980 Ti for around $235 or buy a brand new GTX 1660 Ti for $280? To find out we grabbed our Core i9-9900K test system, installed the 980 Ti and got benchmarking. We tested 33 games in total but we\\'ll be discussing the results for a dozen of them before looking at the summary and performance breakdown graphs. All testing took place at 1440p, and while we are using a reference card, we overclocked it 1190 MHz, allowing it to hold a typical boost clock of 1.3 GHz. With those notes out of the way, letâ\\x80\\x99s get into the results.BenchmarksRather than limiting this test to a dozen or so titles, we wanted to aim for at least 30. This gives us a fuller perspective of how the older Maxwell GPU stacks up and how performance in older games compare to recently released ones. We mention this because to test such a massive range of games, it didn\\'t make sense to invest twice as much time testing the 980 Ti at different clock rates. In choosing to go with either the Nvidia spec or an aggressive factory overclock, we went with the latter. For the most part the overclocked configuration offers 10% performance, with gains as large as 15% seen under certain conditions.Our first test Metro Exodus was the only game in which we included the stock 980 Ti result as a reference. Here we see that the overclock provided a 9% performance boost, which is decent.The 980 Ti goes from being slightly slower than the GTX 1070 to slightly faster. This reference point serves us well and it\\'s usually what would find on the higher-end AIB models, such as Gigabyte\\'s Gaming G1 for example.The overclocked 980 Ti performs extremely well in Shadow of the Tomb Raider, beating the GTX 1070 by a few frames to come in just 2fps down on the 1660 Ti and 1070 Ti.Moving on we find that Forza Horizon 4 is a good game to include in this comparison as it utilizes modern GPU architectures really well. The GTX 980 Ti was only able to match the Radeon RX 590, making it ~10% slower than the 1660 Ti and RTX 2060.Next up we have Just Cause 4 and here the GTX 980 Ti came in just behind the GTX 1070 and 1660 Ti. Nothing that unusual about these results, so letâ\\x80\\x99s move on.Firing up Resident Evil 2 we see that the overclocked 980 Ti is able to roughly match the GTX 1660 Ti, making it 16% slower than the RTX 2060.Testing with Hitman 2 saw the 980 Ti match the GTX 1660 Ti while it was 14% slower than the RTX 2060 and 16% slower than Vega 56. Still a pretty solid result for the old Maxwell GPU.Fortnite uses the Unreal Engine 4 and this game engine is very familiar with the Pascal architecture.Here the overclocked 980 Ti just edged out both the GTX 1070 and 1660 Ti to come in just behind the GTX 1070 Ti and not much slower than the RTX 2060. It also beat AMDâ\\x80\\x99s Vega 56 by a 6% margin.The GTX 980 Ti performs really well relative to the Pascal competition, beating the 1070 by a few frames. That said, Turing performs very well in this title and as such as the 980 Ti was 10% slower than the 1660 Ti and 22% slower than the RTX 2060.When testing with Battlefield V we see that the overclocked GTX 980 Ti is on par with the GTX 1070 and we also find much the same when comparing it to the GTX 1660 Ti.The 980 Ti was 14% slower than a typical AIB version of the RTX 2060, not a massive margin but given the Maxwell GPU used to set us back $650, the fact that weâ\\x80\\x99re not getting more performance for just $350 is good news.Although itâ\\x80\\x99s gone through several major updates, World of Tanks is still a very old game at its core. Itâ\\x80\\x99s also well optimized for older GPU architectures and Nvidia has made sure their older GPUs still perform as they should in this popular title. In the case of the 980 Ti that means delivering GTX 1070 Ti-like performance making it 16% faster than the 1660 Ti and just 6% slower than the RTX 2060.Apex Legends is one of the newest games we\\'re testing and as you can see the overclocked GTX 980 Ti falls just short of the GTX 1070, behind the GTX 1660 Ti, and massively behind the RTX 2060.The gap to Pascal isnâ\\x80\\x99t too big but the more complex Turing architecture provides a significant performance uplift here, especially if we were to compare GPUs with similar core counts.The last game in this round is Far Cry New Dawn. Here the overclocked 980 Ti matched the stock GTX 1070 and 1660 Ti with an average of 74 fps. It was 15% slower than the RTX 2060, though Iâ\\x80\\x99m not suggesting that makes the budget RTX option a worthwhile upgrade for current 980 Ti owners... it is not.Individual MatchupsGTX 980 Ti vs. GTX 1070As weâ\\x80\\x99ve found time and time again in the past, the GTX 980 Ti and GTX 1070 deliver basically the same performance. At the stock Nvidia spec the 1070 is generally a little faster and with both overclocked to the max they are also very close, though the 980 Ti generally does a little better overall thanks to its huge degree of overclocking headroom.GTX 980 Ti vs. Vega 56For the most part we found the stock Vega 56 beating the 980 Ti. On average the old GeForce GPU was 7% slower on average with only 5 games where the 980 Ti won by a 5% margin or better. But given what you typically pay for one of these things on the used market, they are often a better choice for budget conscious shoppers.GTX 980 Ti vs. GTX 1660 TiOverall the venerable 980 Ti was a whisker faster, edging the budget Turing GPU by a mere 2% margin. Of course, while we used a stock MSI GTX 1660 Ti Gaming X, the 980 Ti was overclocked. That said, you can only squeeze about 5-10% more out of the 1660 Ti anyway. Itâ\\x80\\x99s still great to see weâ\\x80\\x99re finally getting GTX 980 Ti levels of performance for under $300.GTX 980 Ti vs. RTX 2060Even with the overclock boost the 980 Ti can\\'t hang with a stock RTX 2060. On average the Maxwell part was 12% slower, but we also saw instances where it was 20 to 25% slower, for example in Assassin\\'s Creed Odyssey, Rainbow Six Siege, Wolfenstein II and Apex Legends.Bottom LineThe once mighty GeForce GTX 980 Ti is now a match for a sub-$300 GPU, only took about 4 years. If you dropped $650 on a 980 Ti all those years ago and you still have it today, gaming at 1440p, then itâ\\x80\\x99s fair to say you got your money\\'s worth.Evidently for such a person the GTX 1660 Ti and RTX 2060 wonâ\\x80\\x99t warrant an upgrade, as that would be more of a side-grade. That said, if youâ\\x80\\x99re a $650 GPU kind of shopper then it was unlikely these mid-range offerings were ever going to entice you. For that youâ\\x80\\x99re probably after an RTX 2080 -- in titles such as Metro Exodus youâ\\x80\\x99d been looking at just over a 60% performance boost, which is way more significant.Now, if youâ\\x80\\x99re tossing up between buying a new GTX 1660 Ti or a used GTX 980 Ti... while you could potentially save around $50 by buying the 980 Ti (used price hovers around $235), thereâ\\x80\\x99s always the possibility that you might have to pay more for a good model. Alternatively, if youâ\\x80\\x99re willing to bide your time, you could manage to get one for even less. Thereâ\\x80\\x99s a bit of an art to buying second hand, but no matter how good you are there are still some unavoidable pitfalls.Obviously the 980 Ti is an old product now, so there will be no warranty involved. The risk for $50 savings doesnâ\\x80\\x99t seem worth it. Thereâ\\x80\\x99s also the issue of driver support. Without question the GTX 1660 Ti is going to age better moving forward with game specific optimizations.All in all, the GTX 980 Ti has aged well and today itâ\\x80\\x99s still capable of delivering an enjoyable 1440p gaming experience. Ask any enthusiast and that\\'s saying a lot.'],\n",
       "  [\"Following up to our best value FreeSync monitors feature, we though this review would be ideal. Today weâ\\x80\\x99re looking at the Viotek GFT27DB which is a brand new display release, sporting a new TN panel (ideal for gamers) that promises a lot on paper. Having reviewed several Viotek displays in the last year, weâ\\x80\\x99ve been consistently impressed with the value proposition, so we are hoping nothing changes here in that respect.The Viotek GFT27DB aims to be a flat TN version of their curved VA GN27D, so specs are very similar. Weâ\\x80\\x99re looking at a 27-inch panel, 1440p resolution, and 144 Hz maximum refresh rate. This monitor supports FreeSync with low framerate compensation, and we can confirm it works perfectly with both AMD and Nvidia GPUs.With this new monitor opting for a TN panel instead of a VA, gamers are treated to a flat display which we personally prefer at this size. But there are other key benefits. This is not your standard, crappy, entry-level TN â\\x80\\x93 instead this is what appears to be a brand new high-end TN from AU Optronics. Itâ\\x80\\x99s actually wide gamut, Viotek claims 115% sRGB, which is very rare for a TN, plus itâ\\x80\\x99s native 8-bit, another rarity for TN panels.The goal here is to mitigate some of the known color issues with TN displays, while still providing all the speed benefits, such as fast response times. Itâ\\x80\\x99s an interesting direction to take TN panels, but one that does make sense because the two technologies known for their colors â\\x80\\x93 IPS and VA â\\x80\\x93 are struggling to hit the response times of a good TN. So if a TN can get to the same colors as an IPS or VAâ\\x80¦ it brings it back into calculations for a lot of buyers. And weâ\\x80\\x99ll talk extensively about how this new panel performs in a moment.But before that, letâ\\x80\\x99s talk about the Viotek GFT27DB in particular. It comes in at the same price as the GN27D, so $330, which leaves buyers with a choice between TN and VA. The GN27D is currently the cheapest monitor in its class, but the GFT27DB isnâ\\x80\\x99t quite the cheapest 27-inch 1440p 144Hz TN. That crown goes to Pixio with the PX276, however the Pixio option is using an older TN that isnâ\\x80\\x99t wide gamut, so Viotek is offering potentially better color quality for a mere $20 extra.The design, like most of Viotekâ\\x80\\x99s monitors, is simple. Fairly cheap but minimalist plastic is used for the main monitor section, complete with a matte display coating thatâ\\x80\\x99s slightly grainy, but not unusually so for a TN. Bezels are slim, the three pronged stand is built from metal and looks pretty good, and aside from a very basic red light on the back, thereâ\\x80\\x99s not a lot of gamer style which we very much appreciate.As with all simple designs, there are some limitations. The stand only supports tilt adjustment, if you need height adjust youâ\\x80\\x99ll either have to purchase a VESA mount, or opt for another monitor entirely. The stand is also quite wobbly, more so than a typical monitor of this size. While the general build quality is fine if a bit unamazing, the standâ\\x80\\x99s instability makes it feel a bit cheap. That said, itâ\\x80\\x99s not going to move around on your desk once you put it in place.For ports, all standard stuff: one DisplayPort 1.4 and a generous three HDMI ports including one HDMI 2.0 port, plus an audio output jack. There are a pair of built in 2W speakers, but they are complete rubbish in case you were wondering. Seriously they are so bad we donâ\\x80\\x99t know why Viotek included them.The other complaint we have in regards to the design is Viotekâ\\x80\\x99s continual use of four buttons along the bottom edge to control the on screen display, rather than a much easier directional toggle. Combined with the wobble in the stand, itâ\\x80\\x99s pretty hard to navigate the OSD, where youâ\\x80\\x99ll find features like cheat crosshairs, picture in picture and color controls. And itâ\\x80\\x99s worth mentioning that FreeSync is disabled by default in the settings, so make sure you turn that on.As for the panel itself, our main concern going into this review was viewing angles, as it continues to be the biggest issue for TN monitors, particularly at the higher-end of the TN scale.Weâ\\x80\\x99ve gotta say, this is one of the better TN displays weâ\\x80\\x99ve seen in terms of viewing angles, especially horizontally where there is some contrast shift but it isnâ\\x80\\x99t too bad. However vertically thereâ\\x80\\x99s still a significant change in contrast from viewing at off angles, you will still want to view it dead on and make sure you have it tilted correctly to get the best experience. So while itâ\\x80\\x99s good for a TN, viewing angles are still several steps behind a VA and especially an IPS panel.The other area where TNs tend to suffer is contrast ratio, though the GFT27DB is on the upper end of the TN scale with this new AU Optronics panel. Native contrast ratio is a touch under 1000:1, sitting around the 970:1 mark, which is as good as it gets for a TN. This is in line with IPS panels but does fall well behind a typical VA, where you can expect more than double the contrast ratio for something like the GN27D.Brightness, it all checks out. This monitor significantly exceeds Viotekâ\\x80\\x99s claims. They list a 220 nit peak brightness but it performs well above that; we recorded a maximum brightness of slightly under 350 nits, which is pretty bright and a good result for a TN.Response times are clearly the big selling point of TN monitors; you buy one because you want it to be fast, and fast for gaming in particular. Viotek claims a 1ms response time with overdrive and a 3ms â\\x80\\x9cstandardâ\\x80\\x9d response, which we have to say holds up pretty well with what we found during testing.The optimal overdrive setting here is the maximum â\\x80\\x98Highâ\\x80\\x99 mode, which does have a small amount of overshoot with some transitions but it was within our tolerances and shouldnâ\\x80\\x99t matter all that much. With this mode enabled, the average grey to grey transition was 3.38ms, a typically fast result for a TN, pitting it well ahead of VA alternatives. With the absolute best VAs sitting above 5ms on average, with 8ms a more realistic figure, this TN is noticeably clearer with less blur and less ghosting.The general behaviour of this TN isnâ\\x80\\x99t anything unusual either. Rise times are much slower than fall times, with an average rise of 4.91ms thatâ\\x80\\x99s impacted by a relatively slow black to white transition. Fall times are lightening quick though, at under 2ms, with some transitions hitting that 1ms claimed figure. In general the panel is much faster than the needed 6.94ms for refreshing at 144 Hz, so youâ\\x80\\x99re getting the full benefit of the high refresh here, combined with good clarity.Input lag is also very good, we recorded around 4ms of latency which when combined with the fast transition time makes the GFT27DB a very fast monitor. Youâ\\x80\\x99d expect this from a gaming-focused TN monitor and Viotek has delivered.In terms of color performance, this TN really punches above its weight in a few areas. Weâ\\x80\\x99re getting a comfortable 100% sRGB coverage but itâ\\x80\\x99s the 92% DCI-P3 coverage that really impressed me, thatâ\\x80\\x99s not quite at the level weâ\\x80\\x99d expect from a wide gamut professional monitor but itâ\\x80\\x99s a few percentage points higher than many of the wider gamut VA panels weâ\\x80\\x99ve tested recently from Samsung. Those panels typically clock in between 85 and 90 percent DCI-P3 so for this TN to beat that is certainly quite impressive, as most TNs are not wide gamut whatsoever. Native 8-bit support is also great to see considering many TNs are 6-bit + FRC.Unfortunately Viotek leaves this wide gamut support unclamped, which means that without calibration, the display is oversaturated when viewing sRGB content. And sRGB content is 99% of what youâ\\x80\\x99ll view outside of professional workflows. If you love or donâ\\x80\\x99t mind oversaturation and extra vibrant colors, thatâ\\x80\\x99s great, this panel will look amazing, but if youâ\\x80\\x99re after accuracy unfortunately out of the box the GFT27DB wonâ\\x80\\x99t deliver.Default PerformanceWhat weâ\\x80\\x99d really like to see and what we recommend for all wide gamut monitors, is a simple switch in the on-screen display that toggles between an sRGB and a wide gamut state. Viotek doesnâ\\x80\\x99t provide this, and the result is a saturation deltaE average of 3.18 along with a ColorChecker deltaE of 3.43. Greyscale results are a little off as well with our unit having a slight red tint out of the box along with a deltaE average of 3.60, although gamma is good.If youâ\\x80\\x99re wondering about DCI-P3 accuracy Viotek doesnâ\\x80\\x99t really deliver here either, with deltaEs between 2.5 and 4.0 for the most part. Again it seems like the panel is not calibrated at all at the factory which is typical for a Viotek display and many gaming monitors in general. Were this for professional work, weâ\\x80\\x99d expect better results.Calibrated PerformanceWithout the ability to clamp the gamut thereâ\\x80\\x99s not a whole lot you can do in the on-screen display to improve performance, aside from some minor tweaks to correct the white point. To get accurate results you really need to perform a full calibration, which as always we did with the help of our good friend, SpectraCALâ\\x80\\x99s CALMAN 5. Aside from a couple of outliers, this calibration led to good results with deltaE averages below 1.0. However, like with a typical calibration of a TN display, its contrast ratio did drop to around 900:1, a minor drop but worth mentioning nonetheless.Lastly we have uniformity, performance is decent, better than your average VA panel but not perfect. When looking at this chart weâ\\x80\\x99d basically class the uniformity here as two blobs on either side of the panel with a slight dip in the center and around the edges. Unlike a lot of curved VAs though, thereâ\\x80\\x99s no noticeable vignette effect which is a good thing.Who Is It For?The Viotek GFT27DB is an interesting product, especially because itâ\\x80\\x99s a rare TN with a wide color gamut. The wide gamut does some of the work: in its default state it will be oversaturated, more so than a typical TN, and in line with todayâ\\x80\\x99s VAs, which allows for either 92% DCI-P3 coverage for those that want it, or vibrant sRGB if youâ\\x80\\x99re into that sort of thing.But itâ\\x80\\x99s the true 8-bit panel and the better than usual viewing angles that contribute a lot as well. Itâ\\x80\\x99s in all three of these areas â\\x80\\x93 bit depth, viewing angles and gamut â\\x80\\x93 that TNs usually fall behind but AU Optronics and Viotek have focused on improving these and the results have paid off. And this is without impacting response times, which are still as fast as youâ\\x80\\x99d expect for a TN.This panel looks pretty close to a mid-tier IPS and itâ\\x80\\x99s only in viewing angles where an IPS pulls ahead noticeably. For a TN weâ\\x80\\x99re genuinely impressed with the image quality. But when you combine the viewing angle issue with a contrast ratio of under 1000:1, youâ\\x80\\x99re still not getting a VA-like experience with those beautiful deep blacks. Contrast and viewing angles are very good for a TN, and we think for most people will be perfectly fine, but those who love VAs should stick to that.The GFT27DB also runs into the standard set of issues with Viotek monitors: the stand has limited adjustability and is a bit wobbly, the on-screen display is hard to control, the display is not calibrated, thereâ\\x80\\x99s no sRGB mode... and itâ\\x80\\x99s not available outside of a small handful of countries. While we do spend some time on calibration as part of the review process, most gaming monitors donâ\\x80\\x99t come calibrated, so that's the least important issue. As we often hear complaints in the user comments though, we wish Viotek would expand into the European and Australian markets though.So who would we recommend this monitor to? Itâ\\x80\\x99s an easy buy recommendation for those that want a fast gaming monitor. Itâ\\x80\\x99s 1440p and 144Hz with FreeSync, which we feel is still the sweet spot for PC gaming in 2019, and because itâ\\x80\\x99s a TN, ghosting is not an issue. Combined with the best colors weâ\\x80\\x99ve seen for this type of monitor, itâ\\x80\\x99s basically the best gaming TN on the market, so kudos to Viotek for that.It's a unique monitor in that no other company is using this panel just yet, and we donâ\\x80\\x99t feel this is reflected in the pricing either. At $330 itâ\\x80\\x99s the same price as Viotekâ\\x80\\x99s equivalent curved VA, with both the VA and TN each having strengths and weaknesses. The VA still has superior colors, but the TN is flat and itâ\\x80\\x99s faster. Both are fantastic value, so which option is better will depend on what you want out of a display. Just donâ\\x80\\x99t immediately dismiss the GFT27DB because itâ\\x80\\x99s a TN, itâ\\x80\\x99s definitely a lot better than most TN displays of the past.\"],\n",
       "  [\"The Asrock DeskMini A300 is a tiny PC that takes advantage of Ryzen processors. Almost every custom designed mini PC that weâ\\x80\\x99ve seen to date has used Intel inside and while Intel CPUs are very good, they arenâ\\x80\\x99t the best choice for this kind of system. At least if you want to game or do any kind of 3D work, for that AMDâ\\x80\\x99s Ryzen APUs are unrivaled.So after dozens of Intel-based Beeboxes and DeskMini PCs, Asrock has finally developed an AM4 socket system for Raven Ridge (and Bristol Ridge APUs). Ideally you want to throw in the Ryzen 3 2200G or Ryzen 5 2400G inside this system, our current top budget CPU picks that also happen to have more than decent integrated graphics capabilities.With a recent price drop, for $135 the Ryzen 5 2400G is a cracking good processor and only those not in the know would buy the Core i3-8100 for $150 instead. We'll revisit the 2400G eventually at this price point as it brings SMT support, so twice as many threads than the 2200G for an extra $40. For now, we'll see how it performs in the new DeskMini A300, as this is the best APU you can pair with this compact 1.9L barebone.The DeskMini A300 costs a very reasonable $150 and for that investment you get a custom case that measures 155mm wide, 155mm deep and 80mm tall. Itâ\\x80\\x99s capable of housing two 2.5â\\x80\\x9d storage devices and up to a 46mm tall CPU cooler. Inside youâ\\x80\\x99ll find Asrockâ\\x80\\x99s A300M-STX motherboard which measures just 140 x 147mm.As you might expect for such a tiny motherboard itâ\\x80\\x99s not exactly brimming with features but you do get all the essentials. Front I/O includes a USB 3.1 Gen1 Type-C port along with a Type-A and around the back thereâ\\x80\\x99s an additional two USB ports, a 3.1 Gen1 Type-A and a 2.0 Type-A. Thereâ\\x80\\x99s a basic Realtek audio and Gigabit network solution, three M.2 ports, two for an SSDs and a third for a Wi-Fi module and then two laptop-style memory DIMMs supporting up to DDR4-2933 with Ryzen APUs.The display outputs include a single HDMI, DisplayPort and a legacy VGA port, and itâ\\x80\\x99s possible to use all three simultaneously for triple monitor configurations.There are two optional items, one is a very basic air-cooler which came with our review unit and the other is an M.2 Wi-Fi kit, this didnâ\\x80\\x99t come with our sample. There is a listing over on Newegg which doesnâ\\x80\\x99t include the cooler but does come with the Intel AC-3168 Wi-Fi kit for $150, so thatâ\\x80\\x99s a great option. After all, you can use the Wraith Stealth cooler that comes with your Ryzen APU, though be aware youâ\\x80\\x99ll have to remove the fan shroud, just the top cover with the AMD logo, this reduces the cooler's height by a few millimeters and has no impact on the cooling performance.Powering the DeskMini A300 is an included 120w power brick, itâ\\x80\\x99s a 19v, 6.32A version and thatâ\\x80\\x99s plenty of headroom for using something like the Ryzen 5 2400G.Thatâ\\x80\\x99s the barebone. What you need to bring to the table is a CPU, we recommend either the 2200G or 2400G and then some kind of storage. I recommend something cheap such as the WD Blue 500GB which costs just $68 or if you can find the Plextor S2G 512GB at $55, either would work well for this build. You can fill the two 2.5â\\x80\\x9d drive bays with larger mechanical hard drives or additional SSDs, your choice there.Weâ\\x80\\x99ll discuss more hardware configurations towards the end of the review, for now letâ\\x80\\x99s see what kind of gaming performance the DeskMini A300 paired with the Ryzen 5 2400G has to offer. We wonâ\\x80\\x99t be delving into application performance as nothing has changed since our review of this CPU. Gaming performance, on the other hand, has seen improvements through updated drivers.For testing we used G.Skillâ\\x80\\x99s Ripjaws DDR4-2133 CL15 16GB for a simple reason: itâ\\x80\\x99s cheap at just $80. G.Skillâ\\x80\\x99s DDR4-3200 memory costs upwards of $130 and while it will offer a nice performance gain when using the Vega 11 GPU, we can overclock the DDR4-2133 memory, and thatâ\\x80\\x99s exactly what I did.Setting the frequency to 2933 MHz reduced the timings to CL16-21-21-21-49. You could no doubt manually tune those timings for even better performance, but I wanted to test something that was closer to the out of the box experience.Gaming Performance ImpressionsApex LegendsFirst up we have Apex Legends and here we were forced down to 720p with the lowest possible quality settings. At times the performance was deceiving as frame rates went above 100 fps but others they would dip to half that. Still overall we were looking at around 60-70 fps and performance was consistent for the most part. Bumping the resolution up to 900p was playable but the frame dips were far more noticable.Battlefield VMoving on we have Battlefield V and again we were forced down to 720p in search of 60 fps, and this kind of frame rate is required if you hope to rack up a few kills in the multiplayer modes. With the low quality preset enabled we typically saw around 60 fps with dips into the 50s, but overall a smooth and enjoyable experience, certainly playable by entry level PC standards.CS:GOWhen testing potato-like gaming performance youâ\\x80\\x99ve always gotta include CS:GO and for this one we were able to test at 1080p using high quality visuals, though thatâ\\x80\\x99s not saying much. Frame rates dipped into the 40s but were often up around 60 fps, needless to say with a few quality tweaks maintaining over 60 fps wonâ\\x80\\x99t be an issue.Far Cry New DawnUsing the â\\x80\\x98normalâ\\x80\\x99 quality preset at 720p frame rates fluctuated between 35 and 45 fps. There weâ\\x80\\x99re at times quite serious frame stutters but the game was still playable and I guess youâ\\x80\\x99ve sort of got to expect that kind of thing when gaming with integrated graphics.FortniteNext up we have Fortnite and for this one we went with the low quality settings at 900p and that allowed for at least 75 fps though most of the time we were looking more at 90-100 fps. Sadly though the experience was spoiled somewhat by fairly regular frame stuttering.Rainbow Six SiegeOne of the best experiences we had was with Rainbow Six Siege, using the lowest quality preset the resolution was set to 900p, though with TAA a 50% render scale is set. The game looked very good and played buttery smooth, I couldnâ\\x80\\x99t blame any missed short on frame stutters this time. Frame rates stayed above 60 fps at all times and generally hovered around 80fps, again it was a great experience and a perfect example of the fun that can be had with this compact system.Rocket LeagueWe fired up Rocket League and enabled the â\\x80\\x98performanceâ\\x80\\x99 preset at 1080p and this generally saw frame rates hovering between 70-80 fps allowing for smooth highly playable performance. As we saw with CS:GO itâ\\x80\\x99s possible to play these less demanding, but still highly popular titles, at very respectable quality settings.Power & TempsAs mentioned earlier the DeskMini A300 comes with a 120w power supply which is fine for use with the Ryzen 5 2400G provided you donâ\\x80\\x99t go crazy with overclocking. Actually, you won't be able to overclock anyway as youâ\\x80\\x99ll be limited by thermals.In its stock configuration using the 2400G with a pair of 1TB mechanical HDDs and a 2TB SSD, the DeskMini A300 consumed 82 watts in our Blender stress test and 102 watts when playing Battlefield V multiplayer. So there really isnâ\\x80\\x99t much headroom left for overclocking.Looking at operating temperatures using the optional cooler the Ryzen 5 2400G hit 78 degrees under load in our Blender stress test, but idled at just 33 degrees. This is a reasonable operating temperature and surprisingly this little cooler didnâ\\x80\\x99t make much noise. Replacing the base cooler with the Wraith Stealth only dropped the load temperature by 4 degrees, though the DeskMini A300 was basically silent now. So if you have a Stealth cooler and you're happy to take off the top cover then I recommend using it in the A300.Closing RemarksAsrockâ\\x80\\x99s new DeskMini A300 is everything we hoped it would be and we're glad they finally released a mini PC that supports Raven Ridge APUs. The gaming performance wonâ\\x80\\x99t blow your socks off, but you can at least play in some capacity, this simply wasnâ\\x80\\x99t possible beyond flash-based games on previously seen Intel models.If youâ\\x80\\x99re after a compact gaming rig and you donâ\\x80\\x99t want to buy a console for gaming, a fully functioning PC like the DeskMini A300 can fit the bill nicely. We do recognize this is a very niche product, you have to be in the market for an extremely small PC that can handle some light 3D work.If youâ\\x80\\x99re simply after a cost-effective gaming rig then this isnâ\\x80\\x99t it. Even though the barebones is priced modestly, the A300â\\x80\\x99s inability to support a discrete graphics card makes your only upgrade option here a Zen 2-based APU.Budget Build OptionsAlternatively you could build your own Ryzen 5 2400G system in a larger MicroATX case for a few dollars more and this gives you the ability to snap up something like a Radeon RX 560 or perhaps even one of those insanely cheap RX 570 models and this $100-150 discrete graphics card upgrade will improve gaming performance tenfold.Or for a slight price premium itâ\\x80\\x99s also possible to build a Mini-ITX system, again with the ability to support discrete graphics cards. But in the end, if you seek a super compact Mini PC then we feel there is no better option right now than the DeskMini A300.\"],\n",
       "  ['Following our coverage into Nvidiaâ\\x80\\x99s laptop RTX GPUs, so far we\\'ve looked at the GeForce RTX 2070 Max-Q and the RTX 2060 for laptops. Today we\\'re reviewing the RTX 2080 Max-Q which is supposed to be a decent amount faster than the GPUs weâ\\x80\\x99ve looked at so far.As an \"RTX 2080\" part, this discrete GPU is using a Turing die complete with 2944 CUDA cores, 368 Tensor cores and 46 ray tracing cores. But thatâ\\x80\\x99s where the similarities between the RTX 2080 Max-Q and the desktop RTX 2080 end. To bring what is normally a 215W GPU down into the 80W range for slim and light laptops, Nvidia has had to significantly underclock the GPU, with the base clock now sitting at just 735 MHz compared to 1515 MHz, while the boost clock hits 1095 MHz.Weâ\\x80\\x99ve talked about how Nvidiaâ\\x80\\x99s laptop GPU naming scheme for the GeForce 20 series can be a bit misleading. While Nvidia has differentiated this part from the regular desktop card through its name, slapping Max-Q on the end to let you know this is optimized for smaller laptop cooling designs, we think itâ\\x80\\x99s a bit much to even suggest this is an RTX 2080-class GPU.The clock speed delta between the RTX 2080 desktop and RTX 2080 Max-Q is the largest weâ\\x80\\x99ve seen so far out of Nvidiaâ\\x80\\x99s Turing products. While the desktop card typically hits 1900 MHz, the RTX 2080 Max-Q reaches just 1200 MHz. So this GPU wonâ\\x80\\x99t perform anywhere near a desktop packing an RTX 2080, especially when factoring in other typical laptop bottlenecks.Thereâ\\x80\\x99s added confusion with this GPU because there appear to be two variants in the market: a standard, default 80W version that weâ\\x80\\x99ve been talking about so far, along with a higher clocked 90W variant. The 90W model has a base clock of 990 MHz and a boost of 1230 MHz, which is a significant jump. This increase typically materializes as a typical GPU clock around 1400 MHz.Both variants have the same memory configuration: 8GB of GDDR6 clocked at 12 Gbps on a 256-bit bus, leading to 384 GB/s of bandwidth. This is another area where the RTX 2080 Max-Q is underclocked compared to the desktop card, which sits at 14 Gbps on the memory.For testing we have both variants on hand. The regular RTX 2080 Max-Q is found in the MSI GS75 Stealth 8SG, a slim and light 17-inch notebook. This laptop offers an \\'overclocked\\' mode in the settings that boosts clock speeds by 100 MHz, allowing it to sit between the 80W and 90W variants, but for todayâ\\x80\\x99s review weâ\\x80\\x99re leaving the laptop at its default RTX 2080 Max-Q clocks. In the full review of the GS75 weâ\\x80\\x99ll detail how it performs in the OC mode.The 90W variant of the RTX 2080 Max-Q is found in the Alienware m15, another slim and light system but this time 15-inches in size. This laptop struggles to make use of the 90W RTX 2080 Max-Q due to a constrained cooler solution, weâ\\x80\\x99ll detail more about this problem in our upcoming Alienware m15 review, but we did discover that raising the laptop a few centimetres does allow the GPU to operate at its maximum performance without overclocking, so thatâ\\x80\\x99s what weâ\\x80\\x99ve done for this review.Otherwise the two systems are very similar. Both use Intelâ\\x80\\x99s six-core gaming laptop CPU, the Core i7-8750H. Both have dual-channel memory, which is very important for unleashing the best performance in games -- you can see more than a 10% reduction to performance in some games when running in single-channel, so itâ\\x80\\x99s not something we advise.PerformanceLetâ\\x80\\x99s dig into the results, starting with Far Cry 5. Weâ\\x80\\x99ve decided to use a slightly different approach to showing this data than we have in the past. Rather than showing how our RTX 2080 Max-Q test laptops compare to other single laptop units, this data is an average of several laptops weâ\\x80\\x99ve tested with the listed GPUs inside. To keep things as apples-to-apples as possible, the average only includes laptops with the same CPU and same dual-channel memory configuration. For example, our data for the GTX 1070 Max-Q contains an average of three laptops, which should be more representative of a â\\x80\\x98typicalâ\\x80\\x99 laptop that uses that GPU.Far Cry 5 is a title that is a fairly typical representation of most modern games. The RTX 2080 Max-Q sits at the top of this chart, although noting we havenâ\\x80\\x99t got any current, useful data for the older GTX 1080 or GTX 1080 Max-Q.The base RTX 2080 Max-Q sneaks ahead of the GTX 1070, but only by 2 percent. Itâ\\x80\\x99s also only five percent ahead of the RTX 2070 Max-Q and less than 10 percent faster than the RTX 2060. However the situation is better for the 90W variant, which delivers an additional 6 percent over the 80W RTX 2080 Max-Q.Battlefield V is a little more favourable to the new RTX 2080 Max-Q. With this GPU sitting 5% ahead of the GTX 1070, however the 90W variant is only 3 percent faster than the 80W variant. You will get 9 percent more performance with the base RTX 2080 Max-Q over the RTX 2070 Max-Q.Metro Exodus is the newest title in our test suite, and it presents one of the largest gains between the two RTX 2080 Max-Q models: a 17% performance uplift. While the standard variant is only 1% faster than a GTX 1070, the 90W variant is a huge 18% faster. This is a game where youâ\\x80\\x99ll really want that higher TDP model.Star Wars Battlefront II is a good result for the RTX 2080 Max-Q: 4% faster than the GTX 1070, 7% faster than the RTX 2070 Max-Q, and 10 percent faster than the RTX 2060. The 90W model also delivers a 10% higher frame rate than the base model, which is decent.Middle-earth: Shadow of War is highly favourable to Turing compared to Pascal, showing some of the largest margins. The RTX 2080 Max-Q is 16% faster than the GTX 1070 here, while also being 13% faster than the RTX 2070 Max-Q, and 25% faster than the RTX 2060. And despite these large margins, the 90W model is 10% faster than the 80W model, delivering even better performance.Watch Dogs 2 is a more CPU demanding game, which is why the 1% low results are a little unusual. You also wonâ\\x80\\x99t see much to gain at the high end, with the RTX 2080 Max-Q, the 90W variant, and the GTX 1070 performing around the same. Itâ\\x80\\x99s a step down to the other GPUs, for example the RTX 2080 Max-Q is 13% ahead of the RTX 2070 Max-Q.Deus Ex: Mankind Divided is an older title in our test suite and again, a few weird 1% low numbers here due to some laptops having issues with this title. Thereâ\\x80\\x99s a nice cadence between each laptop GPU however, with the 90W RTX 2080 Max-Q delivering a handy performance increase over the 80W model, at least on average.Lots of people still play Grand Theft Auto V, and here the RTX 2080 Max-Q is on par with the GTX 1070, with the 90W 2080 Max-Q a smidgen ahead. With only a 10 FPS difference between the GTX 1070 Max-Q and the top of the chart, these results are very clumped together.Shadow of the Tomb Raider is the final game in today\\'s test suite, and itâ\\x80\\x99s another one with a few inconsistent frametime results depending on the laptop being tested. It seems the GTX 1070 is particularly strong in this game. With that said thereâ\\x80\\x99s a bit to gain from both RTX 2080 Max-Q models when looking at average frame rates.Individual MatchupsRTX 2080 Max-Q vs. GTX 1070Now itâ\\x80\\x99s time to expand into a number of extended comparison charts which show all the other games weâ\\x80\\x99ve been testing. Weâ\\x80\\x99ll start with what we feel is the most relevant comparison: the RTX 2080 Max-Q versus the GTX 1070.The GTX 1070 was the sweet spot between price and performance for last-generation Pascal systems, and most laptops that used it werenâ\\x80\\x99t outrageously massive. Here the RTX 2080 Max-Q in its standard configuration pulls 5% ahead on average, though it doesnâ\\x80\\x99t win in every game. A fairly modest improvement.The 90W variant fares a little better, with a 12% average performance gain over the GTX 1070, though again it doesnâ\\x80\\x99t win in every title, especially in more CPU demanding games like Assassinâ\\x80\\x99s Creed Odyssey.RTX 2080 Max-Q vs. GTX 1070 Max-QCompared to the GTX 1070 Max-Q, the RTX 2080 Max-Q does see a handy 23% performance uplift, so if you are upgrading your slim and light laptop and jumping up a performance bracket at the same time, this is the sort of margin you can expect. Itâ\\x80\\x99s a decent uplift, but itâ\\x80\\x99s nowhere near the 50 to 60 percent performance gain you can expect if you upgraded from a desktop GTX 1070 to an RTX 2080.RTX 2080 Max-Q vs. GTX 2070 Max-QLooking at the RTX 2080 Max-Q versus the RTX 2070 Max-Q, there is a bit to be gained from opting for the faster RTX 2080 model in your ultraportable. The standard variant is 10% faster on average, and that jumps up to 17% faster when comparing the 90W variant to the RTX 2070 Max-Q base variant.RTX 2080 Max-Q vs. RTX 2060 (Laptop)Decent gains in this matchup. On average you can expect a 16% boost and up to 23% when using the 90W model. RTX 2080 Max-Q laptops are considerably more expensive, but at least they\\'re also faster.RTX 2080 Max-Q vs. RTX 2080 Max-Q (90W)Finally, we have the two RTX 2080 Max-Q models compared. Those who can find a 90W variant in a laptop should be treated to around 7% more performance, although there are some games where performance isnâ\\x80\\x99t significantly better.Putting It All TogetherLooking at all this data, there is a lot to break down, so be prepared for an extended conclusion.The first positive is that the RTX 2080 Max-Q is faster than the GPUs you would expect it to beat. Itâ\\x80\\x99s also faster than other ultraportable-focused GPUs like the RTX 2070 Max-Q and GTX 1070 Max-Q by at least 10 percent, so you are not paying a lot more money only to get a single-digit upgrade. There are gains to be had at 1080p.Based on this data we expect the RTX 2080 Max-Q to be slightly faster than the GTX 1080 Max-Q in its standard configuration, with the 90W variant delivering around a 10% boost. Nothing earth-shattering though.With those positives aside, there are three matters we want to discuss: the two variant situation, the performance margins, and the pricing of RTX 2080 Max-Q laptops.Having two GPUs with different clock speeds and power limits, ultimately translates into different performance, but giving them the same name creates confusion. It makes it hard for consumers to know what they are buying when they see an RTX 2080 Max-Q laptop. Are they getting the slower standard version, or the faster 90W version? There is no way to know.Looking at the product pages for the two laptops we just tested highlights the problem. On the MSI GS75 Stealth page, it tells you that you are getting an RTX 2080 Max-Q, but thereâ\\x80\\x99s no information on clock speeds. Same thing on the Alienware m15 page. For a prospective buyer looking at these laptops, there is no indication that the Alienware m15 includes the 90W variant and is therefore 7% faster on average. Potential buyers of these laptops are not regular PC users either, but gamers and enthusiasts who will want to know their hardware beforehand.By the way, this isnâ\\x80\\x99t an issue with cooler design or throttling. Normally youâ\\x80\\x99d expect some variances between laptops due to coolers and other hardware. But even if you supercharge the GS75â\\x80\\x99s cooling solution, there is no way to make the GPU hit the performance levels of the 90W variant, because it hits the hard-coded, unchangeable power limit of its variant before thermals become an issue.We could go on about this, and there are simple solutions to the problem: different naming for each variant, or use the 90W part everywhere and let it throttle depending on the laptop model.Of course, Nvidia didn\\'t go for either of those which makes us wonder if they are intentionally doing this. That is, giving away less information about their mobile line-up in general and playing the branding card, because after all they have no direct competition in this segment.We do understand the motive behind having two variants. Having more GPU options allows a laptop maker to to choose a product that suits their cooler design best. But there is no reason why the customer shouldn\\'t know this difference. It could even benefit the manufacturers who design better coolers, because they could advertise their laptop has the more powerful GPU version inside. Nvidia, weâ\\x80\\x99re going to call you out every time you try and pull this stuff, whether itâ\\x80\\x99s for desktop cards like the GT 1030 DDR4, or for laptop GPUs. Just stop it.The next issue to discuss is the performance margins. At face value thereâ\\x80\\x99s not much wrong with how the line-up stacks up. In the same thin and light gaming form factor, Turing GPUs are providing about 10% more performance than their direct Pascal predecessor, while also maintaining a 10% barrier between the RTX 2070 and RTX 2080 Max-Q models like there was for the GTX 1070 Max-Q and GTX 1080 Max-Q. Considering laptops are highly constrained by size and power, this sort of improvement isnâ\\x80\\x99t bad given Turing is only a minor efficiency upgrade.The problem is how the laptop ecosystem compares to the desktop ecosystem, and itâ\\x80\\x99s something weâ\\x80\\x99ve talked about before. These Turing laptop GPUs are nowhere near the same performance levels as the desktop version, but more importantly, the margins between each model are not the same.On the desktop, the RTX 2080 is about 20% faster than an RTX 2070. Here with the standard Max-Q models, itâ\\x80\\x99s only 10%. On the desktop, the RTX 2080 is a massive 50% faster than a GTX 1070. Here with the Max-Q models, itâ\\x80\\x99s 20-30% faster. And youâ\\x80\\x99ll find similar situations with basically any other comparison you want to make.This is bound to confuse consumers, with desktop GPUs with outright identical names performing much faster. On the other hand, this situation makes reviews like this one a must-read for those concerned about performance. So thank you Nvidia, we guess.As for pricing, laptops with Nvidiaâ\\x80\\x99s top-end GPUs have never been cheap, especially portable versions using a Max-Q model. But with the 2080 Max-Q in particular weâ\\x80\\x99re reaching crazy territory.Take the Alienware m15 as an example. The RTX 2060 variant is $2,000, but itâ\\x80\\x99ll cost you can extra $850 to get the RTX 2080 Max-Q. Thatâ\\x80\\x99s a 42% increase for 23% more performance. MSI is charging $400 or a 17% increase to go from an RTX 2070 Max-Q to an RTX 2080 Max-Q, despite the upgrade only providing 10% more performance.High-end products always come at a premium, so we canâ\\x80\\x99t expect price to performance scaling throughout the entire product stack. But top-tier GPUs certainly are very expensive this generation, especially with Pascal options still in the market. When a decent GTX 1070 Max-Q laptop costs just $1,700 compared to an RTX 2080 Max-Q laptop for $2,800 or more, itâ\\x80\\x99s really hard to justify spending that extra money on the Turing option.'],\n",
       "  [\"Considering the popularity and widespread use of the GeForce GTX 1060 in gaming laptops, the new RTX 2060 is set to be one of the key offerings in Nvidiaâ\\x80\\x99s latest lineup. We recently reviewed the RTX 2070 Max-Q and spent some time talking about how Nvidiaâ\\x80\\x99s naming scheme for their laptop GPUs can be very misleading for prospective buyers. Thatâ\\x80\\x99s still the case with the RTX 2060, but we're not harping on about this issue. Just know that in this generation, the RTX GPUs are named the same but performance will be slower than their desktop counterparts.The RTX 2060 GPU used in laptops uses the same TU106 GPU as the desktop card, with the same core configuration. That means you get 1920 CUDA cores, 240 Tensor cores and 30 RT cores. It also has the same memory configuration: 6GB of GDDR6 at 14 Gbps on a 192-bit bus for 336 GB/s of bandwidth. Itâ\\x80\\x99s still built on TSMCâ\\x80\\x99s 12nm process, too.However the laptop RTX 2060 isnâ\\x80\\x99t clocked anywhere near what the desktop card can achieve. The desktop card has a base clock of 1365 MHz and a boost of 1680 MHz, with GPU Boost taking the GPU even higher. The laptop variant is clocked at just 960 MHz with a boost of 1200 MHz -- the boost clock is lower than the desktop's base clock -- Nvidia was forced to do this to shave the TDP down from 160W, to 80-90W which is more suitable for laptop designs.Evidently, you wonâ\\x80\\x99t get desktop RTX 2060 performance from the laptop variant, despite both having the same name. Turing laptop GPUs are also more aggressively underclocked relative to the desktop cards than Pascal ever was, so we're not expecting to see the same margins or performance improvements as on the desktop. We'd much prefer if this GPU was called the RTX 2060M instead, but enough on that...For benchmarking the RTX 2060 weâ\\x80\\x99ve used the new Asus ROG Strix Scar II GL504GV. A mouthful indeed, but this Asus laptop is essentially this a typical 15-inch gaming notebook. Itâ\\x80\\x99s not a slim and lighter machine like the Asus Zephyrus, just a regular design thatâ\\x80\\x99s 25mm thick and about 5.3 lbs heavy. Thereâ\\x80\\x99s no Max-Q RTX 2060 model, so this full GPU is most suited to these designs.The ROG Strix Scar II is powered by Intelâ\\x80\\x99s six-core Core i7-8750H, the go-to offering for gaming laptops these days. The same CPU is used in our Pascal laptops and other RTX laptops weâ\\x80\\x99ve tested so far, meaning we get to see a more straight comparison looking at GPU performance specifically.This Asus laptop comes configured with 16GB of RAM in a single-channel configuration, but for testing we swapped that out for 32 GB in dual-channel to keep it apples-to-apples with other laptops in our database. We also used the laptopâ\\x80\\x99s â\\x80\\x9cbalancedâ\\x80\\x9d fan mode because we found no difference in gaming performance between that and the louder â\\x80\\x9cTurboâ\\x80\\x9d mode. Finally, weâ\\x80\\x99ve done all testing at 1080p, the standard resolution for gaming laptop displays, with this unit packing a 144 Hz panel.BenchmarksWeâ\\x80\\x99re going to kick this one off with a look at Battlefield 1. Itâ\\x80\\x99s not the most recent Battlefield title but itâ\\x80\\x99s one that plays well on laptops. The RTX 2060 manages to squeeze ahead of the GTX 1070 Max-Q, providing over 100 FPS on average with a similar 1% low. Itâ\\x80\\x99s also 28% faster than the GTX 1060 6GB in this title. However it canâ\\x80\\x99t quite match the previous generation GTX 1070, falling 10 percent behind the older Pascal GPU.If youâ\\x80\\x99ve been following our desktop coverage of the RTX series, these results might seem a little strange. On the desktop side, the RTX 2060 is more than 50% faster than the GTX 1060 in this title, and more than 10% faster than the GTX 1070. However on the laptop side, the margins are much smaller and the comparisons are very different, with the new Turing GPU failing to beat the GTX 1070.Most gamers looking at the RTX 2060 on desktop might believe their new RTX 2060 laptop will outperform an older GTX 1070 model, which is again why we feel the naming is misleading. While the RTX 2060 easily clocks above 1800 MHz on the desktop, in our Asus laptop it sits around 1450 MHz, hence the large performance discrepancy. Meanwhile our laptop GTX 1070 clocks at 1800 MHz, compared to slightly over 1900 MHz on the desktop. Pascal laptop GPUs simply clocked a lot closer to their desktop counterparts than these new Turing GPUs do.Thatâ\\x80\\x99s not to say performance is bad overall. In Wolfenstein II, which is very favorable to Turing's architecture, the RTX 2060 comes in 34% ahead of the GTX 1060 although it still gets beaten badly by the GTX 1070.Watch Dogs 2 is one of those games that tends to be more CPU constrained in a laptop, although weâ\\x80\\x99re still getting a handy 25 percent performance uplift in this older Ubisoft title. This is one of the few games we tested where the RTX 2060 performed below the GTX 1070 Max-Q.With Far Cry 5 weâ\\x80\\x99re looking at a 25% performance improvement over the GTX 1060. However this time it gets to outperform the GTX 1070 Max-Q by 6 percent and only slotting in 3 percent behind the RTX 2070 Max-Q. With the RTX 2060 consistently delivering above 60 FPS at Ultra settings here, this is a handy improvement on the GTX 1060.Resident Evil 2 is one of the newer games in our test suite. Using the Balanced quality preset, which is nearly visually identical to the maximum preset, laptops in general perform really well, achieving over 100 FPS for the most part.The RTX 2060 is 35% faster than the GTX 1060, only 7% slower than the GTX 1070 and a decent 11% faster than the GTX 1070 Max-Q.Hitman 2 is a punishing title for CPU constrained hardware, but current-generation laptops are still capable of a 60 FPS experience at 1080p using maximum quality settings. The RTX 2060 is 23% faster than the GTX 1060 here, but other margins are in line with what weâ\\x80\\x99ve previously seen. Impressively, the RTX 2060 is 4% faster than the RTX 2070 Max-Q in this title.Dirt 4 sees the RTX 2060 hold a 37% performance advantage over the GTX 1060, while still coming in 12 percent behind the GTX 1070 and 10 percent behind the newer RTX 2070 Max-Q. Even with 8xMSAA in this game at 1080p, youâ\\x80\\x99re in for a rock solid experience.Probably time to throw an older title into the mix. How about Deus Ex: Mankind Divided? This game has one of the smallest margins between the RTX 2060 and the GTX 1070, with the 2060 only 7 percent behind. Itâ\\x80\\x99s also 8 percent faster than the GTX 1070 Max-Q, and 36% ahead of the GTX 1060 6GB.Youâ\\x80\\x99re probably wondering about Battlefield V performance as well. Here the new RTX 2060 is 23% faster than the GTX 1060. At 1080p, paired with a high-refresh laptop display, the RTX 2060 is very capable and delivers a great experience.Rounding this performance comparison out, weâ\\x80\\x99ve settled on Prey. This represents a game that really doesnâ\\x80\\x99t benefit all that much from the faster GPU, itâ\\x80\\x99s only 13% faster than the GTX 1060, 7% slower than the GTX 1070 and 7% faster than the GTX 1070 Max-Q.Assassinâ\\x80\\x99s Creed Odyssey is one of the hardest games to get running smoothly on a laptop, we think itâ\\x80\\x99s down to the CPU and for some reason Gigabyteâ\\x80\\x99s Aero laptops get destroyed in this title. That said, with the Very High preset youâ\\x80\\x99re looking at a 60 FPS average at 1080p with the RTX 2060.The final game in our comparison: Middle-earth Shadow of War has the RTX 2060 performing 32% faster than the GTX 1060, 10% slower than the GTX 1070 and 9% behind the RTX 2070 Max-Q.Individual MatchupsWe ran additional performance summaries putting GPUs head to head in all the games we tested. We actually benchmarked a total of 20 games on the RTX 2060 and refreshed the data for a number of other GPUs just for this comparison.RTX 2060 Laptop vs. GTX 1060 6GBThe big one here albeit probably not the best comparison in terms of value and pricing, which weâ\\x80\\x99ll talk about later. The RTX 2060 is a good 28% faster than the GTX 1060 6GB on average. Thatâ\\x80\\x99s a much bigger margin than we were expecting after the RTX 2070 Max-Q review which only clocked in around 10% faster than its direct predecessor, the GTX 1070 Max-Q.RTX 2060 Laptop vs. GTX 1070 Max-QSpeaking of the GTX 1070 Max-Q, the RTX 2060 is also faster than that GPU, to the tune of a 6% average. Itâ\\x80\\x99s not faster in every title, but itâ\\x80\\x99s only in older games where it appears to struggle. Anything recent and the RTX 2060 pulls away.RTX 2060 Laptop vs. GTX 2070 Max-QTo our surprise, the RTX 2060 isnâ\\x80\\x99t much slower than the RTX 2070 Max-Q on average. Weâ\\x80\\x99re talking 4% slower on average, with some games clocking in faster than what is supposed to be a higher tier GPU. This could place prospective RTX 2070 Max-Q buyers in an awkward position.RTX 2060 Laptop vs. GTX 1070Where the comparison isnâ\\x80\\x99t as favourable, is when comparing the RTX 2060 to the GTX 1070. The RTX 2060 is 11 percent slower on average, losing in every single game in a margin that goes from 2 percent to up to 20 percent.And this is where we see the weakness of Nvidiaâ\\x80\\x99s RTX laptop lineup.On the desktop the RTX 2060 is ~53% faster than the GTX 1060 6GB. But here on the laptop realm, itâ\\x80\\x99s only 28% faster. This means that instead of being 10+% faster than a GTX 1070, itâ\\x80\\x99s 11% slower. Not a good situation for those who might accidentally use desktop data for their laptop buying decision.Within a generation though, the comparisons make sense. The GTX 1070 is about 40% faster than a laptop GTX 1060, which is similar to the desktop margin. The RTX 2060 is 4% slower than the RTX 2070 Max-Q, and while we havenâ\\x80\\x99t tested the full laptop RTX 2070 yet, with desktop cards the 2060 is 11% slower than the 2070 on average. So the margins weâ\\x80\\x99ve seen so far keep the whole series in accord.Bottom LineIf youâ\\x80\\x99re looking for a direct desktop comparison to the RTX 2060 laptop GPU, itâ\\x80\\x99s hard to give a specific one, but so far performance looks to be between the GTX 1660 and the GTX 1060 Ti.Let's now focus on value and pricing of newer RTX laptops, and where they stand compared to previous generations. With the RTX 2060, weâ\\x80\\x99re looking at a typical price for a Core i7-8750H system of around $1,800. For much of the GTX 1060's life, you could find a typical laptop for $1,300. Thatâ\\x80\\x99s a 38% increase in price, for 28% more performance.We're quoting typical mid-generation GTX 1060 laptop pricing there. Today, many GTX 1060 laptops are available for less than $1,100 since they are on their way out.It doesnâ\\x80\\x99t get much better comparing the RTX 2060 to the GTX 1070. On the desktop, the RTX 2060 was priced higher to match the older GTX 1070, but this actually made sense as the RTX 2060 is a decent amount faster, so it's a better value and therefore the better buy in that price tier. But on laptops, itâ\\x80\\x99s not quite the same.We're seeing RTX 2060 laptops coming in around the typical price for a GTX 1070 system: $1,800. But the newer GPU is actually 11% slower. With some of older-gen laptops selling at a discount today, often sitting at $1,500 or less, itâ\\x80\\x99s a no brainer to choose the Pascal option. It's hard to predict what will happen when GTX 1070 laptops leave the market, but right now it doesn't make sense to go for an RTX 2060 at those prices.This situation also throws into question the RTX 2070 Max-Q. The average laptop with that GPU is ~$2,400, or 33% more than RTX 2060 laptops, for nearly no performance uplift. Of course, in this instance you're paying for the portability and often nicer build. To be completely fair, with Pascal the standard GTX 1070 laptops were considerably cheaper than slower GTX 1070 Max-Q units for the most part.Ultimately, we feel RTX 2060 laptops need to be priced closer to $1,500 or less to be a good value. But thatâ\\x80\\x99s a big price cut, which seems like a tough ask when competition does not abound.Do note there are a few RTX 2060 laptops you can buy for $1,400 right now, but those models are equipped with a quad-core Core i5-8300H CPU inside, not the standard six-core 8750H. Those may seem like a better deal, but are not equivalent and we still wouldnâ\\x80\\x99t buy them over a $1,500 GTX 1070 system.\"],\n",
       "  [\"Intel unveiled a new series of U-series laptop processors last year designed for ultraportables. We'll use the term â\\x80\\x9cnewâ\\x80\\x9d loosely here. These CPUs are codenamed Whiskey Lake, and theyâ\\x80\\x99re still 8th generation parts that are not radically different from the previous Kaby Lake Refresh chips that came before it. The main change is the move from Intelâ\\x80\\x99s 14nm+ to their 14nm++ process node, which has allowed slightly higher clock speeds within the same power envelope.The basic design of these CPUs is unchanged which is probably why they are still being called 8th-gen parts, rather than 9th-gen to fit in with Intelâ\\x80\\x99s current desktop line-up. There are only three SKUs: the Core i7-8565U and Core i5-8265U that are 4 core / 8 thread CPUs, while the Core i3-8145U is a dual-core part with 4 threads. All are 15W chips although the TDP can be configured anywhere from 10W to 25W depending on what the OEM wants.The focus of this review will be the Core i7-8565U, which is essentially the new flagship 15W CPU in Intelâ\\x80\\x99s line-up.This is a little confusing as previously there was a Core i7-8650U, but the 8565U is actually clocked higher, at a single core turbo clock of 4.6 GHz (up from 4.2 GHz) and an all core turbo of 4.1 GHz, up from 3.9 GHz. The base clock is a little lower though, at 1.8 GHz compared to 1.9 GHz. And these clock speed increases are even more favorable when comparing the i7-8565U to the i7-8550U which is a more like-for-like comparison going on the naming scheme; comparing those two CPUs gives at least an 11% boost clock advantage to Whiskey Lake.It seems that a lot of OEMs werenâ\\x80\\x99t super excited by Whiskey Lake because we didnâ\\x80\\x99t see a lot of laptop refreshes in 2018 that decided to use these new parts. It wasn't until recently during CES 2019 that more vendors are jumping on board, and I suspect thatâ\\x80\\x99s due to modest clock speed increases, making it less of an urgent or necessary upgrade.In preparation for new 2019 laptop releases weâ\\x80\\x99re going to be detailing how Whiskey Lake â\\x80\\x93 specifically the Core i7-8565U â\\x80\\x93 performs in comparison to a range of other laptop-class processors. This should give you a good idea of how this CPU stacks up, it wonâ\\x80\\x99t be a perfect reflection because laptop vendors can change a number of aspects including the cooler, memory configuration and TDPs which all impact performance, but what Iâ\\x80\\x99ll be showing today should be very close to what youâ\\x80\\x99ll see in most laptop implementations.Crucially, weâ\\x80\\x99ve tested the Core i7-8565U using the new Razer Blade Stealth which is an excellent test platform for a number of reasons.The new Blade Stealth uses the 25W maximum TDP configuration for this CPU, so weâ\\x80\\x99ll see how this chip performs in devices that choose this configuration and have larger coolers. Dell, for example, tends to use 25W for their XPS line. Then, using Intelâ\\x80\\x99s Extreme Tuning Utility, weâ\\x80\\x99ve also been able to set the CPU down to its regular 15W configuration, this is the most common configuration and reflects the majority of ultraportables that will use this CPU.Having both sets of data should give a pretty comprehensive look at how this processor performs.The Blade Stealth is also a good platform because it includes 16GB of dual-channel DDR4, again, a common configuration and dual-channel is key because the best performing laptops have dual-channel memory. On top of this, the laptop also has GeForce MX150 graphics, however for the purpose of our testing we've disabled the discrete graphics. Our full review of Razer Blade Stealth is coming up soon, where we'll test the actual performance of this laptop with its discrete GPU.A few other things we should mention about Whiskey Lake before the benchmark results... the GPU and cache configuration are unchanged compared to Kaby Lake Refresh. So weâ\\x80\\x99re still looking at UHD 620 graphics at up to 1150 MHz in the Core i7-8565U, along with 8MB of L3 cache. Typical PL2 power limits also appear to be unchanged, so weâ\\x80\\x99re still looking at short bursts using up to 44W with the 15W configuration, and 51W with the 25W configuration.BenchmarksStarting with Cinebench R15, the multi-threaded test is a relatively short benchmark but it has a decent mix of boost and steady state clock speed behavior. Despite higher boost clocks, the 8565U in its 15W configuration ends up only 3% faster than the 8550U in the multi-threaded test.The 25W configuration gets a healthier 11% boost, which is in line with the boost clock difference. Both configs are a fair bit faster in the single-threaded test though.To explain whatâ\\x80\\x99s going on here, itâ\\x80\\x99s worth looking at a clock speed comparison during the Cinebench run. Both the 15W and 25W configurations start off at their maximum all core Turbo clock speed, which is 3.7 GHz for the 8550U, and 4.1 GHz for the 8565U. However when the CPU reverts to its PL1 state, so itâ\\x80\\x99s no longer boosting any more, there is quite a difference in behavior.The 25W 8550U is sitting around the 2.6 to 2.7 GHz mark, however the 25W 8565U is up at 3.1 GHz, so thatâ\\x80\\x99s quite a healthy gain for the 8565U and contributes to the larger gain in performance. When looking at the 15W CPUs, the 8550U sits at 2.2 to 2.3 GHz compared to 2.3 to 2.4 GHz for the 8565U. There is a gain for the 8565U, but itâ\\x80\\x99s not as large as you get at 25W.Whatâ\\x80\\x99s apparent here is that the advantage that 14nm++ brings to Whiskey Lake isn't all that accessible with a tiny 15W power limit. You do get a decent jump in boost clock speeds, but when the CPU reverts to its long term PL1 power state, thereâ\\x80\\x99s not a lot to be gained from the 8565U. However at 25W, the taps are opened a bit more and Whiskey Lake can stretch its legs to provide a decent jump in performance.Looking at the Cinebench R15 performance chart again, itâ\\x80\\x99s also impressive to see where the 25W configuration is sitting among the pack. The 25W 8565U is almost as fast as the Core i7-7700HQ in the multi-threaded test, and it smokes it in the single-threaded test. The 7700HQ is a 45W quad-core designed for gaming laptops, so itâ\\x80\\x99s great to see that performance now available in ultraportable form factors.We see a similar situation in x264 encoding, the 25W 8565U is right up there with the 7700HQ, while the 15W configuration is providing up to an 8% gain over the 8550U.Handbrake x265 was a really interesting benchmark to run as it shows an even harsher reality for the 15W configuration of these CPUs. With this TDP limit, there was no difference in performance between the 8550U and 8565U, likely due to the use of AVX instructions that further limit what low-power CPUs can achieve.However with the 25W configurations, the 8565U is a good 17% faster, which is slightly above the difference in long term clock speeds. We also see that while the 8565U was close to the 7700HQ in previous tests, when AVX is required, the 7700HQ and other 45W CPUs begin to pull away.Adobe Premiere benefits strongly from GPU acceleration and the iGPU in these 15W CPUs is pretty weak. You can see that the top three CPUs that are paired with discrete-class graphics smoke the competition here. We also see the 15W 8565U fall slightly behind the 8550U, a strange result and the only benchmark where this was the case. That said, the 25W configuration is now 14% faster.Microsoft Excel is a workload that runs entirely within the PL2 boost state, so there is no difference in performance between the 25W and 15W configurations. The i7-8565U holds a small advantage over the 8550U, and both sit around the same mark as the 7700HQ, another impressive showing.MATLAB is another good result for the 8565U, with an 8% gain present with the 15W configuration over the 8550U, while the 25W config jumps that up to a 14% gain. Again in this short-burst, single-core workload thereâ\\x80\\x99s not a lot of difference between most of Intelâ\\x80\\x99s recent CPUs, and considering it also thrives on memory bandwidth where there has been virtually no improvement, weâ\\x80\\x99re left with a big clumping at the top of the chart.With 7-Zip we see small gen-on-gen improvements with Whiskey Lake again, mostly because this test is short and runs in the boost clock zone.Adobe Photoshop shows some of the largest gains between generations with the 15W SKU delivering 13% more performance and the 25W SKU showing gains of 26%, much higher than the average.Itâ\\x80\\x99s also worth looking at PCMark 10 where we see decent generational gains between each CPU again. The 15W SKU provides 12% more performance, which is in line with some of the single-threaded short burst workloads weâ\\x80\\x99ve seen, and thatâ\\x80\\x99s largely what PCMark tests.Briefly touching on GPU performance, thereâ\\x80\\x99s not a lot to say considering there are no changes to the GPU in Whiskey Lake compared to Kaby Lake Refresh. Big gains are expected for the next generation, but weâ\\x80\\x99re not getting anything here.Looking across our 3DMark workloads like Sky Diver, most of the gains youâ\\x80\\x99re seeing are from higher CPU scores, while looking at pure GPU scores there is next to no improvement. In more GPU intensive workloads, Whiskey Lake still gets handily beaten by AMDâ\\x80\\x99s Ryzen Mobile processors.Let's now look through some overall summaries of how the Core i7-8565U performs...On average, the 15W configuration of the 8565U is 8% faster than the 8550U, though these gains largely appear in either single-threaded workloads, short workloads, or some combination of the two. In longer workloads like encoding, you can expect less than a 5% performance improvement.When comparing 25W configurations, the gains are more significant. Here weâ\\x80\\x99re up to a 15% improvement on average with quite a healthy gain in longer workloads. This is more in line with the clock speed differences between the two processors; the 8565U is simply clocked higher so you can expect it to perform better, especially with a higher power limit.The 25W Core i7-8565U is also now delivering performance in line with the 45W Core i7-7700HQ in some workloads; the 8565U is less than one percent behind on average. This means that in the space of roughly two years, Intel has been able to take gaming laptop level CPU performance and put that into ultraportable-type chassis. Sure, you need to use the upper-end 25W configuration to achieve this, but itâ\\x80\\x99s impressive nonetheless.And finally comparing the 15W 8565U to even just the Core i7-7500U from a few years ago. Itâ\\x80\\x99s a no-contest. With double the core and thread count, the 8565U is on average 35% faster and that margin only increases when looking strictly at multi-core workloads. If youâ\\x80\\x99re coming from a dual-core ultraportable to a quad-core Whiskey Lake system, expect to see significant performance improvements across all workloads, either from the doubling of cores or from large clock speed gains.Wrap UpThere are a couple of ways to look at what Whiskey Lake brings to the table. On one hand, thereâ\\x80\\x99s not a lot to be gained in its 15W configuration. Weâ\\x80\\x99re looking at mostly single-digit improvements and sometimes for longer workloads, no improvements compared to Kaby Lake Refresh. Intelâ\\x80\\x99s 14nm node is clearly limiting and the shift to 14nm++ can only do so much.The best youâ\\x80\\x99ll get from Whiskey Lake is in its 25W configuration where performance improvements tend to match the clock speed gains more closely at around a 15% improvement. However itâ\\x80\\x99s rare to find a 25W system. The 15W config is much more common, so for the majority of buyers looking at a Whiskey Lake system, thereâ\\x80\\x99s not a lot of incentive to upgrade from Kaby Lake Refresh or to buy a Whiskey Lake system if it costs more than a last-gen Kaby Lake-R machine.However itâ\\x80\\x99s hard not to be impressed with what Intel has achieved over the last few years without significant advances to process technology. Sure, on the desktop 14nm+++ is now a bit of a joke and performance gains â\\x80\\x93 outside of increased core counts â\\x80\\x93 are anywhere from unimpressive to non-existent.But on the mobile side, within the same sorts of ultraportable laptop designs weâ\\x80\\x99ve gone from two cores at modest clock speeds, to four cores at reasonably high clock speeds, on largely the same process node and architecture. Performance that used to be restricted to gaming laptops is now accessible in more portable form factors, which is very impressive.And while Whiskey Lake isnâ\\x80\\x99t a huge step over Kaby Lake Refresh, it will be a massive improvement to anyone upgrading from a 7th-gen system or earlier. Typical laptop upgrade cycles are quite long. If youâ\\x80\\x99re using a four year old laptop for example, you can expect huge improvements upgrading to something 8th-gen. That said, Iâ\\x80\\x99d still shop around because you donâ\\x80\\x99t necessarily need Whiskey Lake to access those gains, Kaby Lake-R is also fine.There is still one lingering issue with Intelâ\\x80\\x99s mobile processors, and thatâ\\x80\\x99s the GPU side. With basically zero improvement in this department for generations now, Intel is lagging way behind what is required for a modern ultraportable. AMD realized this as their beefy Vega GPU in their Ryzen Mobile APUs handily crush Intelâ\\x80\\x99s integrated graphics. Lots of OEMs have also realized, and are starting to pair Nvidiaâ\\x80\\x99s MX150 discrete GPU with Intelâ\\x80\\x99s 15W CPUs to get that extra GPU performance.It does seem like Whiskey Lake is a bit of a stop gap until Intel can get their 10nm CPUs out the door at the end of 2019, which will bring a much larger and competitive Gen11 integrated GPU.With only 5 to 15% performance gains on the CPU side, itâ\\x80\\x99s all Intel could do at this point. OEMs are not exactly rushing out Whiskey Lake systems and we believe the small gains are a reflection of that.\"],\n",
       "  [\"Every once in a while, a game comes along that that does something surprising, different, memorable. Anthem is not one of those games.Much of this PlayStation 4, Xbox One, and PC game feels incomplete, while other portions are borrowed from games that did it better. The things that make Anthem special, like flying through magnificent, sun-soaked jungles in customizable exosuits, are ultimately overwhelmed by the drudgery of a now all-too-familiar loot grind, one whose sharp edges and confounding corners protrude unwelcomingly into what otherwise might have been an admirable glimpse into the live-game genre as imagined by the developers at BioWare.Anthemâ\\x80\\x99s core idea of â\\x80\\x9cjetpacks plus gunsâ\\x80\\x9d works excellently on its own, but nothing else in the game quite lives up to it. The missions are boring, much of the loot you find is useless, and the character progression is organized poorly. With all of these anchors dropped, the game is rarely able to get off the ground. When it occasionally does, the experience can be frictionless, heart-pounding fun. But sooner or later, like the exo-suits themselves, everything begins to overheat. Endless repetition punctuated by technical problems sends the machinery hurtling back to the ground with little left to do but let things cool off while waiting for the next content update, the next hotfix, the next post-launch roadmap tease, before rebooting the systems and trying again.BioWare entered the pantheon of beloved game studios by helping to pioneer a role-playing game formula that grew out of Dungeons and Dragons with Baldurâ\\x80\\x99s Gate, but eventually forged a new identity with the dual evolution of the Mass Effect and Dragon Age games. BioWare games in these series have ambitious narrative arcs augmented by consequential player choices, but also intimate and heart wrenching side-stories revolving around non-playable allies whose own goals, motivations, fears, and hopes take on equally compelling drama and urgency of their own.When BioWare released Mass Effect: Andromeda, it was a rare misstep for the studio. It had the best gunplay in the series and an intriguing simulation of life in a burgeoning space colony, but with an underwhelming plot, botched character animations, and almost no memorable player choices. At the same time, BioWare was developing another game that it code-named Dylan, because it was meant to be the Bob Dylan of video games, one which would become part of the gaming lexicon for years to come. Months after Andromedaâ\\x80\\x99s release, BioWare revealed what Dylan would be: Anthem, a multiplayer shooter in the mold of Ubisoftâ\\x80\\x99s The Division and Bungieâ\\x80\\x99s Destiny. Since then, I've been wondering how BioWare, known for gripping, single-player driven stories, would apply that to a genre premised on repeating the same missions over and over again just to watch numbers go up.The answer: It would not.Anthem exhumes the bodies of other developersâ\\x80\\x99 loot shooters, mixes and matches them, and adds a bit of BioWare flair to the Frankensteined corpse. You can choose whether to say something kind or slightly mean when accepting a quest from someone. Gun battles in large arenas are primarily augmented by complex math resulting from the intricate ways different pieces of top-level gear interact with one another.These additions are nice in and of themselves, but they donâ\\x80\\x99t go far enough or make up for the technical problems and quality-of-life issues plaguing the game at launch. This is the first new franchise from BioWare in almost a decade, and the result feels less like a revolutionary new idea and more like a sequel to Mass Effect: Andromeda. Itâ\\x80\\x99s stunning to look at and the jetpack bodysuits feel great to control, but at best it feels less like a game trying to leave a lasting impression and more like a platform for something that will come someday in the futureâ\\x80\\x94maybe.Anthem takes place in a harsh world filled with artifacts from a past civilization, which constantly reshape the terrain and cause natural disasters, caused cataclysms, in the process. Various factions battle for control of these artifacts, hoping to utilize their poorly-understood power for their own purposes. At the same time, roaming mercenaries called Freelancers get inside powerful, flying exosuits called Javelins and try to disarm the artifacts while fighting the monsters they spawn. Itâ\\x80\\x99s a good recipe for endless chaos, but not much else.The story centers around locating an ancient Javelin needed to shut down an extremely dangerous cataclysm called the Heart of Rage. With the exception of a few cutscene-driven beats, most of it feels like filler while waiting for the real game to begin. Then, before I knew it, the credits began to roll, concluding what felt like more of a first act than a full arc. Along the way, there were run-ins with a militarist organization intent on using artifacts for conquest, outlaw groups, and a race of humanoid insects called the Scar who have created an intricate life for themselves out in the wild. This being a BioWare game, you might think you could talk to them and become embroiled in some complex, inter-species struggle. But like the rest of the gameâ\\x80\\x99s enemy mobs, they are for target practice and little more.Thereâ\\x80\\x99s a kernel of the old BioWare in Anthem called Fort Tarsis, the single-player hub where you can meet characters, collect missions, progress the story, and learn about the world. Still, itâ\\x80\\x99s but a shell of the bases in Mass Effect and Dragon Age players learned to call home. Itâ\\x80\\x99s interesting to explore in first-person view, slowly walking the cramped alleyways examining the pockets of humanity who have managed to survive the brutal dangers outside the walls. One particularly heartbreaking episode centered on an older refugee who thought I was her daughter, resurrected by the power of alien artifacts and brought back to her. I spoke to her frequently in between missions, trying to decide how to comfort her, how to help her, and whether to tell her the truth. Confined to our one-off conversations rather than the larger world, the side-story felt like an unfortunate compromise, as if connecting our specific relationship to an actual set of missions or other discoveries made outside the fort would risk complicating Anthem beyond its multiplayer loot chase. While some people in Fort Tarsis are interesting to meet, and wonderfully voiced, their capacity to excite and shock feels stifled by the gameâ\\x80\\x99s desire to keep players in a shared timeline.Fort Tarsis has three main factions: Sentinels, the cityâ\\x80\\x99s guards, Arcanists, the cityâ\\x80\\x99s researchers, and Freelancers, the group you belong to, represented by a retired mercenary whose enthusiasm for the trade is on par with the parent who gets called up to be the assistant coach for their childâ\\x80\\x99s sports team. Whether completing a quest for one of those groups or the odd contract submitted by another resident, missions all have the same structure: Go out into Bastion, the region surrounding Fort Tarsis, fly from one objective marker to another, then kill whatever spawns there until the person on your intercom congratulates you on a job well done.Sometimes, missions will take you deep inside a fort for a big fight in a closed-off arena. Other times, youâ\\x80\\x99ll find yourself fighting mobs in a big open plain. In either case youâ\\x80\\x99ll be tasked with a) defending a point of interest, b) collecting pieces of a broken artifact to fix it, or c) killing a boss. Sometimes the broken artifact pieces look like glowing chunks of metal and have to be carried on foot. Other times they are orbs you can retrieve by flying. Sometimes, Anthem can be thrilling. But the missions can quickly start to feel like busy work, chores that stand between you and Anthemâ\\x80\\x99s endgame, which consists of completing these same tasks but on much harder difficulties for better loot.Youâ\\x80\\x99re basically Iron Man in the Garden of Eden.The natural wonders of Bastion can be scintillating to behold. Flying through a tunnel to come out on the other side greeted by a sunset cascading against the ashen cliffs of nearby mountain ridges is breathtaking. On more than one occasion, Iâ\\x80\\x99ve danced around in circles in a shallow pond just to watch the light reflect off the water as it ripped up along a pebbly shore. The gameâ\\x80\\x99s foliage is on another level, not in individual detail but in the convincing multitudinous variety that constantly bombards your retina. The shadows cast by all of the vegetation are impressive, as is the one cast by my Javelin as it descends down into a lush valley.Youâ\\x80\\x99re basically Iron Man in the Garden of Eden. Thereâ\\x80\\x99s just not much to do there besides the same old tedious tasks.There are four Javelin suits in the game. The Storm is the equivalent of a mage class, with big elemental area-of-effect attacks. The Colossus is heavily armored and good for tanking, complete with an ability for drawing enemy attention. The Interceptor, is light and fast with a focus on melee attacks. And the Ranger is a sort of jack-of-all-trades, offering a good balance of mobility and defense while at the same time being able to deal high damage against single targets. In addition to each handling differently and satisfyingly on its own, their strengths and weaknesses allow for a range of options when it comes to composing a four-person squad.A Colossus player could use their armor and shield to focus on patrolling the battlefield for downed allies who need to be revived, while a Storm player could use their pseudo-magic to clear out groups of weaker enemies. Then, the remaining Ranger and Interceptor could focus on coordinating combos to quickly deal lots of damage to heavier-hitting foes. Or, four friends could all ride into battle as the same class just for the hell of it. It makes teaming up more fun, since there are room for people to take on roles beyond â\\x80\\x9cshoot good and donâ\\x80\\x99t die.â\\x80\\x9dThe Javelins each have one defensive ability, two attack abilities, two guns, and slots for up to eight components that grant various stat bonuses, like longer flying times or higher combo damage. Each of these pieces plays a part in the gameâ\\x80\\x99s loot cycle. The more a particular piece is used to kill enemies, like a Hammerburst rifle or the Rangerâ\\x80\\x99s inferno grenade, the better versions of it youâ\\x80\\x99ll eventually be able to craft using resources that drop from enemies or from breaking down older pieces of equipment.Early on, the fun in Anthem comes from getting a new piece of equipment like an ice grenade and then using it to turn a nest of scorpions into popsicles during the next mission. The core of combat revolves around combos. First, you prime an enemy with a status or elemental effect, like being frozen, and then using a detonator like a missile shot from a shoulder-mounted launcher to deal extra damage. The flow of primers and detonators, each governed by a cooldown timer, combines well with traditional running and gunning to make it feel like thereâ\\x80\\x99s always something cool to do rather than just popping in and out of cover in between reloads. Add to this mix a quick-dash ability, double jumps, and the Javelinâ\\x80\\x99s ability to both fly and hover, and the results can be downright mesmerizing.Eventually, though, the lack of variety in the life of a Freelancer takes its toll, as does the litany of minor impediments whose inclusion in a game in 2019 is truly baffling. The current level cap in Anthem is 30, which in loot shooter terms means that almost nothing you do or acquire until then matters. Only after reaching that point, after around 20 hours of play, does the more interesting end-game begin. This is when it becomes possible to find Masterwork items and their Legendary variants, the rarest tier of equipment, each of which comes with a special name and Javelin bonus.The Balm of Gavinicus autocannon, for example, gives you back 25 percent of your Javelinâ\\x80\\x99s armor every time you hit two enemies with it. Others are much more involved. Tome of Precision is a Storm-specific amor component that increases electrical attack damage by 60 percent for five seconds after getting a precision kill on an enemy with a sniper rifle. I've acquired a handful of the Masterwork weapons, and what they lack in truly great gun-feel they make up for by offering the numerical complexity I crave from a more overtly role-playing style shooter.Unfortunately, the structure of the game means none of this is accessible until much later on, and even then, collecting the Masterwork items you want is mostly a matter of luck. Rather than slowly craft a custom loadout over the course of the entire game, the best, most interesting version of Anthemâ\\x80\\x99s combat is reserved for Grandmaster missions. These are versions of the same stuff you've already done a hundred times on a drastically higher difficulty. At this level, you have to communicate with teammates and know how to squeeze every last bit of efficiency out of your custom Javelin build, but just getting there requires a level of investment that isn't currently worth the eventual payoff, at least not in a world full of so many other ever-evolving team-based shooters.Slowing progress down is the fact that thereâ\\x80\\x99s no way to launch missions one after another. After you finish one, you have to go back somewhere: either Fort Tarsis, the forge where you can customize your loadout, or the launch bay, a small social space where squads can check their inventory and change out equipment without being split apart and having to reform. Each of these areas is separated by a loading screen. That means that finishing one mission, updating your Javelin with your new loot, and getting back to the battlefield requires going through at least three loading screens.To sign on to Anthem at the beginning, as with any live game, is to charter an untested vessel for a difficult voyage.Once youâ\\x80\\x99re in a new mission, thereâ\\x80\\x99s no way to check what you currently have equipped, let alone instantly swap out any newly-acquired gear. Most bizarrely of all, thereâ\\x80\\x99s no easily accessible screen to check your characterâ\\x80\\x99s overall stats based on their existing loadout. Even while tinkering around in the forge, none of the menus will display all the data from all of your equipment all at once. Given how much Anthem loves to bombard you with different numbers, percentages, and hyper-specific modifiers, the lack of any overarching character summary screen is a truly disappointing oversight.There are other things that seem to be missing. The gameâ\\x80\\x99s only vendor is for a handful of cosmetics which can be purchased with either in-game Coin or real-money Shards. Over the course of playing through the main story, I earned around 100,000 Coins, enough to buy one set of Javelin armor and a couple emotes. Neither of these things affect gameplay, but because they are the major tools for changing the appearance of your Javelin, having access to so few at such a high cost is disappointing. BioWare has said the store will refresh every couple days. This has currently left me in the unfortunate position of holding onto my coin in the fear that I might spend it on one set of armor only for one I really wanted to appear a few days later.Thereâ\\x80\\x99s another resource in the game called Ember, which is used for crafting top level gear. The reason to do this is to get a more optimal version of a Masterwork item. While each comes with a preset buff, it also has randomized ones, incentivizing players to farm the resources necessary to re-roll their Masterwork item until they get the version of it that best synergizes with the rest of their build. The ember required to do this can be farmed during missions or purchased with coin, although it costs so much coin that youâ\\x80\\x99re much better off spending that earned currency on cosmetics instead.Free-play, the gameâ\\x80\\x99s mode for when you just want to explore Bastion at your own pace while completing the odd World Event or two, can be just as galling. There is currently no way to set markers on your map for places you want to go to, although BioWare says it will add the feature in an update. Thereâ\\x80\\x99s also no way for other players to know where a World Event is taking place at any given time, meaning in many cases youâ\\x80\\x99ll be forced to complete them alone. Should you die, youâ\\x80\\x99ll have to respawn. Unfortunately, Anthemâ\\x80\\x99s map being as large as it is, and its respawn system being what it is, this usually means being placed back in the map up to a few minutes away from where you were originally killed. By the time you get back whatever objective you were working on, it may or may not still be thereâ\\x80\\x94along with any treasure chests that appeared if another player finished the activity.Progress can be hampered by disconnects and bugs as well. While stability has improved since the gameâ\\x80\\x99s initial launch on February 15 on PC and Xbox One for EA Premier and Access subscribers, I still occasionally encounter problems. Sometimes I get booted from the game with an error message about trying to update my player data. Other times new mission objectives have failed to appear after the previous one was completed. More often thereâ\\x80\\x99s simply a bit of latency that makes it hard to dodge enemy attacks. Iâ\\x80\\x99ll appear to have successfully gotten out of the way, only to spontaneously die three seconds later.Anthem seems to be pushing the limits of the PlayStation 4 and Xbox One. While I havenâ\\x80\\x99t had any significant issues in my time with the game on either console, the menu screens can also be sluggish to populate and navigate, and while playing on console Iâ\\x80\\x99ve noticed that cutscenes occasionally stutter. Within a few minutes of playing Anthem on my PS4, the fan kicks into hyper-drive.Perhaps if BioWare has its way, Anthem will still be up and running on the next generation of consoles in a few years. BioWare says it has a â\\x80\\x9clong-term visionâ\\x80\\x9d for Anthem, and thereâ\\x80\\x99s now a calendar on EAâ\\x80\\x99s website showing what some of those next steps will be. These include new missions in free-play, additional cosmetics and legendary missions to be added in March, and guilds, leaderboards, and weekly stronghold challenges in April. This â\\x80\\x9cfirst actâ\\x80\\x9d of new content is intended to be capped off in May with a new cataclysm mission. To sign on to Anthem at the beginning, as with any live game, is to charter an untested vessel for a difficult voyage in which it might never reach its ultimate destination, or even leave sight of the port.Itâ\\x80\\x99s possible that Anthemâ\\x80\\x99s best days are ahead of it. Itâ\\x80\\x99s likely not an accident that the first screen you see after loading the game is list of whatâ\\x80\\x99s new with the game. But that truth, as with all live games, cuts both ways. There are a number of good reasons to feel like Anthem shouldn't have been released in this state. Its being a live game will provide BioWare with ample opportunities to make Anthem a better loot shooter, but itâ\\x80\\x99s unlikely that they will make Anthem a better BioWare game.Maintaining a live game means keeping track of a shared world that remains unified for all players. BioWare said it committed to not doing paid expansions in part because it didn't want to fracture the gameâ\\x80\\x99s player base. Thatâ\\x80\\x99s not a vision that leaves much room for telling unique character stories driven by individual playersâ\\x80\\x99 choices.As much as Anthem can be fun to play at times, there comes a point in every game like it where some snag leads you to look at the clock and, for an instant, see your whole future with the game flash before your eyes. The weight of the hours you've already spent playing it get projected out into the coming weeks and months, and the fatigue of a futile climb up the ladder from rare loot to rarer loot begins to set in.In Anthem, it doesn't take long for that exhaustion to turn into dread, as you stare into the soul of the game and all that stares back is the ghost of someone continually rolling the dice in the hopes of getting a better gun.\"],\n",
       "  [\"Today we can finally show you how Intelâ\\x80\\x99s new octa-core 9th-gen processors perform. Thereâ\\x80\\x99s a lot to go over, so we wonâ\\x80\\x99t waste much time going over the specs considering the CPUs have been up for pre-order now for 10 days now and nothing from the spec sheet is a mystery.On hand for testing we have the Core i9-9900K -- actually we ended up with a few of these, with more tests and comparisons in the works for later on -- and we also have the i7-9700K which is basically the same CPU, but crucially, with Hyper-threading disabled.The Core i9-9900K is an 8-core processor with Hyper-threading enabled for 16 logical threads. It operates at a base frequency of 3.6 GHz but will boost as high as 4.7 GHz on all cores with a maximum single core frequency of 5 GHz. The L3 cache has been increased from the 8700Kâ\\x80\\x99s 12MB up to 16 MB and quite shockingly despite packing 2 more cores and 4MB more cache, the TDP rating remains at 95 watts which was already a suspiciously low rating for the 8700K. We'll explore the impact of this in a bit.The Core i7-9700K packs the same eight cores but can only process 8 simultaneous threads. It comes clocked at the same 3.6 GHz base frequency while the all-core and single-core clock speeds decreased by a marginal 100 MHz. The L3 cache capacity is dropped down to 12 MB, too.For testing we're using the MSI Z390 Godlike, but also used the Asrock Z390 Taichi Ultimate to confirm the results. Both boards were tested using DDR4-3200 CL14 memory and this same memory was used on all platforms without any manually-tuned timings. The graphics card of choice is Gigabyteâ\\x80\\x99s RTX 2080 Ti Gaming OC. We have loads of results to go over so letâ\\x80\\x99s get started!BenchmarksFirst up we have the memory bandwidth results and unsurprisingly the new Coffee Lake refresh CPUs are on par with previous models such as the Core i7-8700K. So everything is as expected here, letâ\\x80\\x99s check out some Cinebench results.As expected the 9900K and 9700K provide the highest out of the box single thread scores weâ\\x80\\x99ve seen to date, easily breaking the 200 pts barrier thanks to a 5 and 4.9 GHz clock speed when using just a single core.With all cores active the 9900K breaks the 2000 pt barrier making it 14% faster than the Ryzen 7 2700X. Meanwhile the 9700K was managed a score of just over 1500 pts that placed it just behind the old 1800X and just ahead of the 8700K. That also meant it was 26% slower than the 9900K.Given what we saw in Cinebench itâ\\x80\\x99s no surprise that the 9900K outclassed the 2700X in Blender, reducing the workload completion time by rather large 23%. The 8-core Ryzen CPU was a fraction faster than the 9700K though.Moving on to Corona and here we find a similar story, the 9900K reduced the render time by 20% from the 2700X, taking just 96 seconds. Though if your mostly rendering then the Threadripper 2950X makes more sense and Iâ\\x80\\x99ll talk more about that a little later on.The last rendering application we tested with is V-Ray and here the 9900K reduced the render time by 18%, taking just 62 seconds opposed to 76 seconds for the 2700X. The 9700K was a lot less impressive, taking a few seconds longer than the 8700K, making it slower than both the 1800X and 2700X.The PCMark 10 synthetic gaming benchmark relies heavily on both clock speed and core count, that said itâ\\x80\\x99s interesting to see the 9900K only matching the 2700X here while the 9700K was able to edge ahead of the older 1800X.\"],\n",
       "  ['Flagship Z390 vs Budget Z370 MotherboardWe have two final graphs weâ\\x80\\x99d like to discuss which compare the performance of Intel new 9900K on a flagship Z390 motherboard to that of a budget Z370 motherboard, MSIâ\\x80\\x99s Z370 PC Pro packing a real 4-phase VRM and for reference Iâ\\x80\\x99ve included the Core i7-8700K and Threadripper 2950X results.Starting with the stock out of the box performance we saw that the MSI Z390 Godlike allowed the 9900K to produce a score of 2048 pts. Now slotting that CPU onto the Z370 PC Pro resulted in a score of just 1790 pts after a 6-run average. Initially the score was up around 1900 pts but on the second pass we saw a lot more VRM throttling and this continued as we ran the test 4 more times to report a 6 run average.This meant on average the 9900K was 13% slower on the budget Z370 board. It is possible to enter the BIOS and remove the power limits and this did see full performance restored but even then we still saw some VRM thermal throttling going on and I did have a 120mm fan directly over the vcore VRM heatsink. So this is likely shortening the life of the motherboard and I donâ\\x80\\x99t recommend removing the limits on boards with them in place.Interestingly, if you overclock the 9900K on the Z370 board with the limits in place the performance is much worse than if you did nothing at all. Basically the increased vcore voltage sees the VRM throttle even harder with the limits in place and this further reduces CPU core clocks. Again removing the power limits does mostly restore performance to what was seen on the Z390 board, but if you were placing this board under heavy load for extended periods on a regular basis I donâ\\x80\\x99t imagine it will have a long and happy life, likely expect fireworks in the short term.'],\n",
       "  ['After a month-long wait since Nvidia unveiled the GeForce RTX 20 series, we can finally bring you our performance review. As you all know by now, we have a new flagship graphics card in the GeForce RTX 2080 Ti with pricing starting at $1,000 for partner cards and $1,200 for the Founders Edition version, we\\'re talking Titan X money here.Meanwhile the vanilla RTX 2080 is landing at $700 for partner models and $800 for the Founders Edition. Later next month we\\'ll also get the RTX 2070 at $500 or $600 for the FE card. This means gamers are looking at somewhere between $100 and $300 more for an equivalent model from the previous generation. On top of that, the Founders Edition models carry a further $100 to $200 price premium, and just know these will be the only models in stock from time to time.Nevertheless it\\'s worth noting that graphics card prices are down to regular levels. After a year of inflated prices due to indirect mining demand, the now two-year old GTX 1080 which currently sells for less than its $500 MSRP, was reaching nearly $1,000 at one point in early 2018. Thankfully for gamers, the craze is now over. We\\'ll discuss RTX 20 series pricing and availability in better detail towards the end of the review. For now letâ\\x80\\x99s get the specs out of the way and then jump into the benchmarks.Based upon Nvidiaâ\\x80\\x99s Turing architecture, and as the RTX in the name suggests, Ray Tracing is all the rage now. Basically what we have here is a type of hybrid rendering that combines ray tracing with traditional rasterization, so along with the new Tensor Cores weâ\\x80\\x99re also getting \"RT cores.\"Nvidia states that the fastest GeForce RTX model can cast 10 Billion rays per second, which compared to the unaccelerated Pascal is a 25x improvement in ray tracing performance. However todayâ\\x80\\x99s games donâ\\x80\\x99t use ray tracing as itâ\\x80\\x99s extremely slow on current hardware, so itâ\\x80\\x99s hard to say exactly what the benefits of hybrid rendering will be. Utimately that will depend on how future games implement the technology, which we\\'ll touch more on towards the end of the review.The flagship RTX 2080 Ti packs 4352 CUDA cores, a 21% increase over the 1080 Ti. This along with the Tensor and RT cores has seen the GPU die size increase by a massive 60%, partly explaining why this part is so damn expensive. Although these new GPUs are manufactured using TSMCâ\\x80\\x99s 12nm \"FinFetNvidia\" process, itâ\\x80\\x99s basically just 16nm with a larger reticle limit. Performance wise they are identical according to TSMC which explains the clock speeds.Speaking of which, the 4352 cores are clocked at a base frequency of 1350 MHz with a boost clock of just 1545 MHz, which is comparable to the 1080 Ti. I should note that the FE model is overclocked to a boost of 1635 MHz. Then using 11GB of 14 Gbps GDDR6 memory on a 352-bit wide memory bus the card has a memory bandwidth of 616.0 GB/s.The GeForce RTX 2080 comes with 2944 CUDA cores, a base clock speed of 1515 MHz and a boost clock of 1710 MHz, 1800 MHz for the FE model. It uses the same 8GB of 14 Gbps GDDR6 memory but on a slimmer 256-bit wide bus for a bandwidth of 448.0 GB/s.As noted earlier the 2080 Ti packs 10 Giga Rays per second, this figure has been reduced by 20% for the standard 2080 down to 8 Giga Rays per second. Then the RTX 2070 to come later next month packs 6 Giga Rays per second, a 40% reduction from the flagship. At this point we have no idea what this means. Is 6 Giga Rays per second going to be useful? Only time will tell.For testing we\\'re using a Core i7-8700K clocked at 5GHz and 32GB of Vengeance DDR4-3400 memory. For the AMD GPUs Iâ\\x80\\x99ve used the Radeon Adrenalin Edition 18.9.1 driver and for Nvidia the GeForce 399.24 driver, while the new GeForce GTX 20 series GPUs will be using driver version 411.51. In total we have a dozen games to go over along with a few other tests. Letâ\\x80\\x99s get to the results.BenchmarksStarting with the Battlefield 1 1440p results we see that the RTX 2080 is able to deliver GTX 1080 Ti like performance, initially I was worried that the 2080 would be slower than the 1080 Ti, placing it between the 1080 and 1080 Ti. Thankfully itâ\\x80\\x99s better than that and here we see it delivering 27% more performance when comparing the average frame rate, 23% for the frame time result.The 2080 Ti is an absolute beast here, beating both the 2080 and 1080 Ti by over a 25% margin. Pretty incredible stuff, but letâ\\x80\\x99s move on to 4K. Here the GTX 2080 like the 1080 Ti was able to provide playable performance, rendering 24% more frames on average when compared to AMDâ\\x80\\x99s Radeon RX Vega 64 Liquid.But it was the 2080 Ti that again blew us away with an incredible 100 fps on average, at 4K, in Battlefield 1, using the ultra quality settings. The frame time performance of the 2080 Ti was higher than that of the 1080 Tiâ\\x80\\x99s average frame rate.Moving to Far Cry 5 and again we see the RTX 2080 mirroring the GTX 1080 Tiâ\\x80\\x99s performance with 112 fps on average, making it 27% faster than the GTX 1080.As for the 2080 Ti, that pushed the average frame rate out to 132 fps, but we are clearly running into a system bottleneck here as the frame time performance is similar to that of the 1080 Ti.Who would have thought that at 1440p using ultra quality settings a 5GHz 8700K would be holding things up.Moving to 4K relieves the system bottleneck and now the 2080 Ti is 28% faster than the 1080 Ti on average and 25% faster for the frame time result. The 2080 is also 30% faster than the GTX 1080. That said, while the 2080 does average 61 fps you will notice dips below that, of course itâ\\x80\\x99s still very playable but for a silky smooth experience the 2080 Ti is truly breathtaking.Next up we have ARMA 3, a title thatâ\\x80\\x99s always heavily requested. Now at 1440p weâ\\x80\\x99re certainly not GPU bound, itâ\\x80\\x99s quite clear there is some kind of system bottleneck or possibly even a limitation with the game. So weâ\\x80\\x99ll have to move to 4K to see what the new RTX GPUs can offer there.Right, so at 4K we see a drop in performance for the 2080 Ti when compared to the 1440p numbers. This means itâ\\x80\\x99s now ~11% faster than the 1080 Ti. Meanwhile, the 2080 is 27% faster than the GTX 1080 and thatâ\\x80\\x99s a pretty serious margin right there.Moving along we have some Grand Theft Auto V results and again I know this is a seriously old title but itâ\\x80\\x99s still very popular and you guys seem to lose you mind if I donâ\\x80\\x99t include it, so in an effort to keep the peace here are the results. Also please note the game has been maxed out, including the advanced graphical settings.Despite that weâ\\x80\\x99re seeing a pretty heavy system bottleneck at 1440p so these results are somewhat useless, though they do inform us that if youâ\\x80\\x99re a massive GTAV fan and only play at 1440p then the RTX series wonâ\\x80\\x99t provide a notable performance boost.Once again itâ\\x80\\x99s the 4K resolution that helps separate the RTXâ\\x80\\x99s from the GTXâ\\x80\\x99s. Here the 2080 Ti was 37% faster than the 1080 Ti, and thatâ\\x80\\x99s a seriously nice gain. We also see that the 2080 roughly matched the 1080 Ti, placing it well ahead of the vanilla 1080, in fact it was almost 50% faster, so an amazing result here.'],\n",
       "  [\"Finally after an inevitable delay, itâ\\x80\\x99s time for our hands-on GeForce RTX 2060 coverage. The first reviews based on Nvidiaâ\\x80\\x99s Founders Edition card went out on January 7th, the same day Nvidia officially introduced the card, but sadly we werenâ\\x80\\x99t included. Then cards started to appear on store shelves on the 15th, selling around the $350 MSRP. By then we had received our review samples but since we were technically already late, rather than rush in we decided to take our time and prove an extremely in-depth benchmark comparison.Today we have a 36 game benchmark with over a dozen different GPUs. You will also find included a number of big performance breakdowns at the end of the article, along with the usual cost per frame stuff. By the time we're done youâ\\x80\\x99ll know exactly where the RTX 2060 stands, and what it has to offer gamers.For testing we have the Gigabyte RTX 2060 Gaming OC Pro and the MSI RTX 2060 Gaming Z. Both are great looking boards, but since the Gigabyte card arrived first that's what we used for all the testing. Later on we'll look into providing better detail about thermals and other elements that are reserved for individual graphics card reviews as those will vary from one model to the next.As for the RTX 2060 GPU... itâ\\x80\\x99s an interesting proposition. Whereas the RTX 2070 comes in at $500, the RTX 2060 is 30% cheaper at $350 yet packs just 17% fewer CUDA cores. It uses the same 14Gbps GDDR6 memory but the memory bus has been cut down by 25%, meaning memory bandwidth has been reduced by that amount. RT cores have also been lowered from 36 to 30.Still the RTX 2060 shouldnâ\\x80\\x99t be a great deal slower than the RTX 2070, so in terms of value itâ\\x80\\x99s likely a better product. Itâ\\x80\\x99s also priced to compete with AMDâ\\x80\\x99s Vega 56, so it'll be interesting to see how those two compare.Getting on with the benchmark results, out of the 36 games tested we'll discuss individually the most noteworthy ones and then jump into our breakdown analysis with all the data. Our GPU test system consisted of a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. We used AMD's Adrenalin 2019 Edition 19.1.1 drivers for the Radeon GPUs and Game Ready 417.35 WHQL for the GeForce GPUs.BenchmarksTo kickstart things off we have Shadow of the Tomb Raider. Here the RTX 2060 was good for 63 fps on average using the highest in-game quality preset. This placed it on par with the GTX 1080 and Vega 64, making it just 5% slower than the RTX 2070. It was also 9% faster than the GTX 1070 Ti, though just 3% faster than Vega 56.Moving on to Strange Brigade, the RTX 2060 was good for 86 fps on average or 15% slower than the RTX 2070 this time. It was also 7% slower than Vega 56, though thatâ\\x80\\x99s not surprising considering how well this title runs on AMD hardware. Still the RTX 2060 edged out the GTX 1070 Ti, so overall performance was very respectable.The RTX 2060 performs very well in Battlefield V, trailing the RTX 2070 by a mere 4% margin. It was able to beat the GTX 1070 Ti and Vega 56, so far the RTX 2060 is making the RTX 2070 look rather pointless.The RTX 2070 does pull away when testing with Sniper Elite 4, here the RTX 2060 was 15% slower but that's not a massive margin given it costs 30% less. Also we see that the 2060 is able to sit on top of the GTX 1070 Ti and Vega 56.Monster Hunter World plays quite well with the RTX 2060 at 1440p. Using the highest quality preset we saw an average of 54 fps and this meant the 2060 was barely any slower than Vega 64 and the GTX 1080. It was also 4 to 5 fps faster than the GTX 1070 Ti and Vega 56 on average. Moreover, it was just 11% slower than the RTX 2070.The new Turing GPUs donâ\\x80\\x99t stack up that well in Warframe when compared to their Pascal predecessors. The RTX 2070, for example, is 8% slower than the GTX 1080, meanwhile the RTX 2060 is 6% slower than the GTX 1070 Ti and just 2% slower than Vega 56.Moving on we have Just Cause 4. Here the RTX 2060 averaged 59 fps making it just a few frames slower than the GTX 1080 and Vega 64. It was also 11% slower than the RTX 2070 and Vega 64 Liquid. Frame time performance was similar from the GTX 1070 right up to the GTX 1080.\"],\n",
       "  ['To say there are a ton of laptops out there is an understatement. As is the case with smartphones, within a given segment or budget often there isnâ\\x80\\x99t much that differentiates themâ\\x80\\x94other than components and small design changes. But with Asusâ\\x80\\x99 latest ZenBook Pro, thereâ\\x80\\x99s one element that really makes it stand out from the crowd: a touchpad thatâ\\x80\\x99s also a touchscreen.Before we take a closer look at that particular feature, itâ\\x80\\x99s worth mentioning all of the Asus ZenBook Pro 14\\'s specs. The UX480 model we received for review measures 14 inches (thereâ\\x80\\x99s also a 15.6\" version) packing a Whisky Lake-U quad-core i5 or i7 CPU. You can also choose from 8GB or 16GB of 2400MHz DDR4, and storage options include 128GB or 256GB SATA SSD, 128GB or 256GB PCIe 3.0 x2 SSD, or a 1TB PCIe 3.0 x4 SSD.The model tested is powered by a Core i7-8565U (see our full review) clocked at 1.8GHz and can boost to 4.6GHz. While this isnâ\\x80\\x99t on the same level as the i9-8950HK found in the top-end 15.6\" model, itâ\\x80\\x99s more than enough power for everyday users and can handle most professional tasks, such as video and photo editingâ\\x80\\x94we did notice the underside got pretty toasty at times though.The ZenBook comes with a \\'Pro\\' moniker, but that doesnâ\\x80\\x99t mean it relies solely on integrated graphics; instead, you get a GTX 1050 with either 2GB or 4GB of GDDR5 VRAM. Itâ\\x80\\x99s the Max-Q version, allowing for a more efficient, space-saving design.While this isnâ\\x80\\x99t the GPU you want when playing Metro Exodus at max settings, the GTX 1050 can hold its own when you tone things back. I found that even this 2GB version kept Far Cry 5 and other older but still demanding titles at 30fps or above as long as the settings werenâ\\x80\\x99t maxed.At 3.5lbs (1.6kg) and measuring 17.9mm, this isnâ\\x80\\x99t the slimmest or lightest of laptops, which is partially due to the ScreenPad and the dedicated GPU. But it does feel reassuringly sturdy. Itâ\\x80\\x99s also a very nice-looking device, with fetching dark blues and rose gold accents.One interesting feature of the design is the ErgoLift hinge. This elevates the keyboard section off your desk at an angle of three degrees when the lid is open, supposedly improving airflow, typing, and acoustics. Itâ\\x80\\x99s something weâ\\x80\\x99ve seen variations of in other laptops, and while the hinge does have its benefits when on a flat surface, I sometimes found it dug into my legs when using the machine on my lap.While there\\'s no fingerprint reader, you do get the security of signing in through Windows Hello and the built-in IR camera.It feels like the ZenBook Pro was designed with desks in mind. The Harmon Kardon-branded stereo speakers are satisfyingly loud and meaty, but their position means they can sound muffled when on a userâ\\x80\\x99s lap. Ultimately, itâ\\x80\\x99s definitely one of the better sounding machines at this price range, but not quite as good as some of the more expensive laptops.The 14-inch display is a touchscreen, which some argue is an unnecessary feature on laptops that arenâ\\x80\\x99t 2-in-1 convertibles or detachables, I found it to be quite helpful when used in conjunction with the TouchPad, especially when in Extension Display mode.Asus even makes its own iPad-style pen but, like Appleâ\\x80\\x99s accessory, it must be bought separatelyâ\\x80\\x94though itâ\\x80\\x99s about half the price or cheaper.At this size, you get the option of a full HD display (1920 x 1080) resolution with a 60Hz refresh rate, but the 15.6-inch ZenBook Pro offers a 4K version. It boasts 100 percent coverage of the sRGB color gamut, has Pantone Validated certification for better color accuracy, and Delta-E of less than three. I found the screen to be noticeably colorful and vibrant, though its maximum brightness of around 280 nits would benefit from being higher when youâ\\x80\\x99re using it outside on sunny days.Asus claims that the ZenBook Pro 14â\\x80\\x99s 70Wh battery will last 12.5 hours with the screen brightness on 80 percent and battery saver mode turned on. How long the laptop runs for will depend entirely on how much you use the ScreenPad. Keep the second screen turned off and youâ\\x80\\x99ll get close to what Asus claims, but constantly watching tiny YouTube videos and playing around with apps will see your juice drain at a surprisingly rapid rate.I liked the keyboard on the ZenBook. The chiclet-style keys feature adjustable backlighting, 1.4mm of travel, and are sturdy enough that even the most heavy-handed of typists should find them agreeable. While it doesn\\'t compare to the Surface Book 2â\\x80\\x99s magnificent keyboard, I still found it a pleasure to use and couldnâ\\x80\\x99t find any complaints, though some might find them a bit mushy.There are fair number of ports on offer here: a USB 3.1 Gen2 Type-C, a USB 3.1 Gen2 Type-A, a USB 2.0, a HDMI port, microSD slot, and an audio jack. It doesnâ\\x80\\x99t have Thunderbolt 3, sadly, but neither does the much more expensive Surface Book 2.When it was released last year, Asus said the ZenBook Pro came with worldâ\\x80\\x99s first smart touchpad. Sticking the word â\\x80\\x9csmartâ\\x80\\x9d behind words to describe products that arenâ\\x80\\x99t particularly clever or useful has been a tech industry habit for years, but is this one of the exceptions?The ScreenPad measures 5.5 inches and boasts a full HD resolution. There are four modes on offer, which can be selected using the F6 key: ScreenPad mode, Extension Display, Touchpad mode, and Touchpad Disabled.ScreenPad mode is the main feature here. The first thing youâ\\x80\\x99ll notice is that despite being a touchscreen, itâ\\x80\\x99s also an excellent trackpad that works flawlessly. But Asus wants you to use the apps, which are accessed by swiping down from the top.In addition to simple applications such as the calculator, calendar, and media player, thereâ\\x80\\x99s a launcher for starting the likes of Steam, Chrome, and Mail. You can also access the Asus store, where you can download ScreenPad apps to sit alongside those that are pre-installed, such as Office.The ScreenPad offers various functions depending on what youâ\\x80\\x99re doing and what apps are installed. Watching YouTube, for example, will bring up video controls, while the Spotify app, which sadly didnâ\\x80\\x99t want to work for me, offers similar functions for the streaming music service.But possibly the best integration is with Microsoft Office products. Bring up Word or Excel and the ScreenPad lets you save your work, change fonts and colors, and much more.While the feature is innovative and offers more functionality than the MacBook Proâ\\x80\\x99s Touch Bar, itâ\\x80\\x99s hard to imagine anyone buying a ZenBook Pro for the ScreenPad alone. The biggest problem, other than the occasional glitches, is there still isnâ\\x80\\x99t enough apps out there. It would certainly be interesting to see one for social media sites or a Steam app where you could scroll through all your games/friends.Then thereâ\\x80\\x99s Extension Display; essentially, it turns the ScreenPad into a second monitor. Itâ\\x80\\x99s meant to aid multitasking by allowing you to, for example, move your emails or a Chrome window running Facebook onto the small screen, all while you use the main display for something else. The idea behind the Extension Display seems solid enough, though I feel the screen is just a bit too small for it to work the way Asus intends. But it does look super cool when using it in public, admittedly.The Asus ZenBook Pro 14 is a fine laptop thatâ\\x80\\x99s great for general use, handles most games, and can run professional apps, but the TouchPad is not a killer feature. While it will certainly make you stand out in a coffee shop, more developers need to support it to fulfill its potential, but then again, you could argue that Apple\\'s touchbar is a failure and even touchscreens on laptops are not killer features for most.For pros who work with photo and video, spending the extra cash and go for the 15.6\" version of the same laptop may be worthwhile for the 4K display, while hardcore gamers are likely to pick something with a beefier graphics card and VRR. The Asus ZenBook Pro thus occupies a nice middle ground where the ScreenPad is an added bonus.The ZenBook Pro 14 gets it right in striking a good balance is every department and at a competitive price. Currently there\\'s no US availability for the 14\" model we reviewed, but it is readily available in UK and other European countries. In the UK it\\'s currently selling for Â£1,199 ($1,590). In the States the 15.6-inch, 4K version, which comes with Thunderbolt, an i7-8750H CPU, and the same TouchPad is selling for $1,549 at Best Buy. If youâ\\x80\\x99re looking for a good all-rounder with a focus on creators, and one that differentiates itself from the crowd, this could be the laptop for you.'],\n",
       "  [\"With the release driver on hand for the Radeon VII we decided to go back to the test bench. Steve has spent the last few days doing nothing but benchmarking... after spending the few days before that, doing nothing but benchmarking.Since our day-one review of the Radeon VII, we were told by AMD driver improvements were coming but there would be no performance improvements, however the numerous stability issues we experience should be solved. And indeed, with the update in place the Radeon VII is now rock solid, which is a massive improvement.Today we have over 30 games to check out, including the new World of Tanks update as well as Apex Legends. For testing we used our Core i9-9900K test bench clocked at 5 GHz with 32GB of DDR4-3400 memory. We'll cover some of the results individually and then jump into the full performance breakdown with all 33 games.Since the review covered results for Fortnite, Battlefield V, World of Tanks, Strange Bridge, Monster Hunter: World, Shadow of the Tomb Raider, Rainbow Six Siege, Far Cry 5, Forza Horizon 4, Resident Evil 2, ARMA 3 and Hitman 2, we're going to skip over them and instead pick a dozen different games to discuss.BenchmarksApex Legends is a fresh new battle royale game that came out of nowhere, but it does seem like people are enjoying this one. Itâ\\x80\\x99s based on the same engine as Titanfall 2 and like Titanfall 2 itâ\\x80\\x99s capped at 144 fps.The Radeon VII does OK in this one. Itâ\\x80\\x99s slightly slower than the GTX 1080 Ti, 12% slower than the RTX 2080 and 26% faster than Vega 64. So thatâ\\x80\\x99s kind of Radeon VII in a nutshell.The Radeon VII performs quite well in Sniper Elite 4, but who didnâ\\x80\\x99t see that coming? This is a well optimised title that plays well on both AMD and Nvidia hardware. The GeForce RTX 2080 was still slightly faster but the margin is small. The Radeon VII also manages to improve on Vega 64 by a 30% margin.Deus Ex: Mankind Divided is a late 2016 release but itâ\\x80\\x99s certainly not the oldest game we test with. For those wondering DirectX 11 works better than DX12 on both AMD and Nvidia GPUs, so thatâ\\x80\\x99s why we use it.The Radeon VII was able to match the GTX 1080 Ti, making it a little slower than the RTX 2080 but a good bit faster than the RTX 2070 and Vega 64.Itâ\\x80\\x99s not the most well optimized title to be released last year but Just Cause 4 is still a lot of fun. Anyway, performance for the Radeon VII wasâ\\x80¦ okay. The 1% low performance matched that of the GTX 1080 Ti and RTX 2080 which is good, the average frame rate positioned it squarely between the RTX 2080 and 2070, which isâ\\x80¦ okay.The Star Wars Battlefront II results are decent, here the Radeon VII provides strong 1% low performance coupled with a slightly lower than expected average frame rate. In the end it was 10% slower than the RTX 2080 for the average frame rate, but 32% faster than Vega 64.Project Cars 2 has always favored Nvidia hardware. We see this when comparing Vega 56 and the GTX 1070 for example, typically the Vega GPU is faster. As a result Radeon VII was only able to beat the GTX 1080, making it much slower than the RTX 2080 and way slower than the GTX 1080 Ti.Moving on we have Assassinâ\\x80\\x99s Creed Odyssey, a title that never plays nice with AMD hardware comparatively even though itâ\\x80\\x99s an AMD sponsored title. Evidently that doesnâ\\x80\\x99t mean much as this game prefers Intel CPUs and Nvidia GPUs.The Radeon VII is a big step forward from Vega 64 offering 28% more performance, though it was only able to match the RTX 2070, which is not great for a $700 flagship AMD GPU.On previous tests we were using the standalone World of Tanks benchmark, but because that hasn't been updated and the game has received a major overhaul we're switching back to testing with the HD client.The game engine improvements were mostly focused on better CPU utilization, which is useful for those using low clocked multi-core processors. Before the Radeon VII was 17% slower than the RTX 2080, now itâ\\x80\\x99s 16% slower, so the result is the same.On Vermintide 2 the Radeon VII was slower than both the RTX 2080 and GTX 1080 Ti, though it was a huge step forward from Vega 64, delivering 36% more frames on average.The Witcher 3 has the Vega 56 card matching the GTX 1070, however the Radeon VII didn't fare as well, where it was 9% slower than the GTX 1080 Ti and even slower than the RTX 2080.AMD no longer enjoys a performance advantage in Vulkan-based titles. Take Wolfenstein II for example, here the Radeon VII was 12% slower than the RTX 2080.DiRT 4 is always an interesting inclusion as CMAA enables god mode on the Radeon GPUs and we again see this here with the Radeon VII. Vega 64 was able to beat the 1% low performance of the RTX 2080 Ti and here we see the Radeon VII shatter it while delivering the same 154 fps on average. AMDâ\\x80\\x99s GPU division would be in pretty good shape if every game looked like this.Performance SummaryThatâ\\x80\\x99s how the Radeon VII stacks up in another dozen titles, so between this feature and the day-one review weâ\\x80\\x99ve closely looked at 24 games, but thereâ\\x80\\x99s another 9 that were part of our mega benchmark.The graphs below will show you how the Radeon VII compares to the RTX 2080, 2060, GTX 1080 Ti and Vega 64 in all 33 games...In our review featuring just a dozen games, Radeon VII was 4% slower than the RTX 2080. Now with 33 games itâ\\x80\\x99s 7% slower, so a slight change there. The previous 12 titles are included here so an even amount of favorable and unfavorable titles for the GCN 5th-gen architecture.What the graph above tell us is that thereâ\\x80\\x99s an overwhelmingly good chance that in a given title the RTX 2080 will be faster, making it the superior gaming graphics card right now.The Radeon VII was 2% slower than the GTX 1080 Ti with our 12-game sample, now itâ\\x80\\x99s 5% slower. No major changes, but for the most part the much older GTX 1080 Ti was faster. Moreover, it was faster by a 10% margin or greater in 10 of the 33 games whereas the Radeon VII was faster by a 10% margin or greater in only 2 of the 33 games.Against the RTX 2060 the Radeon VII is 28% faster...Finally compared to Vega 64 the latest Radeon is 24% faster. Thatâ\\x80\\x99s a nice little performance bump, but as we said in the day-one review, youâ\\x80\\x99re paying 40% more for a little over 20% more performance.Putting It All TogetherFor better or worse, it seems we pretty much nailed it with our sample of games in the day-one coverage. The Radeon VII is indeed slightly slower than the RTX 2080, while costing the same amount and consuming a little extra power. Currently the reference card is extremely loud, but AMD has promised a fix is coming for that. We're also glad to report all the stability issues we were seeing prior to release have been addressed, so anyone who bought this card should enjoy a flawless experience.We've yet to delve into any overclocking and/or undervolting, something we'll report on if we find there's room for tweaks and squeezing more performance there.Something worth noting, AMD does believe that the Radeon VII is performing as it should be. Even though we plan to continue benchmarking this GPU during its lifespan, as we always do, AMD is not putting up excuses and saying drivers are still immature or early revisions. A surprise would be nice, but in the meantime what you see is what you get.The Radeon VII is a fine graphics card, but it's not competitive enough against the RTX 2080. We review it exclusively as a gaming product (where that 16GB frame buffer is not really a factor) and frankly it under delivered. If it was more efficient than the RTX 2080 and ran quiet, maybe we could deem it a worthy alternative, but letâ\\x80\\x99s not sugar coat this or beat around the bush, itâ\\x80\\x99s not as efficient, itâ\\x80\\x99s loud and for the most part itâ\\x80\\x99s slower, only a little slower but itâ\\x80\\x99s slower.For those reasons we canâ\\x80\\x99t recommend the Radeon VII over the GeForce RTX 2080. As badly as we wanted the RTX 2080 to be obliterated, forcing Nvidia to get real with their pricing, that has not happened. Therefore gamers are forced to pay 2016 pricing for 2016 performance (unless there's a DLSS miracle, but wait, no titles supporting that...). It's not a bleak scenario at all -- read why we believe building a gaming PC right now is a good idea -- but letâ\\x80\\x99s still hope later in the year we see GPUs deliver better value for gamers.\"],\n",
       "  [\"Nvidia's latest attempt to excite gamers arrives in the form of a new mid-range GPU with no RTX features on board. The new GeForce GTX 1660 Ti comes as no surprise as itâ\\x80\\x99s been widely rumored for some time and most recent leaks were so detailed they virtually confirmed its existence, down to the exact specs and pricing.And while Nvidia has been pushing the more expensive RTX series as the holy grail for gamers, the reality has been a whole lot less miraculous. Then again, this is akin to what we and many others were asking for: a cut down Turing GPU that trims off the fat. After all, RT cores and Tensor cores are of no value on a mid-range part.We doubt many of you will miss the ability to enable ray tracing or DLSS anyway, so by dropping these architectural features, Nvidia is able to offer a sub-$300 Turing GPU. However we still get the new SM units featuring dedicated cores for processing FP32 and integer operations simultaneously, and FP16 processing at double the rate of FP32, while also including the latest advancements in programmable shading. The new SM units also feature enhanced caches for increased capacity and bandwidth.Compared to the RTX 2060, the GTX 1660 Ti packs 25% fewer CUDA cores, but because it also drops the RT and Tensor cores the die is 36% smaller. When compared to the Pascal-based GTX 1060 6GB, the 1660 Ti die is 42% larger, but only packs 20% more CUDA cores. But as we just mentioned, those cores have been upgraded and are now much wider.The new GTX 1660 Ti also comes with 6GB of GDDR6 memory clocked at 1500MHz for a DDR clock of 6 GHz, and a data rate of 12 Gbps. Using a 192-bit wide memory bus, that results in a peak throughput of 288.1 GB/s which is a 14% reduction in bandwidth when compared to the RTX 2060, but 13% higher than the GTX 1070 and 50% more than the 1060.With those kind of specs, weâ\\x80\\x99re expecting the 1660 Ti to offer GTX 1070-like performance at the designated $280 price point, which would be good news for gamers.For testing weâ\\x80\\x99ll be looking at performance in a dozen titles. As we get some extra time for running more tests, we'll run our full suite of 30+ game benchmarks. The focus of this review will be on 1080p and 1440p performance, using our Core i9-9900K test system clocked at 5 GHz with 32GB of DDR4-3200 memory. We used Game Ready 418.91 WHQL drivers for the GeForce GPUs tested and Adrenalin 2019 Edition 19.2.2 for the Radeon GPUs.BenchmarksFirst up we have Shadow of the Tomb Raider and here the GTX 1660 Ti looks impressive, matching the GTX 1070 Ti with 87 fps on average at 1080p. This allowed it to match Vega 56 and therefore destroy the RX 590 by almost 40%. It remained lethal at 1440p as well, again matching the GTX 1070 Ti and beating the RX 590 by an impressive 41% margin.Forza Horizon 4 was actually one of the first games we benchmarked and we were shocked to find the GTX 1660 Ti so close to the RTX 2060. Weâ\\x80\\x99ve seen already that Turing GPUs arenâ\\x80\\x99t much faster than their Pascal counterparts in this title, and in the case of the RTX 2070 itâ\\x80\\x99s actually a little slower than the 1080.The GTX 1660 Ti had no trouble keeping up with the GTX 1070 and as a result was just 5% slower than the 2060.We see the same trends at 1440p. Here the 1660 Ti was just a single frame slower than the 2060, we donâ\\x80\\x99t expect weâ\\x80\\x99ll see many more results like this though.Keeping frame rates above 60 fps in Just Cause 4 certainly isnâ\\x80\\x99t easy, but even so the GTX 1660 Ti is now by far the best sub $300 GPU in this title, averaging 73 fps at 1080p, making it 14% faster than the RX 590.Similar margins are seen at 1440p. The 1660 Ti matches the GTX 1070 and this means it was just a few frames slower than Vega 56.The GTX 1660 Ti slayed Resident Evil 2 with an impressive 108 fps on average, this placed it between the 1070 and 1070 Ti, so a solid result there. It maintained this position at 1440p and in a strong title for AMD it was still 11% faster than the Radeon RX 590, so another very solid result for Nvidiaâ\\x80\\x99s new sub $300 GPU.Hitman 2 is a bit odd at 1080p because weâ\\x80\\x99re CPU bottlenecked at this resolution. Moving to 1440p gives us a better idea of how these GPUs compare and here we see the 1660 Ti roughly matching the GTX 1070, making it just 12% slower than the RTX 2060.The GTX 1660 Ti is no doubt going to be a popular choice amongst Fortnite players, itâ\\x80\\x99s by far the cheapest way to push well past 100 fps using high quality settings. It also proves very capable at 1440p, pushing well over 70 fps throughout our test run. Once again, youâ\\x80\\x99re looking at GTX 1070-like performance.Metro Exodus is an exciting new game that weâ\\x80\\x99ve already covered in-depth, but obviously this is the first time weâ\\x80\\x99ve tested it with the GTX 1660 Ti. Performance at 1080p is respectable as is gaming at 1440p. On the latter, the 1660 Ti matches the 1070, making it 30% faster than the 6GB GTX 1060.Rainbow Six Siege always provides us with interesting results and todayâ\\x80\\x99s review is no exception. For the most part the GTX 1660 Ti has mirrored the GTX 1070â\\x80\\x99s performance, but here it shatters it and manages to even edge out the GTX 1080.Much the same is seen at 1440p, where the 1660 Ti matches the GTX 1080 making it 16% faster than the 1070 and 58% faster than the 1060 6GB, while it was just 2% slower than Vega 56.Our updated Battlefield V testing saw the GTX 1660 Ti come in just behind the GTX 1070 at 1080p, but with 90 fps on average it was a good step up from the GTX 1060. Then at 1440p we see much of the same. There the 1660 Ti was slightly slower than the GTX 1070, though with just 2fps in it, you could comfortably say they provided the same experience. Not only that, but with over 60 fps rendered during our test, the 1660 Ti provided highly playable performance in Battlefield V at 1440p.Interestingly the GTX 1660 Ti was unable to match the GTX 1070 in World of Tanks at 1080p, where it was 7% slower. A rare result from the new Turing GPU coming in behind the 1070. It did manage to beat Vega 56 however, and was almost 40% faster than the RX 590.At 1440p we see very similar results. Vega 56 creeps ahead for the average frame rate, but is down for the 1% low result. Still at 35% faster than the Radeon RX 590, the 1660 Ti more than gets the job done.As the new ultra popular battle royale title in town, weâ\\x80\\x99ve tested Apex Legends with the new GTX 1660 Ti. The card spat out 123 fps at 1080p, making it a whisker faster than Vega 56 and a whisker slower than the GTX 1070 Ti. It was also 16% slower than the RTX 2060.Moving to 1440p, the 1660 Ti is found positioned between the 1070 Ti and Vega 56 making it 9% faster than the vanilla 1070 and 55% faster than the 6GB GTX 1060.Wrapping up the game benchmarks we have the new Far Cry title: New Dawn. The results are similar to what we saw in Far Cry 5, which is to be expected since itâ\\x80\\x99s the same world and game engine. Here the 1660 Ti matches the GTX 1070, making it a little slower than Vega 56.Then at 1440p as weâ\\x80\\x99ve seen numerous times already, the margins remain the same.Power & TemperaturesWhen it comes to power consumption the GTX 1660 Ti pushed total consumption to just 271 watts making it very efficient. In essence youâ\\x80\\x99re getting GTX 1070-like performance for an 8% power saving for the total system.We're not going to bother comparing that result with Vega 56, you can see the difference for yourself, plus Vega 56 has to be modified before it can be placed in a power graph like this, you know, standard out of the box stuff like undervolting until itâ\\x80\\x99s running on lemon juice. But in all seriousness, this is what you can expect from Vega 56 and 64 out of the box, same goes for Radeon VII.As for operating temperatures, the MSI GTX 1660 Ti Gaming X we used was whisper quiet, even after an hour of heavy load. The fans only spun up to around 1500 RPM and this allowed for a peak GPU temperature of 66 degrees in a 21 C room. Very impressive stuff from this relatively small graphics card.OverclockingEven when overclocked, the 1660 Ti maintained basically the same operating temperature and fan speed, running just a single degree warmer. The operating frequency was only boosted by 4% on average, but we were able to massively overclock the memory speed, increasing the data rate from 12 Gbps to 15 Gbps, the maximum speed MSI AfterBurner would allow us to hit.This boost combination allowed for an 8% performance increase when testing with Far Cry New Dawn. It's not exactly mindblowing stuff, but thatâ\\x80\\x99s a pretty typical overclock for a modern Nvidia graphics card.Individual MatchupsWeâ\\x80\\x99ve got a pretty good idea of how the new GeForce GTX 1660 Ti performs out of the box, how hot the MSI Gaming X runs, how well it overclocks, and how efficient it is. But how about we now check out exactly how the GPU stacks up against the GTX 1060, 1070, RTX 2060, Radeon RX 590 and Vega 56? Letâ\\x80\\x99s do that...GTX 1660 Ti vs. GTX 1060The GTX 1660 Ti is actually a decent upgrade over the GTX 1060 6GB, at least it can be depending on the title. On average it is 38% faster at 1080p and 40% faster at 1440p, and that right there is a big performance boost and certainly justifies spending $280 in our opinion. Especially if you want to delve into 1440p gaming.Upgrades aside, you wouldnâ\\x80\\x99t even entertain purchasing a GTX 1060 6GB anymore. We mean, you shouldnâ\\x80\\x99t have as the RX 580 is much better value. Anything north of $200 and we wouldnâ\\x80\\x99t give it a second look.GTX 1660 Ti vs. GTX 1070This one we called it throughout the review and benchmark results, so you know the GTX 1660 Ti is kind of the new GTX 1070. Performance does vary a bit in a few titles, but for the most part weâ\\x80\\x99re looking at a single digit performance difference. We see the same 2% performance difference at 1440p.The 1660 Ti performed well in Rainbow Six Siege, Shadow of the Tomb Raider and Apex Legends, but as we said overall performance was much the same.GTX 1660 Ti vs. RTX 2060Itâ\\x80\\x99s interesting to note that the GTX 1660 Ti is 20% cheaper than the RTX 2060, but at 1080p is just 12% slower. You could of course argue that the 2060 is better value because of those RT and Tensor cores that enable ray tracing and DLSS, but weâ\\x80\\x99d scoff at such a claim. The 1660 Ti was also just 12% slower at 1440p.GTX 1660 Ti vs. Radeon RX 590Compared to the similarly priced RX 590, the GTX 1660 Ti is a full 24% faster.Since we reviewed the RX 590, we said the pricing for that GPU was not right and was not a good value, but now AMD will be forced to make adjustments. The new 1660 Ti gets you 24% more performance for an 8% price hike. Not only that, but in the worst gaming scenario the 1660 Ti was still 7% faster than the RX 590, while in the best case up to 46% faster. So itâ\\x80\\x99s clearly the better buy.Expect further RX 590 price drops now that the 1660 Ti is on the scene. The margins we just described were maintained at 1440p, where the 1660 Ti was ~25% faster on average.GTX 1660 Ti vs. Vega 56Finally we have Vega 56 which typically sells for $400, with the crazy sale here and there dropping it down closer to $300. The GTX 1660 Ti was 7% slower on average and even if Vega 56 was to sell at $330 (the last deal we saw recently), the 1660 Ti would still be 15% cheaper, making it the better value overall. Not to mention, much more efficient.Cost per Frame and ConclusionWith everything weâ\\x80\\x99ve just seen, youâ\\x80\\x99d expect the GTX 1660 Ti to fare well in the value department and it does. Basically you're looking at a similar price-to-performance ratio as the Radeon RX 580, which is incredibly good news for gamers.The GeForce GTX 1660 Ti is an entire tier above the RX 580, so for this GPU to deliver a similar cost per frame is simply great.With a similar level of performance to that of Vega 56, the GTX 1660 Ti comes out at a saving of 25% per frame. It is also 19% more cost effective than the GTX 1060 6GB. The least impressive comparison can be made with the RTX 2060, here it was just 9% more cost effective, which is still an impressive margin and Iâ\\x80\\x99ll talk more about this in a moment.The first Turing GPUs were released last September, and in our opinion it wasnâ\\x80\\x99t until Nvidia released the RTX 2060 that we got a well rounded model worth buying. To be fair the RTX 2080 Ti is an incredible beast and we love gaming at 4K with it, but itâ\\x80\\x99s also stupidly overpriced at $1300. For a wide majority of gamers, itâ\\x80\\x99s not that good.The RTX 2080 was a snooze fest at launch, particularly because it was selling well above the MSRP. Today it's definitely better. The RTX 2070 meanwhile is far worse off, as it was effectively eliminated by the 2060 just three months after release. Then for those that hadnâ\\x80\\x99t yet nodded off, AMD chimed in with some elite level ASMR with their Radeon VII...The GTX 1660 Ti may be the first GPU release in a while that weâ\\x80\\x99ve come away feeling completely pleased. Weâ\\x80\\x99re getting GTX 1070-like performance for a slight price premium over the GTX 1060 6GB, and while many would have prefered pricing set at $250, without any competition from AMD at the moment we think this is a pretty good result.Speaking of AMD, the 1660 Ti puts them in a tough spot. Previously the RX 580 was the obvious sub-$300 option, but now the choice is more difficult and frankly weâ\\x80\\x99d go for the 1660 Ti. Itâ\\x80\\x99s $80 more but offers higher tier performance. In todayâ\\x80\\x99s games the RX 580 is more of a 1080p solution, whereas the 1660 Ti can be used for high refresh rate 1080p gaming or 1440p gaming. Itâ\\x80\\x99s also much more efficient and we expect all cards to run cool and quiet. Fingers crossed AMD can pull a rabbit out of their hat with Navi later in the year, that would certainly benefit us all.In the meantime though, the GeForce GTX 1660 Ti is the new sub-$300 king, and we are not at all concerned by the lack of RT and Tensor cores. Frankly, weâ\\x80\\x99d much rather have a GTX 2080 Ti with some extra CUDA cores... weâ\\x80\\x99d certainly be more compelled to spend $1300 on that.The only question remaining is, how much will you have to pay to get a new GTX 1660 Ti? As it often happens, pricing will likely be all over the place during the initial few weeks, but with the RTX 2060, 2070 and 2080 all selling at the MSRP, we expect the same will be true for the GTX 1660 Ti before long.\"],\n",
       "  [\"When we recently tested the new GeForce GTX 1660 we noted that Nvidia was making a bold claim in the review guide saying that the 1660 was a whopping 113% faster than the GTX 960, making it a perfect upgrade option for owners of the old mid-range Maxwell GPU.Considering the GeForce GTX 960 is the fifth most popular GPU among Steam users -- and that's a very representative sample of PC gamers -- seems there are plenty of you using it, so we thought we'd investigate what truth there is to Nvidia's words.Before we get to the benchmarks, a few quick stats. We first reviewed the GTX 960 back in January 2015 aiming at the $200-$250 bracket. It came packing 1024 CUDA cores, 27% less than the 1660 and theyâ\\x80\\x99re clocked 34% lower. Thereâ\\x80\\x99s also 33 fewer ROPs and while both make use of GDDR5 memory the GTX 960 has 42% less memory bandwidth due to lower clocked memory and more crucially a narrower 128-bit wide memory bus.For this test the focus will be on 1080p performance, looking at a dozen titles closely before jumping to a 33 game breakdown comparing the GTX 1660 and 960 head to head. All tests were performed in our GPU test bed which includes a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. We relied on Adrenalin 2019 Edition 19.2.3 drivers for the Radeon GPUs and Game Ready 419.35 WHQL for the GeForce GPUs.Gaming BenchmarksPlaying Apex Legends at 1080p with the high quality preset isnâ\\x80\\x99t much fun with a GTX 960. Youâ\\x80\\x99re looking at frame rates that are consistently well below 60 fps, far from ideal when playing a fast paced first person shooter. Those not wanting to reduce visual quality will have to upgrade and the new GTX 1660 offers a healthy 136% performance boost with 106 fps on average.Moving on to the recently released â\\x80\\x98The Division 2â\\x80\\x99 and we find a dire situation with the GTX 960.Just 27 fps on average, the games is nearly unplayable using the ultra quality preset at 1080p. Admittedly the game is quite demanding with these quality settings, but those looking to upgrade to a GTX 1660 can look forward to a 152% performance boost, not that bad. Though if youâ\\x80\\x99re only looking at playing The Division 2 then the RX 590 might be a better choice.We find a similar situation when testing with Shadow of the Tomb Raider, using the highest quality preset the old GeForce GPU spat out just 34 fps on average and with frame dips into the 20â\\x80\\x99s. Meanwhile the 124% performance bump offered by the GTX 1660 meant that the game could now be enjoyed in all of its glory at 1080p.The GTX 960 does do a decent job in Forza Horizon 4, as long as you have the 4GB model. Even so the GTX 1660 still offered a 126% performance boost hitting 95 fps on average and with over twice as many frames pumped out each second it did offer significantly smoother gaming experience.Hitman 2 was also playable with the GeForce GTX 960 and it wouldnâ\\x80\\x99t take much to dial back the quality settings to achieve 60 fps. That said, if you want to enjoy Hitman with the ultra quality settings enabled then something like the GTX 1660 is the way to go as it spat out a much more impressive 93 fps on average.Testing with Just Cause 4 saw the new budget Turing GPU deliver just over twice as many frames when compared to the GTX 960.The jump up from 32 fps on average to 65 fps is massive and it makes leaping and flying around while you attack enemies much easier and perhaps more importantly, much more enjoyable.We see a pretty consistent trend here, testing with Resident Evil 2 saw the GTX 1660 providing 129% more performance at 1080p with the maximum quality preset enabled. More than doubling the frame rate obviously leads to significant improvements in gameplay and while this isnâ\\x80\\x99t a game that necessarily needs big frame rates, the smoother motion certain adds to the experience.Of all the games weâ\\x80\\x99ve benchmarked so far, Fortnite is by far the least demanding. As such the GTX 960 was good for 52 fps on average and we are using the Epic quality preset, so with competitive settings the GTX 960 would be pushing over 60 fps at 1080p. However if you want 144 fps+ with competitive settings the GTX 1660 will be required and even with the Epic preset enabled it still averaged 113 fps.Metro Exodus is a new and very demanding title. The GTX 1660 canâ\\x80\\x99t reach a 60 fps average at 1080p with the ultra preset enabled. That said itâ\\x80\\x99s a darn sight better than the 27 fps on average youâ\\x80\\x99d be getting with the aging GTX 960, so thereâ\\x80\\x99s that.We must admit we were expecting the GTX 960 to do quite well in Rainbow Six Siege. While 48 fps on average is playable we did expected more, though come to think of it we're not exactly sure why given the RX 570 only renders 69 fps on average. This means the upgrade to the GTX 1660 would net you a healthy 146% performance boost, hitting 118 fps on average.Battlefield V is an interesting result as itâ\\x80\\x99s one of the smallest deltas we found for the 1660 over the 960, across the 33 games tested. Here the Turing GPU was 81% faster. Still a big margin but given weâ\\x80\\x99ve typically seen more than double the performance out of the 1660, this is a bit of a surprise.Although World of Tanks has recently received a major overhaul and we are testing with the HD client, itâ\\x80\\x99s still very well optimized for older hardware. For example, the GTX 960 was good for 55 fps on average and thatâ\\x80\\x99s very playable performance in this title. Still if you seek more performance then the GTX 1660 can help you out with a nice 91% performance bump.The GTX 960 also has just enough grunt to play Far Cry New Dawn at 1080p using the ultra quality preset. Pretty impressive and it really speaks to how well optimized this title is.Still, if you want to keep frame rates well above 60 fps at all times than the 98% performance boost youâ\\x80\\x99ll get for the GTX 1660 will be very welcomed.PowerWhen it comes to power consumption we see just how impressive the Turing-based GTX 1660 is in terms of efficiency. Despite often offering twice as much performance as the GTX 960, it pushed total system usage just 16% higher, hitting 262 watts. Even by todayâ\\x80\\x99s standards the GTX 960 isnâ\\x80\\x99t terrible and youâ\\x80\\x99ll get away with a very modest power supply.Putting It All TogetherFour years later, weâ\\x80\\x99ve seen how the GTX 960 performs in modern titles at 1080p using dialed up quality settings. Itâ\\x80\\x99s not great, but itâ\\x80\\x99s also about what youâ\\x80\\x99d expect from a mid-range graphics card that no doubt is still serviceable unless you require top graphics fidelity.The GTX 960 went on sale for $200 for the 2GB models, and typically around $40-50 more for the 4GB version that we tested today. If you are running a 2GB model then the performance uplift is going to be even greater that we saw today.Based on the 13 games that we just looked at, the GTX 1660 is about twice as fast as the GTX 960. At 1080p weâ\\x80\\x99re looking at a 117% performance boost on average. Thatâ\\x80\\x99s massive and it means Nvidia were right on the money.Granted, the GeForce GTX 1660 today will cost you ~$280 but that's not unreasonable to ask in today's market and has in fact made other GPUs below or above it adjust accordingly. The more affordable Radeon RX 580 8GB costs $190 at the moment, placing it roughly on par with the GTX 1660 in terms of cost per frame if you're going for ultimate value for your buck.This does mean GTX 960 owners have had a somewhat similar value option in the RX 580 for some time, making the GTX 1660 a little less exciting. Nonetheless, there is now a number of solid upgrade options for those still using a GTX 960 or something of similar performance. You can check out our recent 33 game test of mainstream GPUs here, or if you're using a GTX 980 instead, we revisited that one a few weeks back, too.\"],\n",
       "  ['More Productivity: 7-zip, Excel, Handbrake, Premiere CCWhen it comes to file compression performance the 9900K is roughly on par with the 7820X while the 9700K fell just short of the 8700K which meant it was also slower than the 2700X.Decompression performance is significantly stronger on the AMD CPUs and here even the Ryzen 7 1800X is able to edge ahead of the 9900K. The 9700K was able to slot back in ahead of the 8700K making it just a fraction faster than the 2600X.Moving on to our Excel testing and here we see a rapid completion time of just 1.8 seconds for the 9900K, reducing the completion time by 32% when compared to the 9700K and 19% when compared to the 2700X. So a solid result for the 9900K, the 9700K was much less impressive though, only slotting in between the 8700K and Ryzen 5 2600X.The Core i7-8700K already had the Ryzen 7 2700X beat in HandBrake so the new 8-core models take that a step further, even the 9700K was seen to be faster than the 8700K here. When compared to the 2700X the 9900K was 32% faster, though it was 20% slower than the 2950X.For those of you like me who use Premiere Pro CC a lot or any other video software on a daily basis these numbers will be of great interest. Here the encoding performance of the 9900K is roughly on part with the 7820X, meaning was 8% faster than the 2700X and 17% faster than the 9700K. That said it was almost 20% slower than the 2950X, so if time really is money thatâ\\x80\\x99s still a better option.Editing performance with our warp stabilizer test sees the 9900K only just edge ahead of the 8700K and 9700K. This meant while it was a bit faster than the Ryzen 7 2700X it was quite a bit slower than the 2950X.'],\n",
       "  [\"Recently we've looked back at the GeForce GTX 980 Ti and the GTX 960, both popular GPUs from yesteryear. Those features have been warmly welcomed, but besides the overall positive responses what we noticed in common in your feedback was the request to test the GeForce GTX 970, which was the performance/value offering of the time and a GPU some of you are still rocking in today's games with some success.So today weâ\\x80\\x99ve got an old fashioned GPU battle pitting the GeForce GTX 970 against the Radeon R9 290. Whether you're looking to upgrade or not, it should be interesting to see after 5 years which GPU is the ultimate winner...Quickly rewinding for a moment, Nvidia revealed the GTX 970 for the first time in September 2014, some ten months after the Radeon R9 290â\\x80\\x99s release. The GTX 970 was the newer product and unsurprisingly it came in offering a little extra performance at a slightly better price. AMD did respond with aggressive R9 290 price cuts but with the 970 generally 10-15% faster on average and considerably better power efficiency, it went on to be a best seller, despite the VRAM controversy.Throughout 2015 the GTX 970 was typically the puncher GPU and with a slew of quality AIB models to pick from, it easily outsold the R9 290. However many of those who purchased the R9 290 were jumping up and down about how it would be a better investment down the track. At least today we can settle which GPU offers the best performance in 2019.For this test we have 33 games tested at 1080p, a suitable resolution for these GPUs. Weâ\\x80\\x99ll look closely at the results for 13 of the titles before moving into a few head to head comparison graphs. As usual the test system used is powered by a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. Let's see those blue bar graphs now...BenchmarksFirst up we have Apex Legends and right away weâ\\x80\\x99re seeing pretty neck and neck performance between the GTX 970 and R9 290. This oneâ\\x80\\x99s too close to call and well within the margin of error. Itâ\\x80\\x99s safe to say performance is identical using either GPU. When compared to a modern GPU weâ\\x80\\x99re looking at GTX 1060 3GB-like performance.The Division 2 is a title that works exceptionally well with AMD hardware. As evidenced by Vega 64 beating the GTX 1080 and RTX 2060, while the RX 580 crushed the GTX 1060. With those comparisons previously established it comes as no surprise that the R9 290 dusts the GTX 970 beating it by a 19% margin. The 970 does perform significantly better than the 3GB 1060, but even so youâ\\x80\\x99ll want to dial the quality settings down a little.Shadow of the Tomb Raider is another title that sees AMD GPUs doing quite well and here the GTX 970 was 12% slower than the R9 290. The GTX 970 mimicked the GTX 1060 3GB, or I guess you could say the 1060 mimics the 970, either way performance was decent despite trailing the R9 290.We promise the order of the games wasnâ\\x80\\x99t chosen on purpose but here we have yet another title that works extremely well with AMD GPUs. Again, the GTX 970 was 12% slower than the R9 290, though despite that performance was still very playable. Still in three of the more recently released titles weâ\\x80\\x99ve seen the R9 290 doing well.Finally a solid win for the GTX 970, this time testing with Hitman 2 using the DirectX 11 API. The GeForce GPU was 12% faster though I should point out that enabling the new patched-in DX12 mode didnâ\\x80\\x99t change things. The 970 was still the superior GPU in this title.Performance in Just Cause 4 was similar using either GPU. The GeForce pulled ahead in our test when looking at the average frame rate, however the 1% low result was identical with both managing 40 fps, so overall the experience was much the same.Moving on we find a close battle when testing with Resident Evil 2, here the R9 290 was just 6fps faster at 1080p and this meant the 970 performed more like a 3GB 1060, not a bad result given the average frame rate was 66 fps, but this one is still a win for the Radeon 290.Fortnite is a game that favors Nvidia GPUs thanks to its use of the Unreal Engine 4. This can be seen when comparing the GTX 1060 and RX 580 for example, it can also be seen when comparing older GPUs such as the GTX 970 and R9 290. Here the 970 was 18% faster, delivering 87 fps on average.The GTX 970 also stacks up really well in Metro Exodus with an average of 48 fps at 1080p using the ultra quality settings. This meant it was 23% faster than the R9 290, placing it basically on par with 6GB GTX 1060 and RX 590. So a very solid result for the old Maxwell GPU.Next up we have Tom Clancy's Rainbow Six Siege and here the GTX 970 is more competitive than I was expecting. I recall this being a very easy win for the R9 290 back in the day, but here it was just 5% on average. That said the 1% low performance was more consistent, though the experience was still extremely smooth with the 970.Frame rates were surprisingly similar when testing with Battlefield V, here the GTX 970 was just 3 fps faster which comes to be a 5% margin. This is why we typically call it a tie when the margin is 5% or less. The GTX 970 did well in this title and as weâ\\x80\\x99ve seen many times already, itâ\\x80\\x99s often very similar to the 3GB GTX 1060.World of Tanks is a game that doesnâ\\x80\\x99t like AMD GPUs that much, or AMD GPUs donâ\\x80\\x99t like World of Tanks. Either way itâ\\x80\\x99s not great news for the red team. The R9 290 was able to deliver very playable performance but we see a 36% uplift when looking at the GTX 970 and thatâ\\x80\\x99s obviously significant.And the last game weâ\\x80\\x99re going to look at individually is Far Cry New Dawn and here we have a tie at 70 fps a piece. That placed both the GTX 970 and R9 290 on par with the 3GB 1060. Performance at 1080p using the ultra quality preset certainly was respectable and you wonâ\\x80\\x99t need to turn down any quality settings here.Power ConsumptionThis is one area where the Maxwell GPUs had a big advantage over the AMD competition and itâ\\x80\\x99s an advantage Nvidia still holds today. For what is a similar level of performance the R9 290 drove up system power consumption by 30%. This is why GTX 970 graphics cards typically run cooler and quieter than R9 290 cards.Performance SummaryThatâ\\x80\\x99s how the GTX 970 and R9 290 stack up in those preliminary titles, seemed as though we saw quite a bit of back and forth. Based on what weâ\\x80\\x99ve seen so far we donâ\\x80\\x99t imagine the GTX 970 is still 10-15% faster like what most reviewers found back in late 2014. That said letâ\\x80\\x99s see what the 33 game comparison has for us.Thereâ\\x80\\x99s your â\\x80\\x98fine wineâ\\x80\\x99 working right there, a.k.a. AMDâ\\x80\\x99s slow driver development. Seriously though, thatâ\\x80\\x99s an impressive comeback for the R9 290. Still the GeForce 970 was faster by a 5% margin or greater in 16 of the 33 games tested, while the R9 290 was faster by a 5% margin or greater in just 11 of the games.So the GeForce GTX 970 was the more consistent performer, the smaller 3.5GB primary VRAM partition will have hurt it in Wolfenstein, especially with the settings used for testing. DiRT 4 also favors Radeon GPUs using CMAA, while Strange Brigade is an AMD sponsored title and well optimized for Radeon GPUs. AMD also does well in The Division 2, Sniper Elite 4, Forza Horizon 4, and Shadow of the Tomb Raider.It was mostly the older titles, or Nvidia sponsored games where the green team did well, games such as World of Tanks, Warhammer II, Metro Exodus, Project Cars 2, and Fortnite, for example.GeForce GTX 970 vs. GTX 1060 6GBMoving on we decided to see how the GTX 970 stacks up against the GTX 1060 6GB. On average the 970 was 14% slower. So here weâ\\x80\\x99re comparing a 2014 release with an MSRP of $330 to a 2016 release with a $250 MSRP. For those wondering back when we reviewed the GTX 1060 6GB at launch, the 970 was 15% slower, so no â\\x80\\x98Nvidia gimpingâ\\x80\\x99 as itâ\\x80\\x99s often referred to appears to have taken place.Of course, Nvidia doesnâ\\x80\\x99t actively gimp performance, weâ\\x80\\x99ve proven thatâ\\x80\\x99s a load of nonsense multiple times now, as have others. They are guilty of prioritizing newer architectures while optimizations for older generations tend to come later, if at all for the really old models.GeForce GTX 970 vs. RTX 2060Now if youâ\\x80\\x99ve been holding out all these years for a $300-$400 upgrade, the GeForce RTX 2060 might be of interest.Closing RemarksThere you have it, the GTX 970 went from ~10-15% faster four years ago to a percent faster in 2019 against the Radeon R9 290 based on our 33 game test sample that includes many newer titles.Despite its 3.5GB of fast VRAM, the GTX 970 remains the more reliable performer which might surprise some of you, especially after youâ\\x80\\x99ve no doubt heard over and over again how itâ\\x80\\x99s doomed and will be completely useless before too long. It's also the cooler GPU but most important of all, either solution will let you play games at 1080p comfortably all these years later.The doomsday scenario still hasnâ\\x80\\x99t happened and with a few minor tweaks the GTX 970 can carve its way through the latest and greatest titles at 1080p without an issue. To give credit where credit is due, the Radeon R9 290 is also extremely impressive in 2019.Weâ\\x80\\x99re completely attributing the R9 290â\\x80\\x99s comeback to AMDâ\\x80\\x99s driver development and not anything to do with the partitioned VRAM buffer or Nvidia neglecting driver support. The 'fine wine' here is all about AMD getting on top of driver optimization over the past few years.\"],\n",
       "  [\"Having reviewed nearly a dozenRazer laptops in the past few years of which this is our fifth Blade Stealth, it's always a delight to discover what's new and what's been improved on newer iterations.The new Razer Blade Stealth uses a new design with new hardware. All models come with an Intel Core i7-8565U processor and a 13.3-inch 1080p display. The base model does not include discrete graphics, it packs a 256GB SATA SSD and 8GB of RAM. The graphics model, the version we received to review, has GeForce MX150 discrete graphics, a 256GB PCIe SSD, and 16GB of RAM. Both variants come with 53 Wh batteries, while in some regions thereâ\\x80\\x99s an even higher-specced model with a 4K display and a 512GB SSD.Following up to our look at Intel Whiskey Lake CPU performance, itâ\\x80\\x99s now time to properly check out the laptop we used for those benchmarks. Do note this laptop was released towards the end of 2018, so it's readily available and itâ\\x80\\x99s an impressive piece of hardware thatâ\\x80\\x99s worth looking at in detail.Pricing wise, weâ\\x80\\x99re looking at $1,400 for the base model, $1,600 if you want the MX150, and $1,900 if you also want the 4K touch display. As tends to be the case with Razer laptops, itâ\\x80\\x99s pretty expensive.The previous Blade Stealth looked pretty decent and improved upon the chunky older designs in many ways, but this newer design steps things up another notch. It uses a similar build to the 15-inch Razer Blade in that itâ\\x80\\x99s a squarer design with slimmer bezels around the display. Itâ\\x80\\x99s now at the point where the display dominates the lid area like a lot of modern ultraportables. Weâ\\x80\\x99re not at crazy screen to body ratios just yet, but the new Blade Stealth doesnâ\\x80\\x99t feel like thereâ\\x80\\x99s wasted space around the screen.Build quality has always been a killer aspect to Razerâ\\x80\\x99s notebooks and thatâ\\x80\\x99s still the case here.The main chassis is a solid aluminium unibody with a black anodized finish, itâ\\x80\\x99s simple, it feels really sturdy and it looks fantastic. Itâ\\x80\\x99s a bit of a fingerprint magnet but thatâ\\x80\\x99s the case with most black metal laptops.While the overall footprint is smaller than before thanks to its slimmer bezels, it is ~1mm thicker than previous models which makes next to no difference. The key area of interest is the weight, which varies between 2.8 and 3.0 lbs depending on the model. Thatâ\\x80\\x99s not particularly light for an ultraportable in 2019 but itâ\\x80\\x99s not heavy either.Looking at the ports and features, itâ\\x80\\x99s all standard affair for this machine. Thereâ\\x80\\x99s a Thunderbolt 3 port, a USB-C 3.1 port and two USB 3.1 Type-A ports plus a headphone jack. While there are more USB ports on this laptop than its predecessor, thatâ\\x80\\x99s come at the expense of the full-sized HDMI port which is disappointing. To hook up a monitor youâ\\x80\\x99ll likely need some sort of HDMI or DisplayPort adapter.The Blade Stealth also has a basic 720p webcam above the display with Windows Hello support. There are speakers on either side of the keyboard that are fairly average as is the case with basically every laptop these days. The trackpad is expansive, accurate and responsive just like Razerâ\\x80\\x99s other trackpads. And the keyboard has a great tactile response to it, nice and clicky for a laptop keyboard which is what I like to see. Itâ\\x80\\x99s also not cramped and includes full sized arrow keys as well as a range of handy functions mapped to the F-keys.It wouldnâ\\x80\\x99t be a Razer product without some form of Chroma RGB lighting, but the new Blade Stealth includes it in a pretty limited form.Rather than per-key RGB lighting for the keyboard like previous Razer laptops, the Blade Stealth just has a single backlight zone that encompasses the entire keyboard. This limits the available effects, but I donâ\\x80\\x99t think per-key RGB is all that necessary for an ultraportable and Iâ\\x80\\x99m sure this new single-zone design saves both space and battery.The display, at least for the base and graphics models, is a 13.3-inch 1080p IPS at 60 Hz. Nothing too fancy, just your typical laptop-grade display. A key feature is that Razer is individually factory-calibrating every display, and they are boasting 100% sRGB coverage as well, which is great news for those that want color accuracy and decent colors for content creation.This panel is one of the better ones Iâ\\x80\\x99ve seen in laptops, boasting a contrast ratio of 1300:1 and brightness up to 400 nits, which is above average in both regards. Color accuracy is good out of the box, too. The white point is perfect at 6500K and while that does fall away somewhat over the greyscale range, thereâ\\x80\\x99s no noticeable color tint.Greyscale deltaEs of 2.04 are just slightly higher than what we class as â\\x80\\x98very goodâ\\x80\\x99, while a saturation deltaE of 1.25, and a ColorChecker deltaE average of 1.55 are fantastic results. This level of accuracy is well above average for a laptop and makes it a great display for content creators.PerformanceLetâ\\x80\\x99s talk about performance. We're going to focus on the Blade Stealth with discrete graphics and workloads that specifically make use of the GPU to see how the MX150 makes an impact. We already covered the performance of the Core i7-8565U in a separate review. So if youâ\\x80\\x99re interested in how CPU-only workloads perform, or how the base model will perform in general, check out our Whiskey Lake CPU review.As a refresher, the Blade Stealth uses the quad-core i7-8565U which is Intelâ\\x80\\x99s new high-end Whiskey Lake CPU built on 14nm++. Normally this CPU is configured to use a 15W TDP, however Razer has chosen to use the cTDP up state of 25W, which allows the chip to run at higher sustained clock speeds.The rated clock speeds for the 8565U are 1.8 GHz base with a 4.6 GHz single-core Turbo and a 4.1 GHz all-core Turbo, but typically the 25W version of the chip hovers around 3.1 GHz during sustained non-AVX workloads.In CPU-heavy benchmarks the 25W 8565U is up to 33 percent faster than the 15W 8565U, so thatâ\\x80\\x99s something to consider when comparing the Razer Blade Stealth to other 8565U laptops that use the standard 15W configuration. You wonâ\\x80\\x99t see 33% gains in every app, short workloads are largely unaffected and single-thread performance is 4 to 7 percent higher, but for long encoding tasks itâ\\x80\\x99s a significant difference.The 25W 8565U is also about 15% faster than the 25W Core i7-8550U, Intelâ\\x80\\x99s previous Kaby Lake Refresh CPU. And that margin increases to 20% on average over the 15W 8550U, with gains up to 38% in some workloads.That could definitely justify an upgrade over the 8550U depending on the system you previously had. And of course, if youâ\\x80\\x99re coming from anything dual-core, like a Core i7-7500U or older, expect gains of 62% on average.Now letâ\\x80\\x99s explore what the MX150 brings to the table...Itâ\\x80\\x99s crucial to note this is not the '1D12' variant of the GeForce MX150, rather this is the fully fledged version. Razer specifies this is a â\\x80\\x9c25Wâ\\x80\\x9d MX150 and looking at GPU-Z confirms this is a regular 1D10 model. So thatâ\\x80\\x99s good news, though I wish Nvidia wouldnâ\\x80\\x99t be so confusing with their GPU names.The MX150 comes into play in anything thatâ\\x80\\x99s GPU accelerated, so that includes popular applications like Adobe Premiere. Here the MX150 has a noticeable impact on render times, both with and without Lumetri color processing effects.With Lumetri effects, render times are more than halved and you can also see that this fully fledged MX150 is several minutes faster than the 1D12 version. Even without Lumetri effects, our render benchmark was 30% faster thanks to the discrete GPU.Photoshopâ\\x80\\x99s Smart Sharpen filter also benefits significantly from GPU acceleration, again cutting processing times by more than half. Intelâ\\x80\\x99s integrated GPU in their 15W processors is extremely weak, the MX150 is easily more than twice as powerful and that plays out in this scenario.CompuBench is a great benchmark for testing the compute performance of these GPUs across a range of synthetic tests, and looking at one of these results for optical flow again shows the power of the MX150.Itâ\\x80\\x99s around 2.6x faster than the UHD 620 integrated GPU, and itâ\\x80\\x99s also a decent 23% ahead of the Vega 8 GPU in AMDâ\\x80\\x99s Ryzen 5 2500U. That said, I expect the Vega 10 GPU in the Ryzen 7 2700U to provide similar performance to the MX150, albeit with lower CPU performance than the 8565U.In 3DMark we also see enormous gains when comparing the MX150 to the regular Intel iGPU, itâ\\x80\\x99s typical to see more than double the performance, especially looking at graphics scores where in Time Spy, for example, the MX150 approaches being 3x as fast.In terms of gaming this doesnâ\\x80\\x99t exactly turn an ultraportable into a gaming powerhouse but it certainly makes entry-level titles playable where they previously were not. Intelâ\\x80\\x99s UHD 620 is only suitable for 2D games and other simple things, whereas the MX150 is pretty capable at playing Fortnite, for example. You can expect 1080p, 60 FPS gaming in Fortnite using low settings.A few other things worth mentioning regarding performance...The Razer Blade Stealth has two performance modes, balanced and low power. Throughout this review weâ\\x80\\x99ve used the balanced mode. When enabling the low power mode it limits the CPU to a TDP of around 8W, so performance is significantly reduced. That could come in handy to extend battery life though.The cooler is always a contentious issue with Razer laptops because they tend to run hot and loud. But this is only somewhat the case with the Blade Stealth. CPU temperatures were relatively okay, hovering around the 75C mark which isnâ\\x80\\x99t that hot for a laptop. However the cooler is audible during heavy load.My recommendation would be to use the manual fan speed control to turn down the fan speed to around 3000 RPM, although that depends on the conditions. When doing this I saw temperatures rise by around 7C but the laptop was much quieter and overall that was a better experience in my opinion. I think Razerâ\\x80\\x99s default fan curve is too aggressive and it could annoy people that buy this laptop and donâ\\x80\\x99t realize there is a manual fan slider.The GeForce-equipped model comes with a 256GB Liteon CA3 PCIe NVMe SSD which performs well, especially in random workloads. It doesnâ\\x80\\x99t have the fastest sequential performance Iâ\\x80\\x99ve seen, but this is still a fast drive and should provide a noticeable performance upgrade compared to the SATA drive in the base model.Another important area for laptops is battery life, however because we havenâ\\x80\\x99t been benchmarking many ultraportables lately we donâ\\x80\\x99t have a good selection of battery data to compare the Blade Stealth against. If you want detailed battery life information we recommend reading otherreviews. From what I have been testing, Iâ\\x80\\x99d describe the battery life as above average for what itâ\\x80\\x99s worth.Who Is It For?The new Razer Blade Stealth offers an impressive overall package. The design is excellent, but thatâ\\x80\\x99s been complemented this time around with top-end hardware for what is possible in this sort of ultraportable chassis design. Weâ\\x80\\x99re getting the 25W version of the Core i7-8565U, so it will outperform most other laptops that use the 15W configuration, or are stuck on Kaby Lake Refresh.Weâ\\x80\\x99re also getting a fully fledged MX150 discrete GPU, so Razer has matched a powerful CPU with a powerful GPU for this form factor. Previous models left the GPU a little behind, with Razer pushing their Core V2 external GPU solution for those wanting to do some gaming. This new model is not a gaming powerhouse, but itâ\\x80\\x99s now suitable for Fortnite or Overwatch-type gaming without any external boxes.The display is also impressive, delivering great color accuracy out of the box, making it suitable for creative applications. Youâ\\x80\\x99re also getting Thunderbolt 3 plus full-sized USB, a great trackpad and what seems to be decent battery life. Itâ\\x80\\x99s a really neat package overall.Most of the concerns I have with the hardware package are minor. The fan is too aggressive by default, though that can be adjusted. Full-sized HDMI has been removed. And youâ\\x80\\x99re only getting a 256GB SSD in the models without a 4K display, though because it's an M.2 drive it should be upgradeable.The other concern for some buyers will be the price. $1,400 for the base model without discrete graphics sets a big premium. Yes, you get the 25W Core i7-8565U, but thereâ\\x80\\x99s just 8GB of RAM and a 256GB SATA SSD. Asus offers the 15W 8565U plus 16GB of RAM and a 512GB SSD for $1,200 in the ZenBook 14 UX433FA. So youâ\\x80\\x99d have to really want the extra CPU performance to justify spending more while also sacrificing RAM and storage capacity. The RAM in particular hurts the Blade's chances to fight off value competitors.Then thereâ\\x80\\x99s the $1,600 graphics model, the one we reviewed. In some ways I can see this price being justified, not a lot of systems offer a 25W quad-core plus a full MX150 GPU in this sort of form factor. And youâ\\x80\\x99re getting 16GB of RAM. When you consider that a lot of laptops advertised as having an i7-8565U and MX150 will have the 15W CPU configuration and the slower 1D12 MX150, I do think thereâ\\x80\\x99s some merit in spending more money for the outright best performance.Of course, if you require less performance, you may be perfectly satisfied with the MX150 1D12 and even, say, a previous gen i7-8550U. This combination will end up 20 to 30 percent slower than the Blade Stealth, but you can find similar form factor laptops with that combination that are more than 30 percent cheaper.Itâ\\x80\\x99s not unusual for top-end products with the fastest hardware to offer less than optimal bang for buck; thereâ\\x80\\x99s always a premium associated with getting the fastest or best products. The question is whether youâ\\x80\\x99re willing to pay extra, or whether youâ\\x80\\x99d be satisfied with the multitude of other options on the market.\"],\n",
       "  [\"Power Consumption and OverclockingAs expected the new 8-core models are serious power pigs. The 9700K matched the 7820X with a system draw of 235 watts when running our HandBrake workload. The 9900K pushed consumption 9% higher hitting 255 watts which is 13% more power than the 16-core Threadripper 2950X system consumed.We find a similar story when testing with Blender though the AMD CPUs did perform better in this application and as a result the 2950X was able to work a bit harder, but even so it still consume less power than the 9900K. Thatâ\\x80\\x99s enough power consumption data for the moment, letâ\\x80\\x99s check out overclocking performance.Overclocking these 8-core parts to 5.1 GHz wasnâ\\x80\\x99t easy, it required 1.375v and a massive liquid cooler, you arenâ\\x80\\x99t hitting this frequency with a 240mm closed loop cooler, 5 GHz is probably off the table as well, but we will talk about thermal performance soon.Looking at the multi-threaded results the 9900K saw an 8% performance boost while the 9700K saw a 7% boost. I should also note that I have two 9900K samples and both struggled with the 5.1 GHz overclock. They could boot into Windows at 5.2 GHz and run a few basic tests but anything more would result in the blue screen of death, even at 1.45v.Moving on to Corona the 9900K was 7% faster once overclocked while the 9700K enjoyed a 9% performance bump.Finally we have the Premiere results and again the 9700K saw a 9% performance boost and the 9900K an 8% boost. So only single digit gains making it hard to justify the increased power consumption and operating temperatures.Speaking of power the 9700K configuration consumed 15% more power once overclocked and the 9900K an additional 19% taking the total system consumption to 294 watts.TemperaturesWhen it comes to operating temperatures I have nothing but bad news. these 8-core CPUs might have a STIM pack, I mean soldered thermal interface, but you wouldnâ\\x80\\x99t necessarily know it. Stock out of the box with either a premium air-cooler or a recent closed-loop liquid cooler youâ\\x80\\x99re look at load temps well into the 80â\\x80\\x99s and overclocking is basically out of the question. Sure 5 GHz might be okay for games but if youâ\\x80\\x99re placing all 8-cores under prolonged stress temperatures will hit 100c, and I was testing in a relatively cool room inside a well ventilated case.Using a custom liquid cooling setup only reduced the stock operating temperature by 8 degrees and weâ\\x80\\x99re talking about a $400 - $500 kit here.It was possible to run at up to 5.1 GHz but even then temperatures were stick knocking on the door of 100 degrees which is obviously insane. So the 9900K might be fast but good luck keeping it at a reasonable temperature. We ran out of time to test thermal performance of the 9700K, but we'll include that data in future content, for now letâ\\x80\\x99s move onto games.\"],\n",
       "  [\"Itâ\\x80\\x99s time to evaluate the Radeon RX 570 all over again (we just did last week), because apparently we didnâ\\x80\\x99t do it right the first time. Many were caught out by the fact that the RX 570 is cheaper than the GTX 1050 Ti right now. Admittedly, itâ\\x80\\x99s very surprising considering the RX 570 walks all over the affordable GeForce, but thatâ\\x80\\x99s the current situation and maybe we should have brought up the comparison as soon as we detected this convenient anomaly.During the mining craze, it was really hard to find even budget Radeons, so we turned to the 3GB GTX 1060 as its limited memory buffer made it useless for most mining operations. This meant pricing held steady and as a result it was the best value budget mid-range option for quite some time.Today though, the GTX 1060 3GB is commonly found at $200. The RX 570 is around $150-170, which matches the GTX 1050 Ti (~$170) though it recently got destroyed in a head to head comparison with the RX 570. Perhaps the only real competition for the RX 570 is the RX 580. Thereâ\\x80\\x99s an 8GB PowerColor model selling for just $180 right now with most models priced between $200 and $210. Finally, the fully fledged 6GB 1060â\\x80\\x99s typically cost $240 to $250.Weâ\\x80\\x99re going to put all these mainstream GPUs head to head in 36 games at 1080p and 1440p to see which comes out on top in terms of performance and value. Our test rig for this benchmark session consisted of a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. Drivers we used were Adrenalin 2019 Edition 19.1.1 for the Radeon GPUs and Game Ready 417.35 WHQL for the GeForce GPUs.BenchmarksSomething to note across all graphs is that graphics cards arenâ\\x80\\x99t ordered from fastest to slowest, instead we've kept the order static so they're easier to follow.Starting with Shadow of the Tomb Raider we see that the RX 570 is able to edge out the GTX 1060 3GB, though the margin is small. Meanwhile the RX 580 is a good bit faster than the 6GB 1060, providing 9% more frames at 1080p.Strange Brigade is a well optimized game and although itâ\\x80\\x99s an AMD sponsored title it works very well with Nvidia hardware. Here the RX 570 outpaced the 3GB GTX 1060 as well as the 6GB model, meanwhile the 1050 Ti was left trailing by a mile. The RX 570 was just 6% slower than the 580.Battlefield V is another title where these affordable Radeon GPUs stack up very well against the competition. The RX 580 blitzed the 6GB 1060, while the RX 570 just outpaced the 3GB 1060 at 1080p, but beat it quite comfortably at 1440p. The 570 was basically on par with the 6GB 1060 here.Sniper Elite 4 is another game where the Radeon GPUs do very well. Here the RX 570 matched the 1% low performance of the 3GB 1060, while beating it for the average frame rate convincingly at 1080p.The RX 580 also beat the 6GB 1060 by a comfortable margin and was even 18% faster than the 570 at 1080p.Performance in Monster Hunter World is competitive between the RX 570, 580 and GTX 1060. The 3GB 1060 edges out the RX 570, while the RX 580 beats the 6GB 1060, but bang for your buck the RX 570 stacks up very well.At 1080p we see that AMD GPUs enjoy slightly better frame time performance in Warframe, though the 1060â\\x80\\x99s did provide the best average frame rate performance. Frame time performance improves for the GeForce GPUs at 1440p and here the 3GB 1060 beats the RX 570 by a comfortable margin.The Just Cause 4 performance was also close. Here the RX 570 and GTX 1060 3GB were neck and neck, while the RX 580 was 7% faster than the 6GB 1060 when looking at the average frame rate.Grand Theft Auto V provided the RX 570 with its smallest win over the GTX 1050 Ti, so itâ\\x80\\x99s no surprise to find the GTX 1060 pulling ahead here. The 3GB 1060 was able to beat the RX 580, though the frame time performance was nearly the same. Put in other words, GeForce cards are hands down the better choice for playing GTA V.Assassinâ\\x80\\x99s Creed Odyssey is another title that doesnâ\\x80\\x99t work that well with AMD hardware, despite the fact that itâ\\x80\\x99s an AMD sponsored title. The RX 580 gets trounced by the 6GB 1060, though the 3GB version of the card struggle with some pretty ugly frame time results at 1440p and 1080p.The Hitman 2 results are quite competitive. Here the 3GB 1060 edged ahead of the RX 570 at 1080p, though it did fall behind at 1440p. The RX 580 was able to beat the 6GB 1060 at both resolutions.Radeon GPUs have always done well in Rainbow Six Siege and here we see the RX 580 trashing the 6GB GTX 1060, though the 3GB model stacks up very well against the RX 570. Now, we should note that the ultra quality preset uses a default render scale of 50, not 50%, just 50, what this means is 1920x1080 becomes 1360x764. The game uses temporal upscaling, so you end up with an image thatâ\\x80\\x99s rescaled to the output resolution.We used to test with these settings as this is the default mode, but many Rainbow Six Siege players complained that our frame rates were too high, not realizing that they had manually configured these options and that the maximum in-game preset actually sets the render scale almost 30% lower than the output resolution. So now we select the Ultra preset and also adjust the rendering scale to 100 every time.This makes the game very VRAM hungry. At 1080p the game calls for at least 6.4 GB of memory, so the 3GB 1060 and 4GB RX 570 get hit pretty hard. Itâ\\x80\\x99s not often the 8GB RX 580 offers over 40% more performance than the 4GB RX 570 at 1080p, but thatâ\\x80\\x99s the situation using these settings. In the end, the RX 580 easily beats the 6GB 1060, while the 3GB 1060 and RX 570 are comparable across the two tested resolutions.The last game weâ\\x80\\x99re going to discuss the results for is World of Tanks and here all GPUs performed well at 1080p. Both GTX 1060 models easily beat the Radeon competition, particularly at 1080p. The 3GB 1060 for example was 22% faster than the RX 570. That margin was reduced at 1440p to 16% which is still a solid win for Nvidia.Performance SummaryIn our recent Radeon RX 570 test, we found this budget GPU to be 43% faster than the GeForce GTX 1050 Ti on average. That metric was the result of calculating the percentage difference per game and then averaging those figures by the number of games tested.Since we're comparing not two but five GPUs this time, we will do our math a bit differently. Instead weâ\\x80\\x99re simply taking the fps data from all the games and then averaging those figures. This sees the RX 570 come out on top by a 41% margin agains t the GTX 1050 Ti, so as you can appreciate, there's a difference but the outcome should be the same.The 3GB GTX 1060 was on average 4% faster than the RX 570 across the 36 games tested, giving it a 3 fps advantage. Then we see a further 14% jump in performance to the RX 580 8GB and GTX 1060 6GB, both of which averaged 82 fps across the 36 game sample.This made the 8GB RX 580 on average 19% faster than the 4GB RX 570 and this makes sense given the 580 packs 13% more cores with an 8% core clock speed advantage, and we saw a few instances where having twice the VRAM was also advantageous.Before moving on, here is a quick breakdown of how the RX 570 compares to the other GPUs tested on a per game basis. In our previous article, we saw how it dominated the GTX 1050 Ti across all 36 games, and as I said earlier when calculating the percentage difference per game we end up with a 43% performance advantage in favor of the RX 570 due to the rounding error.Against the GTX 1060 3GB, the RX 570 was 4% slower on average and on the chart below you can clearly see where the Radeon GPU got the better of the 3GB 1060, and where the GeForce GPU came out on top.The RX 570 enjoyed big wins in Forza Horizon 4 and Hitman, while it lost by a convincing margin in Total War Saga, GTA V and Fortnite.As you might expect the performance deficit is extended when compared to the 6GB GTX 1060...In a mainstream Radeon vs. Radeon comparison, the RX 580 against the RX 570, the latter was on average 14% slower.Right so thatâ\\x80\\x99s how they stack up across a wide range of games and those are certainly the margins you can typically expect to see. The only question left to answer now is, how do they stack up in terms of value, and to answer that letâ\\x80\\x99s check out the cost per frame figures.Finding the Best Value: Cost Per FrameTo put together the cost per frame chart, we used all US dollar amounts taken from Newegg listings. We ignored the absolute lowest price if there was a single model available at that price which we hope is the most accurate and fair representation of current prices.At a current retail price of $150 the Radeon RX 570 costs just $2.17 per frame, but even if we used the $170 MSRP it still only comes to $2.46 making it 11% cheaper than the 3GB GTX 1060.The RX 580 is also exceptional value at the current $190 asking price. Itâ\\x80\\x99s just 6% more expensive per frame than the RX 570 and an impressive 21% cheaper than the GeForce GTX 1060 6GB. At the bottom of the graph we see that the GTX 1050 Ti does not offer good value at the current asking price. At $170 it's almost 60% more expensive per frame than the RX 570, so for that GPU to make sense it should be selling closer to $140.Taking those figures into account (and the fact that good FreeSync monitors should prove very popular with gamers all of a sudden) we find that if youâ\\x80\\x99re looking to spend less than $200 on a graphics card, itâ\\x80\\x99s hard to go past the $150 RX 570.Nvidia has said that GTX 10 series cards will be running out of stock soon (matter of weeks), but for that to happen at the mainstream, some form of drop-in replacement GPUs will have to be introduced. For now, we'd avoid the 3GB GTX 1060 given that the limited VRAM buffer is already a problem and will become more of an issue with many upcoming titles, even at 1080p. Even 4GB is right on the edge, so if you can stretch the budget, we recommend snapping up an 8GB RX 580.\"],\n",
       "  [\"I donâ\\x80\\x99t try to be the best in battle royale games. I donâ\\x80\\x99t troll or anything, but Iâ\\x80\\x99m not as concerned with skillful wins as I am with having fun or coming away with an exciting story. But Apex Legends makes me want to try. I also get some great stories in the process.Apex Legends is a battle royale from Titanfall developer Respawn. Itâ\\x80\\x99s loosely related to that seriesâ\\x80\\x99 world, but the connections aren't all that important. Twenty teams of three heroes, known as Legends, face off on a map. Each Legend has different abilitiesâ\\x80\\x94healing drones, air strikes, shielding domesâ\\x80\\x94that can give your team an advantage in battle. The hero aspect of the game is a change for battle royales, but other than that, the basics of Apex Legends are standard for the genre: scooping up weapons and attachments, outracing an encroaching circle, and murdering your way to the last team standing.Apex is a tasting menu of battle royale moments, rather than the potato chip jump-die-restart of my Fortnite experiences. There are countless moments to surprise or disappoint yourself. If I donâ\\x80\\x99t die immediately upon landing on the gameâ\\x80\\x99s map, failing to find a gun before someone else does, I tend to last until well into the endgame, surviving multiple brushes with death before Iâ\\x80\\x99m downed.The gameâ\\x80\\x99s Legends are compelling to choose between, but they lack the personality of Overwatchâ\\x80\\x99s characters or the visual flair of Fortniteâ\\x80\\x99s myriad skins. You can customize them to an extent, giving them different voice lines and outfits, but I've yet to find any combination of customizations that makes even my favorite Legend, the healer Lifeline, feel like mine. Instead of being defined by who they are or how fans can customize them, Legends are defined by what they do.Do you want to be hyper-mobile? Choose Pathfinder for his ziplines or Wraith for her portals. Like using your environment? Deploy Bangaloreâ\\x80\\x99s smoke launcher or Causticâ\\x80\\x99s gas traps. Prefer to use stealth and trickery? Try Bloodhoundâ\\x80\\x99s tracking abilities or Mirageâ\\x80\\x99s decoys. Legends are bodies rather than characters, but those bodies are compelling to inhabit.Unlike the cartoon clumsiness of Fortnite or the technical crawl of PUBG, Apex charactersâ\\x80\\x99 movement feels balletic. They can run quickly, slide down hills, and mantle seamlessly up high walls and over obstacles. Thereâ\\x80\\x99s no fall damage, and itâ\\x80\\x99s dizzying and thrilling to leap from impossible heights and keep moving. The gameâ\\x80\\x99s map feels built for athletic joy, with balloons to rappel up to and dive from into towns designed for shifting from low to high ground. You can cross swathes of the map at a clip, move in and out of fights fluidly, and reposition yourself like a ninja to get that final shot off.The capable, creative bodies Apex gives me inspire me to live up to them with my tactics. I take risks I donâ\\x80\\x99t take in other battle royales. I weave through an explosive airstrike to rescue a teammate. I slide down long hallways to escape danger. I leap from a cliff to surprise an enemy team below. At this point, even just walking around the game makes me feel like a badass.The actual shooting, in contrast, feels less freeing. Aim drifts, guns recoil wildly, and your magazine is laughably small. You need to find the right attachments and add-ons to counteract weaponsâ\\x80\\x99 natural tendency to rebel. Even with the perfect load-out, you can find yourself firing frantically in a fight while barely doing damage. The contrastâ\\x80\\x94how wonderful and skillful it feels to simply move in the game, versus the attention and control necessary to actually succeed in combatâ\\x80\\x94has sent me down YouTube videos and Reddit threads, back to the gameâ\\x80\\x99s brief tutorial to test weapons and study bullet spray, to the wall by my computer with tape and printouts of weapon rankings and attachment lists.As if in concession to its unpredictable weapons, Apex Legends is forgiving in other ways. You find armor and helmets of different rarities, from common white to purple epic to the legendary gold shield that can let you self-revive if youâ\\x80\\x99re downed. Additional health kits and shield recharges let you top yourself up mid-fight. As a result, Legends are hardy. Friends Iâ\\x80\\x99ve played with have remarked on how many hits we can take or dish out without dying or felling an opponent. I can fall and be revived, down an enemy only to have them down me minutes later. My team can be outgunned and sent packing, then demolish the next enemy squad we face, then find ourselves outmatched yet again by the next one.In Apex, down isn't out. Like other battle royales, you have a certain amount of time after being downed during which you can still be revived. Once youâ\\x80\\x99ve officially died, your teammates have even more time to grab your â\\x80\\x9cbanner,â\\x80\\x9d a customizable tablet representing your character, and take it to one of the mapâ\\x80\\x99s respawn beacons. From there, you can be brought back to life, sans your gear, and stay in the game.You have to calculate the risks of this system. Is the enemy leaving your downed teammate alive as bait? Are they going to camp the respawn beacon and catch you then? This tension creates some memorable scenarios. Once, I raced back into the circle to scoop up my friendâ\\x80\\x99s banner the second before it expired and took them out of the game for good. Another time, I weaved through bullets like Neo from The Matrix to snap up both my teammatesâ\\x80\\x99 banners as two other teams fought over their loot.The respawn system also inspires loyalty, especially in a game where youâ\\x80\\x99re unlikely to last long as a lone wolf. In most of my matches, a teammate has resurrected me instead of leaving me to die. Once, after fetching my banner, a teammate said to me over voice chat, â\\x80\\x9cDonâ\\x80\\x99t worry, I wonâ\\x80\\x99t loot your stuff.â\\x80\\x9d When I laughingly asked why, they responded, â\\x80\\x9cEthics, man.â\\x80\\x9d I was able to respawn and recover my hard-earned gear.Apex Legends encourages this intimacy, both because you need your teammates and because they have so many ways of talking to you. Apex Legends has voice chat, as well as a ping system, which lets you communicate by placing markers instead of talking. Success in all battle royales hinges on team communication, and Apex Legends makes it easy to find a way to talk that you feel comfortable with. With a click of my mouse I can indicate enemiesâ\\x80\\x99 locations, loot, where I want to go, what gear I need. I have voice communication muted in Overwatch and Fortnite, but not in Apex Legends. In part this is because thereâ\\x80\\x99s no non-clunky option to mute others, which, along with a current lack of in-game report feature, means it can be hard to avoid toxic players. On the other hand, I donâ\\x80\\x99t often need to mute voice chat because relatively few Apex players bother to use it. There are many easy ways to communicate non-verbally, which might be why, in my experience, only the friendliest or most aggressive players seem to get on chat.This intimacy is bolstered by a startling amount of transparency between players about their desires and in-game choices. At the start of a match, you have a limited amount of time to choose a character; by hovering over their icon, you can indicate who you want to pick, which your teammates can either respect or override. Once you select, youâ\\x80\\x99re shown your teammatesâ\\x80\\x99 rank and their stats, indicated by customizable trackers revealing damage dealt, kills, revives, or character-specific successes, such as kills while in tracker Bloodhoundâ\\x80\\x99s â\\x80\\x9cBeast of the Huntâ\\x80\\x9d ultimate ability or healing youâ\\x80\\x99ve done with Lifelineâ\\x80\\x99s tactical drone. You also see the matchâ\\x80\\x99s â\\x80\\x9cchampion,â\\x80\\x9d a high-ranking player; taking them out will earn you extra experience.Banners can give you insight into how good another player is or what their gameplay priorities are. Trackers and their unlocks are character specific, so they donâ\\x80\\x99t necessarily paint a full picture of a given playerâ\\x80\\x99s skill, but itâ\\x80\\x99s still much more information than other battle royales Iâ\\x80\\x99ve played have shown me about the people responsible for my in-game life. Sometimes, seeing the skill gap between random teammates and myself can feel like a lot of pressure, which either inspires me to try my best or sends me into a spiral of anxiety (or both, in turns).In a recent match, a lower-level player and I were teamed up with a highly ranked player who had hundreds of kills. Over the gameâ\\x80\\x99s text chat, the higher-ranking player told us we had â\\x80\\x9cfuck boi statsâ\\x80\\x9d and disconnected. This encounter heralded a night of toxic match after toxic match, with other teammates criticizing my character pick and my stats, swearing at me when I made a mistake, refusing to recover my banner, or disconnecting from the match the moment they died instead of giving me a chance to respawn them. Sometimes, other playersâ\\x80\\x99 nastiness makes me choke; Iâ\\x80\\x99ll do worse when I hear teammates berate me for every whiffed shot or less-than-ideal choice of landing spot. Other times, this pressure makes me try harder, feeling a need to prove myself. Once, when my teammate with hundreds of kills got downed, I swooped in to finish off their attacker and revive them; their praise in that moment meant more to me than I ever expected, even though it was really just a stranger typing â\\x80\\x9cthanks.â\\x80\\x9dI've experienced this pressure from the other side, too. When Iâ\\x80\\x99m teamed with newer players, I feel a sense of protectiveness and, also, a need to rise to the occasion and be an infinitely better player than I am. Recently, I found myself in a match with two new players, who both revealed themselves to be seemingly minors when they started using voicechat. They marvelled over my paltry number of kills and asked me how I was â\\x80\\x9cso good at the game.â\\x80\\x9d They even asked if I was superstar streamer Ninja, despite my clearly non-Ninja Origin handle.I held back on responding for a while, feeling weird about being an adult man playing with two kids. But their questions were so energetic and openâ\\x80\\x94Whatâ\\x80\\x99s armor? Why does that lady keep mentioning a ring? My mouse wheel is broken, what should I do?â\\x80\\x94that I finally hopped onto voice to explain the game. I made sure to be hyper-conscious of my language, drawing on my experience as a teacher from over a decade ago. Armor lets you take more damage, I told them. The ring is that glowing orange wall and we should avoid it, but it wonâ\\x80\\x99t kill us right away. You can press the number keys.While looting, one of them fired a full clip into the air for no reason. I leapt into protective mode, scanning the horizon with my sniper rifle and urging them to get into cover in case anyone heard us. I felt a fierce loyalty to them as we looted our way across the map, calling out what I found and then explaining to them why they might want it. Through a good choice of landing spot, we actually made it into the final three teams, even as I kept having to explain why they should run away from the ring instead of into it.When gunfire sounded nearby, my heart leapt into my throat. I urged them to hang back as I scouted up ahead, feeling the full weight of their first experience of the game upon me. When they started shooting despite my adviceâ\\x80\\x94kids these days!â\\x80\\x94I leapt into action. I ziplined across a ravine and slid into cover to the enemiesâ\\x80\\x99 flank, then popped out and drew fire. I danced between buildings and pinged enemies, attracting as much attention as I could. Eventually I was downed, and they both died seconds later. I turned the game off immediately, feeling way more intense emotions than I expected or even necessarily wanted from a quick turn through a battle royale. But it was also a better match than I had ever played before: riskier, more tactical, more meaningful. For that one match, Apex Legends brought out my best.In a recent chat with some of my Kotaku colleagues about battle royales, I said that the community of a game is what will make me stick with or leave it. As Apex Legends finds its footing, its community could become anything. According to Respawnâ\\x80\\x99s roadmap, the game will see seasons and a battle pass, which could mean new cosmetics, loot, weapons, and characters. The game will change, and its community will change with it. Two months from now, it could be the same serious landscape of Call of Duty or the same after-school playground of Fortnite.Right now, thereâ\\x80\\x99s a lot of plurality in the Apex Legends community. It has its assholes and its heroes. Anything could happen. All I know is, the game asks for my best. And as it becomes itself, I want to live up to wherever itâ\\x80\\x99s going.\"],\n",
       "  [\"Today weâ\\x80\\x99re taking a look at another gaming laptop, the brand new Alienware m15, which is the company's portable laptop offering to rival systems like the MSI GS65 Stealth, Gigabyte Aero 15 and Razer Blade. Historically we havenâ\\x80\\x99t been huge fans of Alienwareâ\\x80\\x99s chunkier laptop designs, but the m15 is one of the best weâ\\x80\\x99ve seen from the company so far.Let's talk specs to begin with. If youâ\\x80\\x99ve seen reviews of this laptop from a few months back, those were probably for the original model that used GTX Pascal GPUs. What we're reviewing today is the RTX version, more specifically the RTX 2080 Max-Q model. But there are other options available that might suit your budget better, including RTX 2070 Max-Q and RTX 2060 options.Like most Dell laptops, the m15 is highly customizable. The CPU for most models stays the same as an Intel Core i7-8750H, but from there you get many storage, RAM and display options. Our review unit came with 16GB of dual-channel DDR4, plus a combination of a 500GB SSD and a 1TB hard drive. The display is a 15.6-inch 1080p 144Hz IPS.Build and DesignThe m15 is Alienwareâ\\x80\\x99s most portable 15-inch gaming laptop design yet. While this is definitely a good thing, itâ\\x80\\x99s probably only good when comparing the m15 to Alienwareâ\\x80\\x99s other laptops. For example, the regular Alienware 15 is 30mm thick and 7.7lbs heavy (!), whereas the Alienware m15 is about 27mm thick and crucially just 4.8 lbs (2.2kg). The weight difference is massive, which makes the m15 a significantly more portable system.However if you compare it to say, the Gigabyte Aero 15, the Alienware 15 is larger in all dimensions. Gigabyteâ\\x80\\x99s system is only 22mm thick and weighs just a tad over 2.0kg. Itâ\\x80\\x99s also smaller due to slimmer bezels. The Alienware 15 doesnâ\\x80\\x99t give you that awesome slim-bezel design that weâ\\x80\\x99ve been seeing from other laptops, which keeps it larger than its competitors.Weâ\\x80\\x99d say this new unit is a big step forward for Alienware but itâ\\x80\\x99s still got a way to go until it matches the best from its competitors. Something like the MSI GS65 is nearly a pound lighter and for some people that could be a better option.However, there is no faulting the build quality here, which is superb. This isnâ\\x80\\x99t a unibody construction and there are a few different choices of materials, including metal for the lid, a soft touch matte plastic coating around the keyboard, and glossy plastic around the display. But the key here is just how seamless these materials join together, it just feels really solid and well built.Visually this isnâ\\x80\\x99t our favorite design, mostly due to the angular body and a gamery lid. It looks quite nice when opened but it doesnâ\\x80\\x99t do enough to take the crown from the elegant MSI GS Stealth line. Itâ\\x80\\x99s definitely the best Alienware design and the focus on slimness has helped a lot there.In other areas, Alienware has done a lot of things right. The keyboard has a nice though somewhat spongy tactile response, however the layout is good and it includes a full numpad plus four macro keys. The entire thing is lit up with four-zone RGB backlighting. Thereâ\\x80\\x99s two other RGB elements as well: the power button and alien head logo on the lid.The trackpad is a little small given the space allocated to it, but it has a nice coating and is very responsive. The speakers are found in the bottom corners of the laptop and they do fine at medium volume but suffer a little bit from distortion at high volumes, if you end up using them at all.The I/O selection is fantastic, weâ\\x80\\x99re getting three USB 3.1 type-A ports, Thunderbolt 3, HDMI 2.0, mini-DP 1.3, a 3.5mm audio jack, Ethernet and Alienwareâ\\x80\\x99s Graphics Amplifier port. We particularly like that the display outputs are on the rear of the laptop, itâ\\x80\\x99s nice and convenient to hook up a display without obstructing either side.PerformanceLetâ\\x80\\x99s talk performance, and as always itâ\\x80\\x99s important to set the scene with the settings we used. The main consideration are the fan profiles, with Alienware providing four options: Balanced, which is the default, plus Cool, Quiet and Performance modes. We didnâ\\x80\\x99t spot any significant difference between the Balanced and Performance modes because both seem to ramp up the fan to the same level while gaming. The Performance mode simply sets the fan to this speed at all times, while Balanced quietens the fan during less intensive workloads. Our recommendation is obvious, to use the Balanced mode and thatâ\\x80\\x99s what we used throughout this review.When it comes to CPU performance, knowing that most other flagship gaming laptops rely on the same Core i7-8750H processor, the main consideration was to see whether the Alienware m15 performed any differently in productivity tasks compared to its competitors. What we discovered is that the laptop performs identically to our average 8750H benchmark numbers, suggesting the system is performing as expected. Across each workload itâ\\x80\\x99s a little slower some times and a little faster at other times, but on average there is nothing to be concerned about here.This means weâ\\x80\\x99re getting all the usual benefits of the Core i7-8750H. As a six-core CPU, itâ\\x80\\x99s ideal for tasks like video encoding, where itâ\\x80\\x99s 40 to 50 percent faster than a last-generation Core i7-7700HQ. Itâ\\x80\\x99s also faster in single-threaded workloads by around 10 percent, which does help reduce CPU bottlenecks in some games.Where the interesting stuff starts to happen is with the GPU. The m15 packs the GeForce RTX 2080 Max-Q in its 90W configuration. If you arenâ\\x80\\x99t aware, there are actually two versions of the RTX 2080 Max-Q that have been spotted, the default 80W configuration and the 90W model which has a higher power limit and is clocked about 100-200 MHz higher. Itâ\\x80\\x99s almost impossible to tell which version a laptop uses from its product page, but we can tell you the Alienware m15 uses the faster model.If youâ\\x80\\x99ve read our review of the GeForce RTX 2080 Max-Q youâ\\x80\\x99ll see how both the 80W and 90W versions perform in a range of games. However with the Alienware m15 itâ\\x80\\x99s a bit more complicated than that, because while this laptop does have the 90W version, its cooling solution in a typical use case prevents it from unleashing the full power of this GPU.Even with the fans at full speed, there just isnâ\\x80\\x99t enough of a gap between the base of the laptop and the desk to intake air at the volume required to cool these components. Most of the intake vents are along the bottom panel, with a few tiny vents on the top side, with air then exhausting out the back and sides. But thanks to the feet raising this laptop by a smaller than usual height, and fewer vents on the top side of the laptop, weâ\\x80\\x99re running into a few airflow bottlenecks.What this means is the Alienware m15 in a typical usage environment on your desk is 4% slower on average than the â\\x80\\x98maximumâ\\x80\\x99 performance the RTX 2080 Max-Q 90W variant is capable of. However it is possible to get the full performance of the 2080 Max-Q if you raise the base of the laptop slightly, which gives the bottom vents less restricted access to airflow. You donâ\\x80\\x99t have to change any other laptop settings, just make this physical adjustment. If you do this, the cooler performs much better and you get the full performance of the GPU inside. Itâ\\x80\\x99s not a very practical solution and we feel this issue could have been resolved with a better optimized series of vents around the design.For the rest of this data, we're showing how the Alienware m15 performs without the workaround because we feel this is how most people will use the laptop. The good news is that when sitting on your desk normally, the Alienware m15 performs about the same as the RTX 2080 Max-Q 80W variant. So itâ\\x80\\x99s not choked to the level that it performs below a typical RTX 2080 Max-Q, itâ\\x80\\x99s merely the same.With the Alienware m15 performing roughly the same as an 80W RTX 2080 Max-Q, the margins between this laptop and other GPUs is pretty similar. Itâ\\x80\\x99s ~10% faster than the RTX 2070 Max-Q, so if you are tossing up between the various Alienware m15 GPU configurations, this is something that will interest you.The other available GPU is the RTX 2060. Here the RTX 2080 Max-Q is 16% faster on average, although it doesnâ\\x80\\x99t win in every game. For example, Hitman 2 is unusually slow with the Alienware m15 compared to the average result for other laptops weâ\\x80\\x99ve tested. On the other hand, there is a significant lead in games like Shadow of War, where the 2080 Max-Q is more than 30% faster.The previous GTX-based Alienware m15 was available with the GTX 1070 Max-Q inside. This new RTX 2080 Max-Q model is a decent 24% faster on average, which is an impressive improvement from the same sort of cooler design and form factor. The RTX 2080 Max-Q models are a lot more expensive however.Finally we have the comparison between the RTX 2080 Max-Q and the standard GTX 1070 for laptops. The RTX 2080 Max-Q is 6% faster on average but itâ\\x80\\x99s a bit of a mixed bag: it wins in some titles, and loses in others. Itâ\\x80\\x99s definitely not a situation where the RTX 2080 demolishes the GTX 1070, which is the case when comparing the desktop variants of these GPUs.Cooling Setup: Could use some workHere is the data showing the Alienware m15â\\x80\\x99s cooling solution isnâ\\x80\\x99t up to scratch for the hardware inside. GPU temperatures are okay without being great, 84 degrees Celsius is fine for most gaming laptops. But itâ\\x80\\x99s the CPU temperatures that are seriously crazy, with the m15 hitting 99 degrees during a Watch Dogs 2 gaming session. Any time a laptop is up around that 100 degree mark, youâ\\x80\\x99re going to get throttling.So with this laptop itâ\\x80\\x99s not the GPU but mostly CPU throttling to be concerned about. Raising the laptopâ\\x80\\x99s base which provides more airflow to the choked cooler (without increasing fan speed) sees a significant 7 degree drop on the CPU and a similar drop on the GPU, but itâ\\x80\\x99s more than that: CPU clock speeds also increase by 500 MHz or so across all six cores.Itâ\\x80\\x99s a huge difference and just shows how in a standard usage case this cooler cannot keep up with the demands of the high-TDP parts inside. Without that CPU bottleneck we see the full performance of these components.And itâ\\x80\\x99s no surprise why the Alienware m15 canâ\\x80\\x99t push the fan any faster: weâ\\x80\\x99re already sitting at 46 dBA during a gaming load, which is on the higher end for a gaming laptop. So this system is both hot and loud, giving basically no room to move. If youâ\\x80\\x99re hoping to overclock this systemâ\\x80¦ well, thereâ\\x80\\x99s zero headroom for that.Those that like to undervolt and replace the thermal compound with something like liquid metal may be able to get more out of this cooler design. Itâ\\x80\\x99s also possible that the lower-tier GPUs like the RTX 2060 wonâ\\x80\\x99t run into this same thermal bottleneck but itâ\\x80\\x99s hard to say for sure.Storage, Battery & DisplayMoving on to storage performance, our Alienware m15 came equipped with a 512GB SK Hynix PC401 SSD which performed pretty well, no real complaints here. Itâ\\x80\\x99s not the absolute fastest gaming laptop SSD weâ\\x80\\x99ve seen, particularly for sequential workloads, but itâ\\x80\\x99s adequate for most people.Battery life was pretty disappointing though. The m15 comes with two battery options, if you want a 1TB hard drive inside, you get just a 60 Wh cell, which was the case with our review unit. If you forgo the 2.5-inch drive bay slot you can increase the battery capacity to 90 Wh, which no doubt delivers better results than the disappointing sub-4 hour run time in our video playback test.Given that internally you get two M.2 slots, one of which was free in our unit and both of which are easily user accessible, we think most buyers would be better off opting for the 90 Wh model and using cheap M.2 storage options to get extra space if necessary. Especially if you want better battery life which we feel is important in a portable-oriented system like the m15.Like a lot of similar laptops, Alienware is using a 15.6-inch 1080p IPS with a maximum refresh rate of 144 Hz. The combination of IPS and 144 Hz works really well for gamers. The high refresh rate is perfect for the RTX 2080 Max-Q that often pushes frame rates above 100 FPS at 1080p, so this is the display option youâ\\x80\\x99ll want to choose.A lot of the basic characteristics are good: 300 nits of peak brightness is fine for indoor use, especially as thatâ\\x80\\x99s combined with excellent viewing angles. A contrast ratio of 1200:1 is also great for this type of display, however we did experience a bit of IPS glow when viewing dark or black content. Not a huge issue, but something to keep in mind.The bigger problem with this display is the lack of color calibration. The default white point is around 8000K which is far too cold, giving the display a blue tone. In fact there doesnâ\\x80\\x99t appear to be any calibration here whatsoever, at least not to the sRGB standard, as deltaE averages across the board are above 4.0.Normally for gaming laptops we donâ\\x80\\x99t find calibration to be a big issue, but with the Alienware m15 itâ\\x80\\x99s a little different. DeltaEs above 4.0 are higher than many other laptops, particularly the 5.7 greyscale deltaE average, so this display is particularly poorly calibrated. And on top of this, many competitors to this system â\\x80\\x93 such as the MSI GS65 and Gigabyte Aero 15 â\\x80\\x93 do come with factory calibrated displays. So while it may not bother every buyer, calibration is something you donâ\\x80\\x99t get here as part of the package.Closing RemarksOverall, we're in two places with the Alienware m15. This is clearly their best laptop design. Itâ\\x80\\x99s thinner and lighter than regular Alienware laptops, and thatâ\\x80\\x99s a step in the right direction. As most gaming laptop competitors have this sort of system on the market, coming from the chunky Alienware beasts, this focus on portability is welcome from a design perspective.On the other hand, we feel Alienware has failed to execute in terms of some of the performance characteristics. In particular, the cooler simply isnâ\\x80\\x99t powerful enough to cool both the Core i7-8750H and RTX 2080 Max-Q without throttling. Thereâ\\x80\\x99s an airflow issue causing lower than expected performance when both the CPU and GPU are utilized, like in games. This becomes less forgiving when you consider the m15 is larger and heavier overall than direct competitors.However, performance is not bad and itâ\\x80\\x99s still around the mark of a typical RTX 2080 Max-Q laptop. But it could have been better, especially as this laptop includes the 90W variant of the GPU, it could have been the best of its features. Future versions of this laptop will no doubt need a cooling layout revision.Build quality is great on the m15 but there's other odds and ends that arenâ\\x80\\x99t as polished. The display isnâ\\x80\\x99t calibrated and doesnâ\\x80\\x99t pack a slim bezel design. The keyboard and trackpad are okay without being amazing. The default option is a small 60 Wh battery and you have to pay to get a larger one.We donâ\\x80\\x99t see what the Alienware m15 does better than a laptop like the MSI GS65. Thereâ\\x80\\x99s no key feature that might get me to recommend it in some situations, like better value or a unique design, or top-tier performance. In every metric weâ\\x80\\x99ve tested, this system is beaten by the excellent MSI option which is basically the same price for an equivalent configuration, regardless of which RTX GPU you choose. And this is without discussing the overall value proposition of Nvidiaâ\\x80\\x99s Turing laptops.The Alienware m15 is a great step forward for the company, but it ultimately doesnâ\\x80\\x99t get our recommendation. Itâ\\x80\\x99s hard to create the perfect portable system on the first attempt, and many competitors have simply been doing this for longer and have more refined, better options today that you should consider instead.\"],\n",
       "  [\"If you're after an inexpensive graphics card to play games, which way do you go? Currently $150 will get you either the Radeon RX 570 4GB or the GeForce GTX 1050 Ti, both solid options that have been out for a long time, but not necessarily at this attractive price point.Today weâ\\x80\\x99re revisiting this battle in clearer detail as weâ\\x80\\x99ve been recommending AMD's RX 570 as the best budget graphics card for months, but have been doing so without a full comparison update to its nearest competitor, the GTX 1050 Ti, so letâ\\x80\\x99s address that.We're not discussing specifications of each GPU as it's safe to assume youâ\\x80\\x99re all very familiar with them. We first reviewed the GTX 1050 Ti over two years ago, while the RX 570 has been tested several times. But to give you better context, the twotimes we ran shootouts with this particular Radeon, it was against the more expensive GTX 1060 3GB (currently $200), so you can already tell the RX 570 at this price bracket will offer tremendous value.In total we tested 36 games at 1080p and 1440p, covering a vast number of game engines and genres. Out of those titles, weâ\\x80\\x99re going to discuss a portion of those results and then jump into our performance breakdown that includes all benchmark scores.Our test rig consisted of a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. Representing the green team is the MSI GTX 1050 Ti Gaming X 4G and for the red team, the MSI RX 570 Gaming X 4G. We used AMD's Adrenalin 2019 Edition drivers 19.1.1 and GeForce Game Ready 417.35 WHQL.As a side note, if you want a breakdown of the best GPU options on every price bracket, check out our Best Graphics Card feature. We'll update that in the coming month or so, when we have our RTX 2060 and Radeon VII tests in place.BenchmarksFirst up we have Shadow of the Tomb Raider, a super demanding title in terms of visuals and using the highest quality preset it was a bit overwhelming for the GTX 1050 Ti. While the GTX 1050 Ti was good for just 32 fps on average the RX 570 pumped out 52 fps making it 63% faster, thatâ\\x80\\x99s obviously a massive win for AMD.AMD enjoyed by far their biggest win in Strange Brigade (no surprise AMD included this title in their preliminar Radeon VII benchmarks). Here the RX 570 obliterates the 1050 Ti by almost doubling its performance at both resolutions. At 1080p the Radeon GPU was 93% faster and at 1440p almost averaged 60 fps, while the 1050 Ti struggled with around 30 fps. Itâ\\x80\\x99s quite incredible that the performance margins can be this extreme.The RX 570 also easily beat the GTX 1050 Ti when testing with Battlefield V, at either 1080p or 1440p. The 1050 Ti was barely able to provide playable performance at 1440p, while the RX 570 delivered the goods with over 50 fps at all times.Admittedly, for a competitive first person shooter thatâ\\x80\\x99s not enough, but these cards are intended for 1080p gaming and here the Radeon maintained over 60 fps at all times.Sniper Elite 4 is another DX12 title that can take advantage of async compute, so unsurprisingly the 570 roasts the 1050 Ti once again. Here it was 75% faster at 1080p, resulting in a night and day difference for the gaming experience.Monster Hunter World is a very demanding title but even so using the highest quality preset the RX 570 was able to provide playable frame rates at 1080p, the GTX 1050 Ti though, well not so much. Here the Radeon GPU was a whopping 62% faster.Both GPUs performed well in Warframe at 1080p and although the RX 570 was 35% faster the 1050 Ti did just fine with 91 fps on average. The performance difference was more noticeable at 1440p, although the RX 570 was still 35% faster than margin was more impactful at the lower frame rates.Just Cause 4 is not our favorite title when it comes to optimization, but still the RX 570 was able to provide very playable performance at 1080p. The same canâ\\x80\\x99t be said for the GTX 1050 Ti however.We were going to discuss the Forza Horizon 4 results but youâ\\x80\\x99ll see the margins for that title in the breakdown graph at the end of the article. Instead we picked GTA V since this is a title where the GTX 1050 Ti previously bested the RX 470.It appears that through driver development and a factory overclock, the 570 is able to stick it to the 1050 Ti, offering 10% more performance.Assassinâ\\x80\\x99s Creed Odyssey is typically a bad title for AMD cards, however the RX 570 is sufficiently faster than the GTX 1050 Ti that even here it manages comes out with a win, offering 17% more performance at 1080p.Using the 'very high' quality preset the Radeon RX 570 was able to keep frame rates above 30 fps at all times and therefore provide what we call playable performance, the GTX 1050 Ti averaged 35 fps but with frequent dips below that mark, gameplay was very choppy.Next up we have Hitman 2 and despite the lack of DX12 support for this most recent installment in the series the RX 570 storms home for an easy win, beating the GTX 1050 Ti by a 41% margin.The previous version of Hitman did support DX12 and using it we find a pretty brutal result. Here the RX 570 crushed the GTX 1050 Ti by an incredible 64% margin.The good news for Fortnite players is that either of these GPUs will allow for playable performance at 1080p. Fortnite uses the Unreal Engine 4 which heavily favors Nvidia hardware, yet the RX 570 was 23% faster when looking at average frame rates in this popular mainstream title.Both the GTX 1050 Ti and RX 570 provide very playable performance at 1080p in Rainbow Six Siege, but with a 28% performance advantage the RX 570 enables a noticeably better gaming experience.The last game weâ\\x80\\x99re going to discuss is World of Tanks. Here the Radeon offered 33% more frames at 1080p, though both cards provided excellent performance. At 1440p the RX 570 was noticeably better hitting 66 fps on average opposed to just 48 for the 1050 Ti.Performance SummaryUpon release the GTX 1050 Ti was not as popular as its more expensive sibling, the GTX 1060 which arguably offered a better performance vs price ratio. However as Radeon GPU prices got blown up by mining demand, the GTX 1050 series became very popular amongst gamers on a budget.Radeon pricing has since fallen back down to regular levels, and in the case of the RX 570 it can often be had for less than its $170 MSRP. In the last few months weâ\\x80\\x99ve seen pricing drop as low as $140. But before we get into that discussion, letâ\\x80\\x99s take a quick look at how these two GPUs compare overall, in all 36 titles we tested.On average the Radeon RX 570 was an astounding 43% faster than the GeForce GTX 1050 Ti at 1080p. Clearly where AMD has not been able to deliver on the high-end, they are making up on budget and mainstream GPU solutions priced below $300.The biggest AMD wins in this comparison were seen on DX12 titles supporting async compute, that said AMD did sponsor Strange Brigade and Sniper Elite 4. Meanwhile, the smallest wins were seen in Grand Theft Auto V and Assassinâ\\x80\\x99s Creed Odyssey. Neither title has proven to be AMD friendly in the past, but even so the RX 570 as able to get up over the GTX 1050 Ti here.Without a doubt, the RX 570 is a faster GPU and now that it's price matched to the 1050 Ti, or is generally a little cheaper, itâ\\x80\\x99s the obvious value choice for most gamers. We will note that the RX 570â\\x80\\x99s power consumption is significantly higher that the GeForce, but youâ\\x80\\x99re looking at well under 300 watts for total system consumption. So while it is extreme relative to the 1050 Ti, itâ\\x80\\x99s not outrageous overall.\"],\n",
       "  [\"Wrap Up: Price vs. PerformanceLetâ\\x80\\x99s take a quick look at a few application price vs. performance charts. For this Iâ\\x80\\x99m using the current market price with the exception of the Core i7-7820X, here Iâ\\x80\\x99m using the MSRP as the current market price is inflated by $300. Itâ\\x80\\x99s not worth buying at the MSRP as it is, so letâ\\x80\\x99s just go with that as a best case scenario.If you want to get some rendering work done on a serious budget thereâ\\x80\\x99s no beating the Ryzen 5 2600 right now, itâ\\x80\\x99s really not that much slower than the Core i7-8700X and itâ\\x80\\x99s a heck of a lot cheaper.But letâ\\x80\\x99s focus on the new CPUs, the 9900K and 9700K. The 9700K is better than the 8700K in terms of value, but still much worse than the cheaper Ryzen 7 2700X.Meanwhile the 9900K smokes the 2700X in terms of performance, but at almost twice the price itâ\\x80\\x99s poor value in comparison. If time really is money, then the much more expensive 2950X seems like the obvious choice. Also please note these price vs. performance charts donâ\\x80\\x99t factor in motherboard and cooling costs, Iâ\\x80\\x99ll discuss that shortly.The Intel CPUs stack up much better in Handbrake and do offer more bang for your buck when compared to the competing AMD CPUs. This is a worse case scenario for Ryzen, at least in our battery of application benchmarks.As we saw previously the Ryzen 7 2700X really is the ultimate value option for content creators and the new Intel CPUs canâ\\x80\\x99t hold a candle to it in terms of value. The 9900K doesnâ\\x80\\x99t offer any new here when compared to the 7820X, while the 9700K was no better than the 8700K, so a disappointing set of results for Intel in Adobe Premiere.Closing RemarksThese new 8-core Coffee Lake processors certainly are interesting animals, but Iâ\\x80\\x99m not entirely sure who theyâ\\x80\\x99re for. Before I try and work that out letâ\\x80\\x99s quickly touch on pricing. The Core i9-9900K comes in at an MSRP of $500, but currently costs more than that at $580, while the 9700K is meant to cost $374, but is currently at $420.The Core i7-8700K is also a little over the MSRP right now, priced at around $390 rather than $360 and this slight price increase is due to a supply issue. So weâ\\x80\\x99ve established that the new 8-core parts arenâ\\x80\\x99t cheap, but who are they designed for?A lot of you are gamers and Intel has been touting the 9900K as the world's best gaming CPU, which it technically is. But at 1080p with an RTX 2080 Ti it barely is any better than the 8700K, the previous gaming king, and arguably still the best in terms of value for high-end gaming. At $150 the Ryzen 5 2600 is without question the best value gaming CPU overall. Thatâ\\x80\\x99s a ridiculously low price for the 6-core/12-thread CPU.In my opinion the minor performance gains the 9700K and 9900K offer in some games using low-res settings doesnâ\\x80\\x99t make them better gaming CPUs than the 8700K, at least not right now. The added power consumption and heat make them less attractive options. For an almost 50% increase in price youâ\\x80\\x99re looking at maybe a 5% increase in performance, assuming you donâ\\x80\\x99t game at 4K.So while certainly very fast gaming CPUs, I feel like neither the 9700K or 9900K make that much sense for those looking to game exclusively. Then if we look at application performance itâ\\x80\\x99s still hard to justify buying either of these new 8-core processors. For the most part, the 9700K is slower than the 2700X, while the 9900K is up to 30% faster when overclocked, so thatâ\\x80\\x99s pretty impressive, less so for the price of admission.The additional cost of cooling is another topic for discussion. Thereâ\\x80\\x99s simply no way youâ\\x80\\x99re going to avoid thermal throttling without spending around $100 on the cooler, at least without your PC sounding like a jet about to take off. Throw in the Corsair H100i Pro and the 9900K now costs $700 and you still canâ\\x80\\x99t overclock, at least not without running at dangerously high temperatures.Later this month the Threadripper 2920X will be landing for $650 packing 12 cores and you can probably guess which one will be faster for productivity workloads. Granted X399 motherboards cost about $100 more, but you get twice as many DIMM slots, way more PCIe lanes and well, a serious workstation platform.The 9900K is every bit as good as the 8700K for gaming, but not nearly as good as the 2950X for most core-heavy productivity workloads. So you canâ\\x80\\x99t exactly say itâ\\x80\\x99s the best of both worlds but if you lean more heavily on the gaming side then I guess itâ\\x80\\x99s a better choice.Obviously no matter your preference, only those with money to burn will be considering the 9900K at its current market price, or even the $500 MSRP for that matter. For me itâ\\x80\\x99s just too expensive and too impractical, keeping it cool seems like a daily challenge and unless youâ\\x80\\x99re going all out on custom liquid cooling, itâ\\x80\\x99s a challenge youâ\\x80\\x99ll likely fail.Basically you can build a Ryzen 7 2700X gaming rig with a GTX 1080 Ti and still save over $100 compared to a i9-9900K build using a GTX 1070. Again, unless you have money to burn, the 9900K is not a great value at the current asking price.The Core i7-9700K is an even worse proposition in my opinion, although we havenâ\\x80\\x99t had time to test thermal performance properly, it does seem to run hotter than the 8700K, despite being soldered. It also consumes quite a bit more power and offers a minor performance bump. I would liken this comparison to the RTX 2070 and GTX 1080, in the sense that the newer product is faster by a small margin while costing a little more. Unfortunately whereas the RTX 2070 uses a little less power than the GTX 1080, the 9700K isnâ\\x80\\x99t more fuel efficient than the 8700K.\"],\n",
       "  ['In Sekiro: Shadows Die Twice, the thread between life and death is tenuous. As the One-Armed Wolf, a loyal shinobi seeking to save a young noble with a cursed bloodline, you traverse a feudal Japan so saturated with the remnants of war that the idea of mortality becomes fickle: dead bodies blending in with the local flora and fauna, so many wounded soldiers sharing their last words that you could create a compendium of the lost. This is all underscored by a cruel and ultimately grueling ironyâ\\x80\\x94you cannot die.Sekiro plays often with its sense of tone. The earnest solemnity of a samurai film is undercut by darkly absurd and comically bleak moments. Thereâ\\x80\\x99s a training section offered to you by a man who exists only to die for you over and over, and heâ\\x80\\x99s kind of chill about it. â\\x80\\x9cHey man,â\\x80\\x9d he might as well say. â\\x80\\x9cWanna mess around and kill me a little more for practice?â\\x80\\x9d Having an understandable existential crisis stemming from his inability to die, he asks you to murder him with the same casual listlessness of a lonely friend asking you over to watch the game for the third time this week. Death is kind of a joke, and itâ\\x80\\x99s that exact sentiment thatâ\\x80\\x99s plunged the gameâ\\x80\\x99s world into utter chaos.The player quickly becomes the butt of that joke, as Sekiro is punishingly difficult. (This should be shocking to roughly no one, given that itâ\\x80\\x99s made by From Software, developer of the infamously challenging Souls games and Bloodborne.) The combat requires real attention to detail and a willingness to drill down on a few sets of possible reactions. Boss and mid-boss battles are a furious interplay of choreographed patterns mixed with improvisation. First you learn an enemyâ\\x80\\x99s moves; then, maybe five or 10 deaths later, the real battle begins. Learning the early boss Lady Butterflyâ\\x80\\x99s attack patterns is that much more satisfying because the presentation is excellent. She moves like a dancer, and her attack animations tell a story.I found myself deeply immersed in the way these battles worked, obsessing over each animation, every cue, every possible breakaway combination that could happen as a result of my own reactions. Combat in Sekiro is like a dance, but itâ\\x80\\x99s also like a series of the fastest-ever choose-your-own-adventure branches: Parrying this leads to a thrust. Not blocking leads to a sweep. With the addition of shinobi prosthetics and skills, all of which can be upgraded via skill trees, the options open up immensely. As stubborn as Sekiro is in forcing players to learn how each enemy telegraphs its moves, there are still lots of ways to approach each encounter.For example, thereâ\\x80\\x99s this one tough boss fight in a poison pit. Enormous statues of Buddha protrude from the sickly seaweed-colored lake, their palms outstretched for you to land in as a Snake-Eyes gunnerâ\\x80\\x99s shots explode in a firecracker flare of twining lights. You can dodge and dash and soar through the air and ultimately clash with her, or you can bait her into the poison pool and sit atop a cliff face while her health slowly, slowly, painfully slowly drains. The game had just given me a tip about enemies in poisonous areas having a higher poison resistanceâ\\x80\\x94I couldnâ\\x80\\x99t tell whether it was warning me not to use the poison or coaxing me into it. I took my win and kept it moving all the same, quietly deciding that maybe that was the only cheese strategy I wanted to use during this playthrough. Generally, that worked out well. As I fought and fought and fought, I found often that playing and dying a lot, resting, and coming back actually made thingsâ\\x80\\x94this word comes up often in discussions about FromSoftware gamesâ\\x80\\x94click.Sekiroâ\\x80\\x99s combat relies on two stats called Vitality, which is health, and Posture, represented by a meter that builds as youâ\\x80\\x99re essentially knocked off balance. Your enemy has the same meters. The higher the Posture meter gets, the less poised you become. The lower your Vitality gets, the faster your Posture meter rises. If your Posture meter maxes out, youâ\\x80\\x99re susceptible to any attack from an opponent, which often results in a substantial punish. If you max out your opponentâ\\x80\\x99s Posture bar, youâ\\x80\\x99re able to perform a deathblow and either kill them or remove a full bar of their health. Generally, this system rewards aggressive gameplay and strategically applying pressure. Itâ\\x80\\x99s hectic and can be an absolute blast.What wasnâ\\x80\\x99t a blast was the feeling that I was repeating myself. Sekiroâ\\x80\\x99s winding world is full of near-duplicate mini-boss fights, and I often found myself asking why. Iâ\\x80\\x99m guessing the developers of the game were trying to coax the player into reconsidering their approaches to boss fights, but I found that I wasnâ\\x80\\x99t really forced to do that in several of these repeat encounters, nor did I even really feel the satisfaction of being able to curb stomp an enemy that had previously led me to struggle. Fortunately, most of these encounters are optional, and while they are necessary for a completionist run (and to be fair, not that much of a time sink), I found myself thankful that I could just pass on them.You do a lot of passing in Sekiro, which is as much a game about stealth as it is about swordplay and shinobi arts. The gameâ\\x80\\x99s stealth started off exciting. It was thrilling and fun to discover new enemy patterns and layouts, dig into how my tools and items worked, and quickly face-plant into the consequences of failure. But as the game progressed, I found myself tired of dodging around random mooks who I could easily kill one-on-one or even one-on-two, even when they were eventually flanked by stronger ninja and new, more disciplined samurai types. That was compounded by the fact that enemiesâ\\x80\\x99 intelligence didnâ\\x80\\x99t appear to grow any more complex and seemed to differ mostly in range of vision, hearing, and how long enemies would stare in your general direction after spotting you. Most of the stealth sections felt interchangeable.But then there are stealth moments that Sekiro gets really, really right. In addition to a couple unique chase sequences that I wonâ\\x80\\x99t spoil, thereâ\\x80\\x99s a particularly striking boss encounter in the late mid-game that has stuck with me. Itâ\\x80\\x99s more of a hunt than a fight, meaning stealth is key, and thereâ\\x80\\x99s a very light puzzle element that makes the change of pace from normal combat deeply refreshing. Mounting pressure from a growing wave of surrounding enemies escalates the difficulty, creating a totally different challenge from what Iâ\\x80\\x99d already seen. Itâ\\x80\\x99s really, really good. Thereâ\\x80\\x99s also a stealth portion in the endgame thatâ\\x80\\x99s so punishing it skirts the line of being interesting, but I still found myself largely over that aspect by the time the game was done and feeling like it could have gone so much farther.The music, mostly sparse, works well when it does show up. Atmospheric touches go a long way to give the string-laden tracks depth and dimension. The encounter music in a monastery area, for example, is underscored by the deep rumble of throat chanting, highlighting the atmospheric differences from other areas youâ\\x80\\x99ve been in and underscoring the underlying theme: These monks have strayed from enlightenment. A normally meditative and harmonious sound is used to creepy, otherworldly effect, highlighting something the mid- and late-game locales completely nail: The line between life and death is blurred, and with it, death creeps into the mortal world in horrifying ways.That theme is also served in interesting ways by the non-linearity of Sekiroâ\\x80\\x99s world. I explored areas I didnâ\\x80\\x99t yet need to visit before progressing the story, and by the time it pivoted even further into the themes of decay and immortality, I had seen for myself parts of the world warped into grotesque and eldritch forms by the quest for eternal life, which made my foray into the next parts of the game that much more poignant. I also felt a sense of accomplishment and was thrilled to realize that I had already nearly completed certain quest objectives, adding to the sense I already had that the world folded in on itself in interesting and rewarding ways. I finally came to understand the way that immortality really figured into the story.The Wolfâ\\x80\\x99s own immortality comes with a price: When you die, the non-player characters youâ\\x80\\x99ve met and befriended will grow infected with a plague called Dragonrot. The illness is apparently deadly, but it wonâ\\x80\\x99t kill anyone, and it can be cured in-game via items. You canâ\\x80\\x99t access sidequests and certain lines of dialogue from ill NPCs, but the story otherwise goes unaffected. I initially found this a little anticlimactic, but now that I consider it more of a background story beat than a truly important mechanic, I feel it has a place in the game. I do wish there were more real implications of inflicting it on those around you, though. What would it look like for Dragonrot to have permanent effects on the game? Probably a lot of rage quits.The gameâ\\x80\\x99s death mechanics do more for the themes than the actual gameplay, and thatâ\\x80\\x99s fine. You lose money and experience toward skill points when you die, which was deeply distressing in my early gameplay but ultimately became trivial as I learned to use cash-storing coin purses and keep an eye out for how much experience I might stand to lose at any given moment. I got more strategic about who to engage and when, and how to prepare for those engagements. Then thereâ\\x80\\x99s Unseen Aid, a blessing from on high that sometimes triggers when you die without an available resurrection, preventing you from losing your resources upon death. It starts with a 30 percent chance of activation and reduces even further when the characters are inflicted by Dragonrot. As a result, I never even thought to rely on it. But again, like the Dragonrot itself, it meshes with the gameâ\\x80\\x99s religious overtones, so I was able to appreciate it on some level.Sekiro gets a whole lot right. Its themes permeate its feudal Japan in a compelling way, and for the most part, the gameplay is deeply satisfying. There are things it could do better, particularly avoiding repetition, but the notes Sekiro does hit are memorable enough that the slog doesnâ\\x80\\x99t totally ruin the flow of gameplay, and the inertia into the end of the game carries strong. The challenge Sekiro presents is daunting and time-consuming. Ultimately, the question I had coming in was, â\\x80\\x9cWill this be worth it?â\\x80\\x9d After moving through countless cycles of life and death, tensing, raging, and finally, conquering my challenges and letting go of my anger like Buddha, I decided that it was.'],\n",
       "  [\"TechSpot's Best Of features are designed to simplify your shopping process by condensing all the information we gather from reviews and tests into digestable buying guides broken down by price bracket or intended use. But even if we've aided in choosing which is the best GPU for you, you will still have to choose a specific graphics card brand and model. And at times you can find dozens of different models based on the same GPU.From the last batch of releases comprised of Nvidia's RTX mid-range and high-end GPUs as well as AMD's Radeon VII, the GeForce RTX 2060 was the best scoring of them all. It is also the most affordable, and that's no coincidence.The RTX 2060 offers the best value within that group, and is poised to replace the GTX 1070 as the middle ground for great performance at a price that remains within reach of most enthusiasts.In the past few weeks we've seen requests on whatâ\\x80\\x99s the best GTX 1070 to buy replaced by what is the best RTX 2060 board. While some readers want to know which model is outright the best, others are more interested in best value or what options are suitable for Mini-ITX builds. Without further ado, here are the top four RTX 2060 cards in the market right now...Best Priced RTX 2060As of writing, there arenâ\\x80\\x99t many models selling at the $350 MSRP, in fact as I put this video together thereâ\\x80\\x99s just two Gigabyte models, the RTX 2060 OC and RTX 2060 Mini-ITX OC. A few days prior there was also the Zotac Gaming RTX 2060 Twin Fan, which we'd been recommending, but itâ\\x80\\x99s now jumped a bit in price.The Gigabyte RTX 2060 Gaming OC features a triple fan cooler and it delivers where it counts. Weâ\\x80\\x99ve been watching this model for sometime now and pricing has been consistent, it came in as one of the cheapest models at $370 and for a month now itâ\\x80\\x99s been available at all major online retailers for $350. If you can get the Zotac Twin Fan model for $350, itâ\\x80\\x99s a worthwhile option, too.The Gigabyte is a basic model, packs some decent features such as alternate spinnings fans, direct touch copper heatpipes and a full length backplate. Itâ\\x80\\x99s by no means the coolest or quietest RTX 2060 out there and if youâ\\x80\\x99re willing to spend $20-$30 more you will get a better product in that respect.Best Premium RTX 2060If youâ\\x80\\x99re not stretching the budget to get an RTX 2060 and have a little extra spending, we highly recommend the Asus ROG Strix RTX 2060 OC Gaming. At $400 itâ\\x80\\x99s $50 more expensive than the few base models you can find at the MSRP, but you get a lot more graphics card for the money.Granted itâ\\x80\\x99s not much faster, but it is significantly cooler and quieter. Out of the box it runs at just 62 degrees and that's an 11 degree improvement over the similarly priced MSI RTX 2060 Gaming Z. VRM and GDDR6 memory temperatures were also very low relative to other high-end 2060 models and our card overclocked like a champ, holding an operating frequency of 2055 MHz with ease.Just be aware that this thing is the size of a high-end graphics card, it measures 300mm long, stands 132mm tall and is a whopping 50mm wide. Thereâ\\x80\\x99s three 85mm fans, a full-size backplate and enough customizable RGB lighting to put your christmas tree to shame, though for those after a stealth look, you can kill the lighting with the press of a button. Overall the Asus ROG Strix RTX 2060 OC Gaming wonâ\\x80\\x99t disappoint those after a premium RTX 2060 experience.Best Mini-ITX RTX 2060There are two main choices here: the MSI RTX 2060 Aero ITX OC and Gigabyte RTX 2060 Mini ITX OC and we're going with the Gigabyte model for a few reasons. MSI has announced the RTX 2060 Aero ITX OC, but good luck finding one, it doesnâ\\x80\\x99t appear to be on sale yet.We'd probably recommend the Gigabyte anyway as itâ\\x80\\x99s slightly shorter. The 126mm height of the MSI model will make compatibility with some Mini-ITX cases such as the Geeek A30 very difficult. The 121mm tall design of the Gigabyte version is more suitable for these smaller cases.The Gigabyte model also has a zero RPM fan mode and when under load is relatively quiet for a single-fan 2060. Perhaps best of all, itâ\\x80\\x99s readily available at the MSRP, meaning you can buy one today for just $350.Best Blower Style RTX 2060We are not too fond of blower-style graphics cards, but in few scenarios they can make sense. Your options here are extremely limited though, and the only widely available version comes from Asus with their Turbo RTX 2060.For $390 itâ\\x80\\x99s up there with the more expensive models and while weâ\\x80\\x99d typically recommend avoiding it, if you must get a blower-style card this would be your best choice.The Turbo RTX 2060 features a 80mm IP5X rated dual-ball bearing fan, some subtle lighting, measures 268mm long and is, of course, a dual-slot card.Closing ThoughtsPicking the right RTX 2060 is a little tricky and for the most part itâ\\x80\\x99s all about the price. There are very few $350 options to choose from, and once you get up over the MSRP they all seem to jump up pretty close to $400.If youâ\\x80\\x99re going to spend ~$380, for example, you might as well just go the full hog at that point and get the beastly Asus ROG Strix RTX 2060 OC Gaming for $400. Itâ\\x80\\x99s still at least $100 cheaper than a base model RTX 2070 and for the most part offers similar performance.The cheapest Vega graphics cards still cost a little over $400 and youâ\\x80\\x99ll be hard pressed to get one that matches the quality of the ROG Strix RTX 2060 OC Gaming.\"],\n",
       "  [\"Like many other refreshed RTX laptops, there hasnâ\\x80\\x99t been a significant change to the design on the Asus ROG Strix or most of the internal hardware. Previously I reviewed the 15-inch Scar II which came with either a GTX 1060 or GTX 1070, while now you can get an RTX 2060 or RTX 2070, albeit at different price points. Our review unit is the GL504GV that packs the RTX 2060 priced at $1,700.Other hardware is largely unchanged from the previous generation. The same Intel hexa-core Core i7-8750H is used, 16GB of DDR4 memory, and a 15.6-inch 1080p 144Hz IPS display. The primary storage options have been bumped up a tier across the board, so where the base model got a 128GB SSD, thatâ\\x80\\x99s now up to 256GB, and so on. Our review unit, and the main option available on Newegg and Amazon, comes with a 512GB PCIe SSD.In terms of design, aside from the brushed aluminium lid, most of the laptop uses plastic, with a soft touch coating applied around the keyboard. This soft touch area has a carbon fiber design with half of it getting a camo print. Weâ\\x80\\x99d probably prefer if the camo wasnâ\\x80\\x99t there but Asus loves to do this sort of thing.Yes, thereâ\\x80\\x99s RGB, too. You get a strip along the front edge of the laptop, plus the off-center Asus ROG logo on the lid, both controllable through Asusâ\\x80\\x99 Aura software. The front RGB strip is directly linked to the four zone RGB keyboard backlighting, you donâ\\x80\\x99t get per-key RGB here but the basic four zone effect is nice enough. The keyboard has a bit of a spongy tactile response, it's not as clicky as I'd prefer, but travel distance is decent, plus you get a numpad and none of the keys are truncated to fit that in. The trackpad is great and we love the inclusion of two separate click buttons.Like a lot of gaming laptops these days, a slim bezel around the 15-inch display keeps the laptopâ\\x80\\x99s footprint quite compact, although thereâ\\x80\\x99s still a sizable bezel below the screen. Unfortunately Asus has gone with probably the worst webcam placement possible, offset and low below the display, if the webcam is at all a concern for you, of course.The chassis is your typical mid-tier option: itâ\\x80\\x99s not a slim and light system like the Asus Zephyrus, but itâ\\x80\\x99s not a chunky beast like the big boy Asus models. It clocks in at 26mm thick and 5.3 lbs, so itâ\\x80\\x99s portable without getting into the thin category that comes at a higher price. The main limitation for portability will be the smaller 62 Wh battery, versus near 100 Wh you get with systems like the Gigabyte Aero 15.I/O is very solid. Three USB 3.1 type-A ports, two gen 1 and one gen 2, plus a USB 3.2 gen 2 type-C port (thereâ\\x80\\x99s no Thunderbolt). You also get HDMI 2.0b, miniDisplayPort 1.2, Ethernet, an SD card slot and a 3.5mm audio jack. Internal connectivity comes in the form of an Intel 802.11ac 2x2 Wi-Fi plus Bluetooth 5.0 combo solution.Before talking about performance, there are a few notes to be made. The first is relating to Asusâ\\x80\\x99 software suite, the Armoury Crate. Thereâ\\x80\\x99s a lot of standard stuff in there, but the important thing are the fan profiles: Windows, Silent, Balanced and Turbo, plus a manual mode. For game testing we used the balanced mode, because we couldnâ\\x80\\x99t spot any difference in performance between that and Turbo. For productivity apps we used the Turbo mode, itâ\\x80\\x99s louder but it does increase the power limit on the CPU which for long workloads like video encoding led to a small performance increase.And while the Turbo mode is nice, whatâ\\x80\\x99s not nice is getting a single 16GB DIMM of DDR4-2666 memory. This means the laptop runs a standard 16GB configuration in single-channel only, which leads to lower memory bandwidth than dual-channel laptops, and therefore lower performance. It does leave a DIMM slot free if you want to upgrade to 32GB down the line, but thatâ\\x80\\x99s come at the cost of out of the box performance.We won't go into excessive detail about the Core i7-8750H because weâ\\x80\\x99ve covered it plenty of times in other reviews. Itâ\\x80\\x99s a popular six-core CPU used in most gaming laptops, and itâ\\x80\\x99s the option weâ\\x80\\x99d choose for productivity and gaming workloads without getting into crazy form factors.PerformanceThe Asus ROG GL504GV performs roughly on par with a typical Core i7-8750H laptop, but thatâ\\x80\\x99s down to a few notable differences that both favor and disadvantage the system. Due to the Turbo fan mode, longer workloads tend to throttle less than standard 8750H performance, so weâ\\x80\\x99re seeing up to a 10 percent advantage for longer multi-threaded workloads like Handbrake. This laptop can sustain higher clocks for longer. On the flip side, due to having single-channel memory, this laptop gets lower marks in our memory heavy benchmarks. That includes 7-Zip compression, MATLAB and Adobe Photoshop Iris Blur.As usual hereâ\\x80\\x99s our comparison between the Core i7-8750H and the last-gen quad-core Core i7-7700HQ. Due to higher clock speeds and more cores, weâ\\x80\\x99re looking at over a 50% advantage in some multi-threaded workloads, as well as higher single-thread performance. You wonâ\\x80\\x99t see this sort of improvement in every workload, but if youâ\\x80\\x99re coming from an older quad-core system, the 8750H will be a notable upgrade.This is the chart for those wondering whether they should buy a larger 15-inch gaming system, or a portable 13-inch ultrabook for their productivity tasks. Generally, the 8750H absolutely smokes a 15W CPU like the Core i7-8565U, delivering more than double the performance in some cases. Throw in proper GPU acceleration and for something like Premiere encodes, a gaming laptop is an order of magnitude faster.When it comes to gaming, we have tested over a dozen games breaking down how the RTX 2060 performs in this exact system. Check that out in our RTX 2060 (laptop) GPU review. Do note that for that review we loaded up the GL504GV with dual-channel memory, so it will be a tad faster because of that.On average, the single-channel configuration of this GPU is 13 percent slower than the dual-channel config. Thatâ\\x80\\x99s a significant difference simply from not having that second stick of memory in there. Some games are barely impacted, such as Dirt 4, Watch Dogs 2 and Wolfenstein II. Others like Prey, Assassinâ\\x80\\x99s Creed Odyssey and Resident Evil 2 see a 25% performance cut or more as the limited memory bandwidth chokes these games. There is no doubt that dual-channel memory is significantly better for gaming at 1080p.In terms of an actual FPS impact, in a game like Hitman 2 weâ\\x80\\x99re talking about going from a 70 FPS average, down to 50 FPS with single-channel memory (!!!). In Assassinâ\\x80\\x99s Creed Odyssey, a 60 FPS experience is cut down to just 45 FPS. Itâ\\x80\\x99s not a good performance loss.This also impacts the margins between this RTX 2060 laptop, and other GPUs. Comparing dual-channel to dual-channel, the RTX 2060 is about 28% faster on average than a GTX 1060 6GB. But this single-channel RTX 2060 laptop, compared to a dual-channel GTX 1060 6GB system is only 12% faster, and is actually slower in some games. Then compared to something with a GTX 1070 inside with dual-channel RAM, it loses by more than 20%.Leaving that empty DIMM slot is a costly trade-off to make it easy for owners to upgrade their RAM in the future. We have to wonder how many people actually open up their system and chuck in that extra stick. Surely the number isnâ\\x80\\x99t high enough to justify cutting performance for most others.Ideally this system should come equipped with two 8GB sticks by default, leading to much better performance from the RTX 2060.If you are thinking of buying the GL504GV and want to solve this problem, youâ\\x80\\x99re looking at spending $100 for a second 16GB DDR4-2666 SO-DIMM stick. Not huge, but something you might want to factor in.Looking at the cooling solution used in the ROG Strix Scar II, using the balanced fan mode for gaming which delivered the performance youâ\\x80\\x99ve seen so far, the cooler is pretty loud under load, pushing up near 48 dBA in our testing. It also tends to ramp up and down a fair bit while youâ\\x80\\x99re gaming, so sometimes the cooler is quieter than this figure, but to be honest the frequent changes are just as annoying as running at 48 dBA all the time.However, temperatures are very good, which indicates the balanced mode is perhaps ramping the fan up too high. We recorded a mere 73 degrees on the GPU and 83 degrees on the CPU in an extended Watch Dogs 2 session, which is well below other laptops with similar noise outputs. We reckon some manual fan tuning would go a long way to balancing out temperatures and noise.At the same time, even when using the Turbo fan profile, the system was much quieter under pure CPU workloads like Handbrake. Clocking in below 40 dBA is a good result. You should still use the Turbo mode though to increase the power limit, even though it doesnâ\\x80\\x99t ramp up fan speeds to ridiculous levels in CPU tasks. While gaming the fans really crank up, so I donâ\\x80\\x99t recommend it.For storage, we received a 512GB Kingston M.2 PCIe NVMe SSD. This is a mid-tier performer, not as fast as some of the Samsung or Intel SSDs weâ\\x80\\x99ve seen in other gaming laptops, but still fast enough to deliver a performance advantage over basic SATA SSDs. Internally thereâ\\x80\\x99s also a free 2.5-inch drive bay for more storage.The laptop's display is good. A decent 15.6-inch 1080p IPS with a 144 Hz refresh rate, peak brightness is up around 300 nits, acceptable for a gaming laptop, and it has a contrast ratio of around 1000:1. In terms of color performance, this is an sRGB display, so no fancy wide gamut, but not that you need that for gaming.Our main concern was with the incorrect white point. Asus is using approximately 7500K rather than the correct 6500K, which gives the display a colder, bluer tone. This leads to average deltaEs between 3.0 and 4.0 which isnâ\\x80\\x99t accurate but isnâ\\x80\\x99t hugely inaccurate either.Putting It All TogetherThe Asus ROG Strix Scar II GL504GV is a fine laptop with a good design, a good trackpad and keyboard, and generous I/O. Itâ\\x80\\x99s not overly portable, itâ\\x80\\x99s not overly chunky. Itâ\\x80\\x99s got a pretty good display, upgradeability options, and the internal hardware and performance is good, too.Our only major concern has to do with the single-channel memory out of the box. It doesnâ\\x80\\x99t impact productivity workloads heavily, but it does shave ~10% off the gaming performance. It is fixable â\\x80\\x93 a $100 stick of memory added into the second slot immediately gives you that performance back â\\x80\\x93 but weâ\\x80\\x99d rather get that included by default.In terms of value, the Asus ROG Strix Scar II is somewhat hard to recommend to buy right now when you look at the gaming performance you are getting. But this is not specific to this laptop, but all RTX laptops currently in the market. At $1,700, the Scar II is considerably more expensive than GTX 1060 laptops with a similar CPU, display, storage and other components, which retail for around $1,100 these days. In plain figures, for 63% more cash, youâ\\x80\\x99re getting a system thatâ\\x80\\x99s only 12% faster... or 28% faster if you add in a second stick of RAM.Itâ\\x80\\x99s also not competitive with outgoing GTX 1070 laptops. The predecessor to this system, the GL504GS, is only $1,500 in certain stores, including Newegg. That gets you a very similar laptop that is faster for less money. Even if for a limited time while supplies last, RTX laptops at launch do not offer great value.Laptop prices will inevitably change over the coming months, so those watching this review in the future may be faced with a completely different situation. For now, we simply dislike the idea of a new product not beating last generation's on raw performance nor value. We're honestly not sure who is responsible for setting these prices. Perhaps it's simply Nvidia charging more for the RTX GPU since every other RTX 2060 system is around the same price as the Scar II.Value discussion aside, Asus has designed the Strix Scar II to be accessible and deliver a decent 1080p gaming experience. You won't find much to set it apart from other generally good gaming laptops at this price point, but it's a good all-round package nonetheless.\"],\n",
       "  [\"Metro Exodus is about to receive our usual GPU benchmark treatment with 36 graphics cards on the bench getting a full performance comparison. The game has generally been well received although it's been out for just a few days. We got access to Metro Exodus ahead of release, which we used to not only build our big GPU benchmark, but also check out ray tracing and DLSS performance. Along with the game's release AMD and Nvidia pushed out updated drivers which have been used for this test.We briefly tested the previous display drivers from both camps though as far as we could tell, neither brought real performance improvements. What did improve performance was Metro's day 1 patch, so our results might differ a bit from those who published early.Generally performance fluctuates quite a bit in most games depending on the levels and areas and thus whatâ\\x80\\x99s being rendered. Though developers try to optimize levels for more consistent performance, we wouldnâ\\x80\\x99t say theyâ\\x80\\x99ve done a poor job of this with Metro Exodus, but you will see frame rates with a GTX 1060 at 1080p as high as 70-80 fps in some areas and then just 30-40 fps in others.Weâ\\x80\\x99ve canned the canned benchmark in favor of an in-game 60 second pass on a demanding section thatâ\\x80\\x99s early on in the game. The benchmark takes place in Moscow, very early on in the campaign and starts at the first checkpoint after the pack of mutated wolves passes by. For testing we're using the â\\x80\\x98Ultraâ\\x80\\x99 preset which sees Nvidia HairWorks and Advanced PhysX disabled by default, Tessellation is enabled though.The game also supports DX11 and DX12, but performance using a Core i9-9900K was identical using either API, so that being the case we decided to go with DX12 for the testing.Our standard GPU test rig packs a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3400 memory. The latest display drivers were installed, for AMD that was the Adrenaline 2019 Edition 19.2.2 and for Nvidia the GeForce 418.91 drivers.BenchmarksStarting at the top and working our way down. As you can see the RTX 2080 Ti had no issue keeping things above 100 fps and while 131 fps at 1080p isnâ\\x80\\x99t amazing for this extreme high-end GPU, itâ\\x80\\x99s better than what we see in titles such as Assassinâ\\x80\\x99s Creed Odyssey, Just Cause 4, Hitman 2 and Kingdom Come Deliverance for example.Moving down the list we see the RTX 2080 delivering very solid performance, here it was 13% faster than the GTX 1080 Ti, so this title appears well optimized for Nvidiaâ\\x80\\x99s new Turing architecture. The 1080 Ti still does well and it is seen overcoming the much newer Radeon VII. Meanwhile the RTX 2070 and 2060 offer very similar performance and both were faster than not just Vega 64 Liquid but also the GTX 1080.For those hoping to around 60 fps at 1080p, you will require some pretty heavy GPU firepower. At the very least a GeForce GTX 1070 or Vega 56 will be required, Vega 56 was the faster of the two as it roughly matched the GTX 1070 Ti.Then for around 50 fps on average the Radeon RX 590 or GeForce GTX 1060 will work and here we see the 1060 6GB is 9% faster than the RX 580 and 14% faster than the 3GB 1060. Then anything below the RX 570 or R9 390, Iâ\\x80\\x99d deem to slow for an enjoyable gaming experience. This meant the weakest Nvidia GPU youâ\\x80\\x99d want to play with is the GeForce GTX 970. So like we said the game demands some serious hardware at 1080p when using the ultra quality preset, which isnâ\\x80\\x99t the highest quality preset available, that would be extreme.Starting with the RTX 2080 Ti, again we see that the average frame rate remains over 100 fps, while the 1% low dropped down towards 90 fps. Still thatâ\\x80\\x99s pretty impressive given the RTX 2080 couldnâ\\x80\\x99t even average 90 fps, instead it was good for 83 fps. The Radeon VII was 14% slower with 71 fps and that meant it was 8% faster than the RTX 2070 and just 16% faster than the RTX 2060, not great given it costs twice as much.The RTX 2060 is actually really good value here. It matched the GTX 1080 and wasnâ\\x80\\x99t noticeably slower than the 2070. Vega 64 Liquid does okay, but Vega 56 is the stand out AMD option here despite getting wasted by the 2060, but this is of course an Nvidia sponsored title.Nvidiaâ\\x80\\x99s older Maxwell GPUs are hanging in there rather well, the 980 Ti matched the GTX 1070 while the 980 matched the 6GB 1060. The 6GB 1060 was also 12% faster than the RX 580, though neither were great at 1440p, averaging less than 40 fps. Basically anything below the RX 580 or GTX 970 were unplayable at this resolution.Then at 4K, well good luck, you need an RTX 2080 Ti or you need to settle for inferior quality settings, at which point you might as well bite the bullet and just play at 1440p.The game was playable using a GTX 1080, RTX 2060, Radeon VII or anything better, but once you start seeing the 1% low performance dipping below 30 fps, the stuttery frame rates harm the experience.Now before wrapping things up we re-tested 31 GPUs, many of them not tested previously, to see if you can get away with older hardware at 1080p using the medium quality preset with Tessellation disabled, and of course HairWorks and Advanced PhysX were still turned off.The medium quality setting allowed the GTX 970 to average 92 fps, this is a massive 119% performance boost over what we saw with the ultra quality preset. Similar gains were also seen for the RX 570 and R9 390. The GTX 1050 Ti saw a doubling of frame rate and this meant it was now able to deliver highly playable performance, which was nice to see.We also see some old timers such as the HD 7970 and GTX 780 Ti offering playable performance and boy, oh boy, has the 7970 aged well. The R9 280X is a rebadged 7970 with a factory overclock and we also see that topping the 780 Ti. Meanwhile the slowest GPUs youâ\\x80\\x99ll get away with here include the R9 380, GTX 780, 950 and 1050.Finally, capping off the testing we have some quality preset scaling results. Weâ\\x80\\x99ve just seen how the medium quality preset offered a massive performance uplift for the older GPUs, especially those with 4GBâ\\x80\\x99s of less VRAM. The gains for the 8GB RX 580 and 6GB 1060 arenâ\\x80\\x99t quite as extreme, but even so they saw around a 50-60% performance boost when dropping down to the medium quality preset from ultra.Whatâ\\x80\\x99s interesting here is that the RX 580 and GTX 1060 deliver basically identical performance with the â\\x80\\x98lowâ\\x80\\x99 and â\\x80\\x98mediumâ\\x80\\x99 quality settings. However, with â\\x80\\x98highâ\\x80\\x99 the GTX 1060 pulls a few percent ahead and then extends the margin further with the ultra preset. It then maintains its 9% lead with the extreme quality settings. Tessellation was enabled for all these tests while HairWorks and Advanced PhysX were disabled. We suspect the level of Tessellation is increased with the ultra and extreme settings and this hands Nvidia a slight edge.Wrap UpThose of you hoping to enjoy Metro Exodus in all of its visual glory, youâ\\x80\\x99ll need one hell of a gaming rig. To be fair though, the game does scale down to support older or lower end hardware well by scaling down the quality setting.If you have less than 4GB of VRAM, then we strongly suggest using the medium quality setting. At 1080p using the ultra quality setting we often saw VRAM allocation reach around 3.8GB on cards with 6-8GB of memory, just over 4GB at 1440p and almost 5GB at 4K.Of course, we skipped over any ray tracing or DLSS testing as we covered that already. We were reasonably impressed with global ilumination ray tracing in Metro, far more so than Battlefield V. The performance hit where we tested for this article (39% hit) was more extreme than what we saw before (29% hit). A ~40% performance hit sees the RTX 2080 Ti at 1440p drop from an average of 105 fps to 65 fps, which is not good. Although itâ\\x80\\x99s a better implementation that what we saw in Battlefield V, that's not saying much.Overall the game looks great, itâ\\x80\\x99s highly detailed and this is largely why itâ\\x80\\x99s so demanding. Both AMD and Nvidia GPUs perform well and we were most impressed with how old products like the Radeon HD 7970 performed using the medium quality setting.\"],\n",
       "  [\"When we recently updated our Best CPUs feature, we noticed that access to affordable first-gen Ryzen processors remains an attractive option for many. The Ryzen 7 1700 is a standout option in particular as this 8-core/16-thread part is selling for $160, meaning you can either buy the R7 1700 or the R5 2600. So today weâ\\x80\\x99ve got a classic head to head CPU comparison for you.For those buying new our best choice in this price range is the Ryzen 5 2600X. It's currently a mere $20 more than the non-X model and comes with a better cooler and more aggressive clock speeds out of the box. In this scenario we feel the small price premium is worth it. However, quite a few of you were wondering if the Ryzen 7 1700 was worth buying over the 2600X. Naturally you get two extra cores, but the downside is that you miss out on those Zen+ optimizations and they do make the 2nd-gen parts a little more responsive, a bit snappier if you will.For productivity workloads that require many cores the R7 1700 is a more obvious choice. The clock speed disadvantage and higher memory latency are generally overcome by the 33% increase in cores. That said, if your workload doesnâ\\x80\\x99t require eight cores then the 2600X will be faster.Then for those of you who prioritize gaming, which one is better? Based on our day-one coverage, youâ\\x80\\x99d have to go with the 2600X, but a year later has anything changed? Are todayâ\\x80\\x99s games more demanding?For all testing we used the GeForce RTX 2080 Ti to help minimize GPU bottlenecks, but before some of you frown about us using such an extreme GPU, please note all testing takes place at 1080p, 1440p and 4K. So the 4K ultra results will be comparable to 1080p with a mid-range graphics card for example. Moreover, those using lower quality settings will see higher frame rates with a lesser graphics card.Since the integrated memory controller of the 2nd-gen Ryzen processors is much improved, we didnâ\\x80\\x99t hamper the 2600X when it came to memory. Instead we paired it with a 16GB DDR4-3400 CL16 kit. The R7 1700 was limited to 16GB DDR4-2933 memory with CL15 timings.Both CPUs were tested on the Gigabyte X470 Aorus Gaming 7 WiFi with the standard box coolers. We're not looking into overclocking (we covered that at release) and the focus is on gaming performance though we ended up testing some application workloads to paint a fuller picture at the end.BenchmarksFirst up we have Vermintide 2 and this is a good example of a title that isnâ\\x80\\x99t particularly CPU intensive, at least not when comparing 6 and 8-core processors. Weâ\\x80\\x99re seeing identical performance out of the 1700 and 2600X despite being what looks to be CPU bound at 1080p with the RTX 2080 Ti. Not much to report here, so letâ\\x80\\x99s move on to Assassin's Creed: Odyssey.The results here are a little more interesting. The Ryzen 7 1700 is seen limiting performance at 1080p, quite heavily in fact when looking at the 1% low result. The 2600X offered a 26% performance boost, keeping frame rates above 60 fps at all times.Moving to 1440p and now weâ\\x80\\x99re becoming GPU bound but even so the average frame rate was still 10% greater when using the 2600X. Once we hit 4K weâ\\x80\\x99re mostly GPU limited but even here the 2600Xâ\\x80\\x99s improved latency and support for faster memory accounted for a small difference.Although Fortnite isnâ\\x80\\x99t particularly CPU demanding when discussing 12 and 16-thread processors, we see quite a significant performance uplift with the 2600X. The clock speed advantage and improved memory performance is playing a key role here.The 2600X was up to 20% faster at 1080p and provided up to 17% more performance at 1440p. By the time we hit 4K the margin is reduced to nothing and by this point we are seeing the same performance regardless of which CPU is used.Apex Legends sees up to a 10% performance advantage going the way of the 2600X at 1080p. That margin is reduced to 7% at 1440p and then completely eliminated at 4K. Given the non-GPU bound results are all over 140 fps, the difference doesnâ\\x80\\x99t matter much here.In Resident Evil 2 the 2600X eked out a few extra frames at 1080p, offering around 9% more performance. That margin was halved at 1440p and then completely eliminated at 4K. So depending on the quality setting and resolution you might see up to a 10% difference, but most likely you'll see little to no difference in this title.Next up we have Just Cause 4 and this time we see up to a 15% performance advantage with the 2600X. Even at 1440p the 2600X was 8% faster. Not a massive margin by any stretch of the imagination but still a decent performance boost at this GPU-demanding resolution.Hitman 2 is always a bit of an odd title, here we see the R7 1700 creating a bottleneck at the three resolutions as it limited the RTX 2080 Ti to 72 fps. However we saw a consistent drop in 1% low performance as the resolution was increased and the 4K result is quite unusual. The 2600X allowed for up to a 10% performance boost and offered more consistent 1% low performance.Project Cars 2 sees the 2600X delivering up to 10% more performance at 1080p and 12% more at 1440p. The 2600X appears to be GPU limited at 1080p and 1440p, while this is only true for the R7 1700 at 1080p. By the time we hit 4K both CPUs are heavily GPU limited, so performance is much the same.The 2600X was up to 15% faster in Rainbow Six Siege and even at 1440p offered slightly more performance, though with both CPUs capable of over 120 fps at all times you have to wonder how much that margin matters.We have to say we were surprised to see the 2600X delivering up to 20% more frames in Battlefield V, even at 1080p. Basically the Ryzen 7 1700 was limiting the RTX 2080 Ti to around 100 fps in our test, creating a bottleneck at 1080p that resulted in similar performance seen at 1440p. At 1440p we are still primarily GPU limited and this is of course becomes even more true at 4K.World of Tanks might better utilize Ryzen CPUs today, but itâ\\x80\\x99s still not exactly CPU intensive. The 2600X edged slightly ahead at 1080p and 1440p by an insignificant margin and then as usual we were GPU limited at 4K resolution.Metro Exodus is another title thatâ\\x80\\x99s not particularly CPU demanding, at least for a modern processor and again while the 2600X was faster at 1080p and 1440p, the R7 1700 was still providing strong frame rates.Ryzen CPUs have always been a little strange in the Far Cry series, the Primal results were always very odd. The game seems quite sensitive to memory latency and despite being what looks CPU limited at 1080p, even with the 2600X, the Ryzen 5 processor was 12% faster at that resolutio and at 1440p.The faster memory and lower latency of the 2600X has to be accountable for this difference. Whatâ\\x80\\x99s really strange here is that we continued to see the R7 1700 fall away at the 4K resolution, we should be entirely GPU bound at that point but that was not the case with the Ryzen 7 processor.Moving on we see the exact same issue when testing with Shadow of the Tomb Raider. Here the Ryzen 7 1700 is seen limiting performance to 83 fps at 1080p and 1440p. Meanwhile, the 2600X limited performance to around 87 fps so it was roughly 5% faster.Frame rates when testing with Monster Hunter: World were much the same. The R5 2600X offered a small boost but overall the experience was not different using either CPU.Strange Brigade is not CPU demanding at all and itâ\\x80\\x99s a good example of how things will look in a typical gaming title, or when GPU bound.Star Wars Battlefront II is both demanding on the CPU and GPU, but when youâ\\x80\\x99ve got 12 or more threads to play with the CPU side of things, it's less of an issue. Both CPUs performed similarly in this game.Finally we have The Division 2 and we see similar performance using either CPU...Wrap Up: Evenly MatchedItâ\\x80\\x99s fair to say that overall the Ryzen 5 2600X and Ryzen 7 1700 are very similar in terms of gaming performance. When we saw a difference, the 2600X was leading the way in every single instance, though usually the delta was limited to a 5 to 10% margin. We feel for most gamers that difference won't be realized.Still if you only intend to play games then the 2600X is the better CPU. Youâ\\x80\\x99ll benefit more from the lower latency and higher clocked memory now, whereas the extra cores of the R7 1700 may never prove useful for the life of the CPU. However, as we mentioned earlier, if you can put those two extra cores to work on productivity apps, then the Ryzen 7 1700 could prove more attractive... How much more attractive? Itâ\\x80\\x99s not massively faster and will require some tinkering to really pull noticeably ahead of a stock 2600X.For those of you not interested in overclocking, the Ryzen 7 1700 is just 4% faster out of the box in Blender. So for content creators the 2600X might actually prove to be a better choice as its clock speed advantage and lower latency memory will make it better for editing. Even when taking on rendering tasks the R7 1700 was just 6% faster, as seen in Corona, so itâ\\x80\\x99s not like the 2600X is getting blown out of the water, despite having only six cores.If you require further proof here are some Cinebench R15 results. The 1700 is only 3% faster when comparing multi-threaded performance. However itâ\\x80\\x99s important to note that for single-threaded or even lightly threaded workloads the 1700 will be around 15% slower, and this was seen in our gaming benchmarks.You probably noticed the Ryzen 7 1800X results in the previous few application graphs and it was up to 20% faster than the 2600X. That is achievable with the R7 1700 through overclocking. The Ryzen 7 1700 has quite a bit of overclocking headroom unless you get a dud chip, but based on our purchases -- and we have bought quite a few of these -- your chances of getting a dud are slim.If youâ\\x80\\x99re not interested in overclocking and you have your choice of either CPU at the same price, weâ\\x80\\x99d get the Ryzen 5 2600X every time. The Zen+ refinements were not game-changing but the latency improvements are there and overall the 2600X is the slightly faster CPU. Frankly though, you wonâ\\x80\\x99t go wrong either way when the alternative is getting an 8-core 16-thread CPU for $160.With Zen 2 just around the corner itâ\\x80\\x99s a difficult choice for those wanting to upgrade now. Do you hold out a little longer or snap up a dirt cheap first or second-gen Ryzen processor now? We leave that decision up to you.\"],\n",
       "  [\"The latest member of the Turing GTX family is making its debut in the form of the Nvidia GeForce GTX 1660. An anticipated release after the recent launch of the GTX 1660 Ti, at $280 this mid-range GPU proved to be a great buy. Now the vanilla GTX 1660 has been set at $220, and at least on paper it looks to provide great value for your money.The standard 1660 is 21% cheaper but packs only 8% fewer CUDA cores. Going off by that alone youâ\\x80\\x99d expect it to be ~5% slower on average, as the base and boost clocks are very similar. However, thereâ\\x80\\x99s one other major change. Whereas the 1660 Ti features a 192-bit wide memory bus using 12Gbps GDDR6 memory for a bandwidth of 288 GB/s, the new vanilla model has been downgraded with GDDR5 memory.It still sports a 192-bit memory bus, but the slower GDDR5 memory means the bandwidth has been reduced by 33% down to 192 GB/s. Thatâ\\x80\\x99s going to produce some interesting results and we donâ\\x80\\x99t expect the GTX 1660 and 1660 Ti to scale evenly.Overall Nvidia says the GTX 1660 is 30% faster than the GTX 1060 3GB and 15% faster than the GTX 1060 6GB at 1080p, a claim weâ\\x80\\x99ll certainly put to the test today.Nvidia also told us that on average the GTX 1660 is 68% faster than the GTX 970 and 113% faster than the GTX 960. Prospective upgrades for those coming from older GPUs is one of the primary targets at this price point. As long as you have an 8-pin PCIe power connector youâ\\x80\\x99re good to go, though you can get 4-pin molex adapters, too.On hand for testing we have the MSI GTX 1660 Gaming X. This comes handy because our 1660 Ti results are based on the same MSI Gaming X model, and the same is true for the RTX 2060. Shout out to MSI for looking after us on these releases. Truth be told without their support this day-one content wouldnâ\\x80\\x99t be possible. On that same note, Nvidia has also been helpful with these recent Turing GTX releases, promptly providing the necessary drivers so we can get on with testing.For this review weâ\\x80\\x99ll be looking at performance in a dozen gaming titles, then sometime next week weâ\\x80\\x99ll follow up with one of our mega benchmarks comprising over 30 games.Before we begin, a few quick notes on our test system. We tested with a Core i9-9900K clocked at 5 GHz with 32GB of DDR4-3200 memory. We used using Adrenalin 2019 Edition 19.2.3 drivers for the Radeon GPUs and Game Ready 419.35 WHQL for the GeForce GPUs. Our focus will be on 1080p performance.BenchmarksKickstarting the benchmarks we have Shadow of the Tomb Raider and here the new GTX 1660 performs exceptionally well. It blows the GTX 1060 6GB out of the water by almost 40% and itâ\\x80\\x99s just 4% slower than the GTX 1070 and 13% slower than the 1660 Ti.Not only that, but with 76 fps on average at 1080p using the highest graphics settings, it provided a super smooth and enjoyable gaming experience. Weâ\\x80\\x99re off to a good start.The GTX 1660 isnâ\\x80\\x99t quite as impressive in Forza Horizon 4, where it matched the RX 590 and was only slightly slower than the GTX 1070. When compared to the GTX 1060 6GB it was 16% faster which is not bad, but not great either.The GTX 1660 averages 65 fps in Just Cause 4 at 1080p, placing it roughly on par with the Radeon RX 590 and placing it 18% ahead than the GTX 1060 6GB in this title. It was ~11% slower than the 1660 Ti, so overall not a bad result.Testing with Resident Evil 2 we find another title where the GTX 1660 is on par with the RX 590, though in this instance it is almost 30% faster than the GTX 1060 6GB, a substantial improvement.At 94 fps on average at 1080p with the game maxed out, it's nothing to sneeze at.Hitman 2 is a CPU limited game at 1080p when using the GTX 1660 Ti or better. We see the 1660 isnâ\\x80\\x99t quite at the limits of the CPU but even so itâ\\x80\\x99s still 16% faster than the RX 590 and 29% faster than the GTX 1060 6GB. Hitman 2 is a somewhat odd title, but the 1660 performs well here regardless.The GTX 1660 does OK in Fortnite, not amazing but it certainly gets the job done. It was 11% faster than the GTX 1060 6GB, 16% slower than the GTX 1070, and 18% slower than the GTX 1660 Ti. With 113 fps on average using the 'Epic' quality preset at 1080p, itâ\\x80\\x99s not what youâ\\x80\\x99d call slow.Performance in Metro Exodus is good, here the GTX 1660 was only 8% slower than the Ti model. It was 16% faster than the GTX 1060 and that meant the difference between nice smooth performance and noticeably lower frame rates.Weâ\\x80\\x99ve seen it before that Turing loves Rainbow Six Siege, or Rainbow Six Siege loves Turing. Either way, itâ\\x80\\x99s good news for prospective GTX 1660 owners. Here the new budget GPU spat out an incredible 118 fps on average at 1080p using 100% render scale. Thatâ\\x80\\x99s a whopping 36% performance boost over the GTX 1060 6GB.Testing with Battlefield V sees the 1660 trailing the Ti model by a 16% margin, and it only managed to match the RX 580. Radeon GPUs do perform very well in this title. When compared to the GTX 1060 6GB weâ\\x80\\x99re looking at a smaller 12% performance uplift.The new GPU doesn't do so well in World of Tanks. Here the 1660 is down 17% on the 1660 Ti, putting it essentially on par with the GTX 1060 6GB.Testing with Apex Legends shows the GTX 1660 providing a nice 28% performance boost over the 1060 6GB with a noteworthy 106 fps on average at 1080p. The card was 6% slower than the GTX 1070 and 14% slower than the GTX 1660 Ti which are relatively narrow margins.When testing with Far Cry New Dawn the GTX 1660 once again matched the RX 590. With 87 fps on average, performance at 1080p was excellent though admittedly it was just 14% faster than the 1060 6GB. The older Pascal GPU spit out 76 fps on average, so the difference wasnâ\\x80\\x99t that noticeable.Power and OverclockingWhen it comes to power consumption we can observe the new GTX 1660 is extremely fuel efficient, not unlike its Ti sibling. The card pushed total system consumption to the same levels as the already very efficient GTX 1060 6GB, doing so while delivering more performance. In terms of efficiency, thereâ\\x80\\x99s no question about it, the GTX 1660 is excellent.As for temperatures, the MSI GTX 1660 Gaming X ran cool and quiet. Under load we couldnâ\\x80\\x99t hear it above the test system and after an hour of the F1 2018 benchmark on a loop it peaked at just 63 degrees in a 21 degree room. Impressive stuff from this new budget graphics card.Overclocking the MSI GTX 1660 Gaming X went smoothly. Firing up the MSI Afterburner utility we arrived at +110 for the core and +900 for the GDDR5 memory. This resulted in a typical operating clock speed of 2070 MHz, though thatâ\\x80\\x99s a mere 5% bump over what you get out of the box.By increasing the GDDR5 memory speed by 22% this helped to boost frame rates by 11% in Apex Legends. Not bad considering how small the core overclock was. Clearly the GTX 1660 is memory limited and squeezing the most you can out of the GDDR5 is an easy way to boost frame rates.Cost per Frame and Closing RemarksOverall Nvidiaâ\\x80\\x99s new $220 mid-range Turing offering is an all-round excellent option. We can get a better idea of just how good the GTX 1660 in terms of value by looking at the cost per frame graph below. At a cost of $2.44 per frame based on our testing, itâ\\x80\\x99s actually 9% better value than the GTX 1660 Ti overall and impressively it even managed to edge out the Radeon RX 580.This is a really good value offering from Nvidia that completely wipes out the GTX 1060 series along with the Radeon RX 580 and 590, though the GTX 1660 Ti has already taken good care of the never-good RX 590.This leaves AMDâ\\x80\\x99s $140 RX 570 as the cost per frame champ, but as you can see the GTX 1660 was 36% faster on average, so paying a bit more for all that extra performance will likely be worth it for most.Bang for your buck the GTX 1660 canâ\\x80\\x99t be beat. The power efficiency is there, and the MSI Gaming X model we used for testing worked like a charm, running cool and quiet with a reasonable amount of overclocking headroom on offer.By the time you read this we hope enough cards show up in stores shortly afterwards, so if you're ready to take the plunge you can do so at the suggested $220 retail pricing.\"],\n",
       "  [\"Today we are reviewing the Gigabyte Aero 15 X9, the first Nvidia RTX laptop we tested and used for our RTX 2070 Max-Q feature earlier this month. Itâ\\x80\\x99s a cool gaming laptop, pretty similar to the Aero 15X v8 we looked at last year, but with a few upgrades that weâ\\x80\\x99ll walk you through here.In terms of hardware, a lot has carried over from the previous generation. The CPU is still Intelâ\\x80\\x99s Core i7-8750H for most models, with an i9-8950HK option available. The display is either a 144 Hz 1080p IPS, or a wide-gamut 4K IPS at 60 Hz, at 15.6-inches in size. Memory ranges from 8GB up to 32GB depending on the configuration, though 16GB models come with single-channel memory. The battery remains 94 Wh, in what appears to be the same chassis.So what are the changes? The big one is the upgrade from a GeForce GTX 1070 Max-Q, to the new RTX 2070 Max-Q. Weâ\\x80\\x99re also seeing a switch from Toshiba NVMe PCIe SSDs to Intel SSDs, which in combination with Killer networking that uses Intelâ\\x80\\x99s chips, makes this Gigabyte laptop all Intel inside.There are a few upgrades on the software side, too. Thereâ\\x80\\x99s a new Gigabyte Command Center application plus some unique AI functionality powered by Microsoftâ\\x80\\x99s Azure AI cloud services. Also worth pointing out, thereâ\\x80\\x99s an Aero 15 Y9 model that bumps you up to an RTX 2080 Max-Q GPU with otherwise the same hardware.Asking price starts at $2,400 for the regular model with the RTX 2070 Max-Q, 144Hz 1080p display, 16GB of RAM and 1TB SSD. The unit we reviewed is the mid-tier X9, which features the 4K display and 32GB of RAM, bumping up the price to $2,700.Gigabyte has been using a similar design for the past couple of gaming laptop generations, and we think it largely holds up today. This design was one of the first to use slim bezels around the display, now thatâ\\x80\\x99s a much more common feature, and it still looks good here. The downside is the webcam is still sitting below the screen. Newer slim bezel designs have fixed this problem, either through a small increase in the top bezel or including a camera bump. Hopefully Gigabyte goes with this approach next time, though if you're anything like me, I almost never use the webcam, so it's not a dealbreaker.The laptop is solidly built, using a combination of metal and plastic. Thereâ\\x80\\x99s not a lot of gamer flare which is nice, making it suitable for both gaming laptop or as a workstation. There are still a few seams around the place that make it look less sleek than something like a Razer Blade or MSI GS Stealth; Gigabyteâ\\x80\\x99s design, while not bad by any means, is more of a functional build than for pure aesthetics.This is also reflected in the size and weight. 18.9mm thick and 4.4 lbs heavy puts it in the slim and light gaming notebook category, but itâ\\x80\\x99s not challenging for any records.Those who want something thatâ\\x80\\x99s not chunky will be happy with the Aero 15X, although chunky laptops tend to be a lot cheaper for the hardware you get. Portability always comes at a price.I/O is very good: two USB-C ports, one Thunderbolt 3 Type-C and the other USB 3.1. Thereâ\\x80\\x99s a further three USB 3.1 ports, two gen-1 and one gen-2. Then you get Ethernet, HDMI 2.0, a 3.5mm audio jack, and an SD card reader. No need for dongles with this machine, it has basically everything.The keyboard still has a nice, reasonably clicky tactile response with per-key RGB backlighting controlled through Gigabyteâ\\x80\\x99s Fusion software. We appreciate the numpad, although the truncated right shift to fit in the arrow keys is a bit annoying. Weâ\\x80\\x99re also very happy to report the crappy ELAN trackpad used in previous models has been ditched for a better, more responsive, high precision model. Itâ\\x80\\x99s a big upgrade.In terms of performance, thereâ\\x80\\x99s not a lot to say in this review that we havenâ\\x80\\x99t covered previously, though we have still put together a performance overview below.For productivity, the Core i7-8750H with 32GB of RAM performs exactly as expected (read our full review). In most productivity workloads, the new GPU doesnâ\\x80\\x99t come into play, so youâ\\x80\\x99ll get the standard productivity performance weâ\\x80\\x99ve seen from these sorts of laptops for about a year now.Comparing the new Aero 15 X9 to the older Aero 15X v8, performance is roughly equivalent. There were a few workloads like Cinebench where the new model is a little slower. There were a handful of workloads like Handbrake where performance is on par. And there were a few tasks where the new model is faster.When breaking down these faster workloads, we have two factors at play: the Aero 15 X9 we reviewed has dual-channel DDR4-2666 memory, which provides higher bandwidth than the single-channel configuration in the Aero 15X v8 with 16GB of RAM. For tasks like 7-Zip and MATLAB, thereâ\\x80\\x99s a performance advantage because to this.Then we also see some improvements in GPU-accelerated applications. Premiere doesnâ\\x80\\x99t benefit significantly from the faster RTX 2070 Max-Q because the GPU is basically doing all it can here to accelerate rendering. Around the level of the GTX 1070 Max-Q thereâ\\x80\\x99s not much else to be gained from extra GPU performance in our test. Blender does see a large improvement, to the tune of 26 percent, in this workload run entirely on the GPU.The good news is depending on your workload, you can expect either the same performance as other i7-8750H systems, or perhaps a little better thanks to the faster GPU. That said, the 16GB Aero 15 X9 is still single-channel, so in those memory bandwidth limited tasks it wonâ\\x80\\x99t be as fast as we're showing here.Itâ\\x80\\x99s worth pointing out that the Aero 15 X9 has several different performance profiles. Aside from manual controls thereâ\\x80\\x99s quiet, normal and gaming fan profiles. In addition we have the AI features, of which there are two modes. The green local mode analyses what you are doing on your system locally and attempts to find the right performance and fan profiles for the task at hand, while the red cloud mode moves that AI work to the cloud for potentially better results.However for CPU-limited workloads, whether short or long, we found that as long as you are using at least the Normal fan mode, there is no difference in performance. The AI modes didnâ\\x80\\x99t do much to enhance the performance. For gaming, the AI modes are an improvement upon the Normal fan mode, especially the cloud mode. However using either the Gaming mode, or setting fan speeds to maximum, provides better performance again. For our gaming tests which weâ\\x80\\x99ll get to in a moment, we used the Gaming mode.We think the AI modes have promise especially if they can learn over time as to what modes deliver the best performance. This is still early days for the Aero 15 X9 and this system. Itâ\\x80\\x99d be cool if it delivered optimal performance while also tuning the fan speeds, because the Gaming mode is quite loud in general, but at least for now sticking to the Gaming mode while in GPU-heavy tasks is the best choice.A few other quick productivity comparisons before moving into gaming performance. If youâ\\x80\\x99re upgrading from a Core i7-7700HQ laptop or another one of Intelâ\\x80\\x99s quad-core 45W notebook CPUs, the six-core Core i7-8750H delivers about 35% more performance on average, though in heavily multi-threaded tasks gains can be up around 50%. If you do a lot of rendering work on the go, itâ\\x80\\x99s definitely worth the upgrade to access those two extra cores.And then if youâ\\x80\\x99re considering whether itâ\\x80\\x99s worth buying a performance-oriented portable laptop over a 15W ultraportable, hereâ\\x80\\x99s how the i7-8750H stacks up against Intelâ\\x80\\x99s latest Whiskey Lake Core i7-8565U.In CPU workloads youâ\\x80\\x99re looking at double the performance for encoding, and when factoring in the significantly more powerful GPU it becomes a bloodbath. For any serious creative work on the go, a configuration like the Aero 15 X9 with a six-core CPU and discrete GPU is definitely the best option.Here's the data in a nutshell, though... the new Aero 15 X9 is 11% faster on average than the Aero 15X v8 with the GTX 1070 Max-Q, and this margin is typical when comparing the Aero 15 X9 to other RTX 1070 Max-Q laptops.Do note this is the margin youâ\\x80\\x99ll see when comparing equivalent memory configurations; the version of this laptop with 16GB of RAM is only single-channel, so those upgrading from a dual-channel laptop will see smaller margins.However thereâ\\x80\\x99s not much to be gained moving from a fully-fledged GTX 1070 to an RTX 2070 Max-Q, as performance is roughly equivalent. Itâ\\x80\\x99s nice that new laptops include more performance in the same form factor, but the increase isnâ\\x80\\x99t massive.The GTX 1080 Max-Q is still faster than the RTX 2070 Max-Q if you are tossing up between those two GPUs. On the other hand, the RTX 2070 Max-Q smokes the GTX 1060 6GB to the tune of 35% on average, making this new GPU is a worthwhile upgrade. And finally, please donâ\\x80\\x99t think that an RTX 2070 laptop will provide the same performance as a desktop RTX 2070, Nvidiaâ\\x80\\x99s new Turing GPUs have widened the desktop-laptop GPU gap compared to Pascal, so the RTX 2070 for laptops is considerably slower than the desktop equivalent.On paper you get RTX features with the 2070 Max-Q, however performance is not sufficient to make it worth your while. In Battlefield V youâ\\x80\\x99re looking at an experience below 60 FPS at 1080p with the Low DXR mode, compared to well over 80 FPS with ray tracing disabled. DLSS is not an effective feature either, as weâ\\x80\\x99ve talked about separately. So our opinion is to treat RTX as more of a bonus than a key feature because support for it simply isnâ\\x80\\x99t wide enough or good enough right now to make it a â\\x80\\x98must haveâ\\x80\\x99 inclusion.While the Gigabyte Aero 15 performs well for its hardware, the cooling solution isnâ\\x80\\x99t that great. To deliver top-end performance in line with other systems, the gaming fan mode should be used, which really ramps up the fans to a high degree. This mode is loud, at 48 dBA, which keeps the Aero 15 on the bottom of the chart as one of the loudest notebooks weâ\\x80\\x99ve tested. This is an area the AI mode could really help, except it simply does not perform as well as the loud gaming mode.In better news, noise levels while using the laptop for productivity tasks is a lot better. Itâ\\x80\\x99s still not a quiet system, at a touch under 40 dBA during a Handbrake encode, but this is mid-tier among gaming laptops. This is using the normal fan mode, as the gaming mode saw no additional performance increases for CPU-limited apps.Thermally the laptop gets quite hot, hotter than the previous Aero 15X v8. CPU temperatures are unchanged but high for a gaming laptop, around 90C, although throttling wasnâ\\x80\\x99t a big concern. Thicker gaming laptops are closer to the 75 to 80C mark. However GPU temperatures with the RTX 2070 Max-Q have increased compared to the GTX 1070 Max-Q, now sitting around 86C compared to 79C.The chassis of the laptop also gets hot. The fans exhaust air through a fap below the display, however the position of these vents means the hot air is redirected in front of the display, towards the keyboard. Most other laptops vent out the sides and back, either behind the display or onto the desk. In venting forwards, the top edge of the laptop gets very hot, up over 50C during gaming in the center near the power button. Directly below this, the keyboard can be over 40C. Luckily the WASD keys are cooled well, around the 30C mark, but that isnâ\\x80\\x99t true for the entire keyboard.The average thermal performance surprises us considering what the Aero 15 looks like internally. We guess there are just some inefficiencies in the design. The laptop is quite upgradeable when removing the bottom cover though, revealing easy access to the memory slots and a second, free M.2 slot for storage upgrades.Our Aero 15 X9 review unit came equipped with the 4K 60Hz IPS display option, rather than the 144 Hz 1080p option we reviewed in the older model. The 4K option is the route youâ\\x80\\x99d take if you want to use this laptop for creative workloads or content creation, as 4K is too intensive for the RTX 2070 Max-Q for gaming. Instead, gamers are be better off with the high-refresh 1080p screen, especially as the GPU can handle many games around the 100 FPS mark on Ultra settings at this resolution.While this display is supposedly X-Rite Pantone certified, we have a few issues with the 4K panel and its calibration. This is a wide gamut monitor targeting 100% Adobe RB and it comes with what is supposed to be a factory calibrated profile installed. However the profile has been generated incorrectly. It looks like Gigabyte has mapped the displayâ\\x80\\x99s wide color primaries as the sRGB color primaries, which leads to oversaturation and other weird behaviour. We noticed this oversaturation right away, everything is extremely vibrant on this panel. Proper calibration should allow both sRGB and wide gamut content to be mapped properly with just a single profile.This is disappointing for those that might have wanted to use the display straight out of the box, however it could also be an issue with our early review unit. On the upside, as the display is more than capable of 100% Adobe RGB coverage, calibrating it properly will lead to good results. We also recorded a near 1400:1 contrast ratio which is strong for an IPS panel, along with peak brightness of 360 nits and very good viewing angles. The panel quality is there, itâ\\x80\\x99s just the calibration thatâ\\x80\\x99s letting it down.Final two bits of performance: the Intel SSD 760p used in the Aero 15 X9 is extremely fast, one of the fastest SSDs available in modern gaming laptops. Thatâ\\x80\\x99s a nice bonus. Then for battery life, it performs similarly to other slim and light laptops, if a little lower due to the 4K display.Overall, the Gigabyte Aero 15 X9 isnâ\\x80\\x99t a huge improvement over last yearâ\\x80\\x99s model, but it does pack some nice upgrades. While Nvidiaâ\\x80\\x99s RTX features donâ\\x80\\x99t provide much in this GPU design, the RTX 2070 Max-Q is ~10% faster for gaming at 1080p, which youâ\\x80\\x99d take every time considering thereâ\\x80\\x99s no change to the form factor. The improved trackpad is a long overdue addition. And some of the new software tools arenâ\\x80\\x99t game changers, but could be promising if more work is put into them.These changes have been made without sacrificing other elements that already made this laptop great. Itâ\\x80\\x99s still a well-built, portable design with a huge battery. The Core i7-8750H performs well, making the laptop great for productivity. Thereâ\\x80\\x99s still a 1080p 144Hz display option for gamers, and when calibrated properly the 4K screen option is great for creators. Thereâ\\x80\\x99s plenty of I/O and easy internal upgradeability for both the RAM and storage. The per-key RGB backlight keyboard is still one of the best in its class.With that said, two of our primary concerns with this laptop design havenâ\\x80\\x99t been addressed. The cooler is still loud and it runs hot to deliver performance equivalent to other systems. The webcam position is also not great, a by-product of this early bezel-free design.Whether the Aero 15 X9 is a good purchase comes down to what price you can get one at. Right now, for a decent hardware loadout youâ\\x80\\x99re looking at $2,400. For the GPU upgrade along over the 1070 Max-Q is not worth the premium, if you can find equivalent thin and light laptops for under $2,000 with nearly identical specs otherwise.But these GTX 1070 Max-Q laptops wonâ\\x80\\x99t be around for much longer... Right now, there are only a few thin and light gaming machines that use the RTX 2070 Max-Q: besides the Gigabyte Aero there's the MSI GS65 Stealth and the Razer Blade 15.The MSI option is selling for the same price, and while we havenâ\\x80\\x99t tested the new RTX version yet, last year we had a slight preference for it over the Aero. The new Blade 15 is $200 more expensive and packs a smaller SSD, while the overall aesthetics are arguably nicer.But as is always the case with these portable gaming machines, it will depend on how much you are willing to spend for the portability factor. For pure performance, there are faster options available in thicker, heavier chassis. Both Asus and Gigabyte offer the full RTX 2070 GPU in their thicker chassis for $2,000. Thatâ\\x80\\x99s a better value option if you donâ\\x80\\x99t see yourself moving the laptop around very much.\"],\n",
       "  ['Itâ\\x80\\x99s time for another mega benchmark and the subject of today\\'s GPU onslaught is Resident Evil 2. A classic survival horror game developed and published by Capcom that itâ\\x80\\x99s also a remake of the original Resident Evil 2 released for the PlayStation way back in 1998.The game has been built upon Capcomâ\\x80\\x99s RE Engine, which was originally built for Resident Evil 7: Biohazard, but has since been used for Devil May Cry 5 as well. Fun fact: the \"RE\" stands for the first two letters of the engine\\'s full name, \"Reach for the moon.\" Told you it was fun.Before the RE Engine, Resident Evil titles used the MT Framework engine. This includes 2012â\\x80\\x99s Resident Evil 6. The RE Engine is much improved and offers a variety of new graphical methods such as dynamic shadows, shadow cache, subsurface scatters and FXAA + TAA. The updated rendering techniques include HDR, a VR specific mode, the ability to output 4K resolution, among others.With those technical details out of the way, letâ\\x80\\x99s get to it. We won\\'t be reviewing the actual gameplay here, but here\\'s a handy review roundup on how it plays. What we will show you here is how a massive range of graphics cards perform in this title, so you know what youâ\\x80\\x99ll need for playable performance at 1080p, 1440p and 4K using the max graphics quality preset. Weâ\\x80\\x99ll also run additional testing with a mid-range preset at 1080p to see what you can get away with.Resident Evil 2 supports both DirectX 11 and 12, however weâ\\x80\\x99ve used the older DX11 API for all the testing because as usual DX12 is a complete and utter mess, plagued by low frame rates and constant stuttering. GeForce RTX cards along with AMDâ\\x80\\x99s Vega models were both over 30% faster when using DX11.For the test weâ\\x80\\x99re recording a 60 second pass in the Police station. From the initial checkpoint we walk up the left staircase, the back down the opposite side and under the roller door, the test ends during a cutscene. For the bulk of the testing the \"Max\" preset has been used which recommends 14 GBs of VRAM which is pretty insane and seems a tad exaggerated given the RTX 2080 Ti only saw an allocation peak of 8GBs when gaming at 4K. Thatâ\\x80\\x99s still very high relative to other titles, but not close to the suggested requirements.When using the \"Balanced\" profile we re-tested some of the newer graphics cards along with much older GPUs. This preset suggests VRAM usage to hit 2.4 GB which is much more manageable for the lower-end cards. As for drivers, weâ\\x80\\x99ve tested with AMDâ\\x80\\x99s Adrenalin 2019 Edition 19.1.2 driver and Nvidiaâ\\x80\\x99s GeForce Game Ready 417.71 driver...BenchmarksStarting with 1080p, here we have all the current and previous generation GPUs. Technically we have 3 generations from Nvidia now, but the point is we have a lot of squashed blue bars so letâ\\x80\\x99s stretch this out and discuss the results as we scroll down to the dreaded unplayable zone.Even with the maximum quality preset enabled the RTX 2080 Ti blitzed this test spitting out an incredible 217 fps on average with a 1% low of 181 fps and a 0.1% low of 133 fps. The Pascal Titan X, GTX 1080 Ti and RTX 2080 all provided very similar results, needless to say they enabled extremely playable performance.Then we see a reasonably large step down to the RTX 2070 and Vega 64, at least when looking at the average frame rate. It should be noted that Vega 56 does incredibly well, beating not only the GTX 1080 but also the new RTX 2060. It slippped behind ever so slightly for the 0.1% result, but was just ahead for the 1% low and average frame rate.From this point on AMD does very well... The RX 590 was able to match the GTX 1070 and this placed it on par with the GTX 980 Ti. Then we see the RX 580 smashing the GTX 1060 6GB by a convincing 26% margin, placing them on a category apart in this title.AMD\\'s previous-gen Fury range didnâ\\x80\\x99t fare as well, and this is down to that limited 4GB VRAM buffer, rendering them unplayable. It was interesting to see the 4GB versions of the RX 580 and 570 fairing much better though. Weâ\\x80\\x99re putting this down to memory optimizations for the GCN 4th gen architecture.Moving down further, we see the GTX 980, GTX 1060 6GB and Radeon R9 390 all producing similar performance at around 70 fps. Then to our surprise, the 3GB 1060 was able to provide playable performance, given its limited VRAM buffer size, and in particular given what we saw from the 4GB AMD cards. I was expecting a Powerpoint presentation here but it was playable. Surely, the 0.1% low performance suffered but the experience wasnâ\\x80\\x99t overly choppy. A similar experience was seen with the GTX 970 and once we drop below that the experience really started to suffer.Increasing the resolution to 1440p sees quite a few GPU models fall below the 60 fps barrier. Many of the entry level GPUs were too slow to get here, so theyâ\\x80\\x99ve been dropped from the chart.As expected the RTX 2080 Ti has no issue at 1440p and the same is true for the non-Ti model along with the GTX 1080 Ti and Titan X. Itâ\\x80\\x99s also interesting to note that the GTX 1080 Tiâ\\x80\\x99s slightly larger VRAM buffer didnâ\\x80\\x99t give it an advantage over the RTX 2080 despite the game allocating around 7 - 7.5GB/s of memory at 1440p, basically placing the 2080 right on the edge here.While the game was often allocating more than 6GB of video memory, the RTX 2060 had no issues keeping up and as was the case at 1080p, remained just 11% slower than the RTX 2070 at 1440p.AMDâ\\x80\\x99s Vega GPUs do very well again. Vega 64 hung in there with the RTX 2070, while Vega 56 topped the GTX 1080 and RTX 2060, at least for the average frame rate. Frame time performance was slightly down, but nothing too alarming.The RX 590 was able to keep pace with the GTX 1070 again, while the RX 580 was 26% faster than the GTX 1060 6GB. Older Fury GPUs continued to struggle due to its limited 4GB VRAM buffer, though GCN 4th gen models with only 4GB of memory fared better again.The GTX 1060 6GB was right on the edge for playable performance, while older Maxwell GPUs, the GTX 980 and 970 were unable to deliver playable performance at 1440p with visual settings maxed out.At 4K resolution we have fewer survivors. The RTX 2080 Ti was capable of providing smooth playable performance. What we got from the RTX 2080, GTX 1080 Ti and Titan X was also very nice. The RTX 2070 and Vega 64 werenâ\\x80\\x99t fast enough, though you could likely reduce a few settings and get them up to an acceptable level of performance. Beyond that, youâ\\x80\\x99re better off gaming at 1440p.Taking It Down a NotchWrapping up the testing, we grabbed a few mid-range graphics cards as well as some older bangers. Loaded up whatâ\\x80\\x99s called the \"Balanced\" preset at 1080p and ran a few more tests. This boosted the performance of the Radeon RX 570 by almost 30%.The truly huge gains were seen with the 2GB and 3GB cards. The GTX 1050 for example saw over an 80% performance boost with the balanced preset.The balanced preset suggests 2.4 GB of VRAM will be used, though that seems like an aggressive estimate as the 2GB models worked just fine.For 60 fps on average, something like the Radeon HD 7950 will do. From Nvidia you will require a GTX 780 Ti or GTX 970, so the requirements are steeper with the older GeForce cards. Older models such as the R9 270X or GTX 960 did enable a playable experience at 1080p using this preset, so not a bad deal there really.We noted issues with GeForce 600 & 700 series cards, basically any models based on the Kepler architecture. We specifically say Kepler because the GTX 750 Ti worked without issue and that\\'s one of the few GeForce 700 series model not to use that architecture, instead itâ\\x80\\x99s a Maxwell GPU. While the game did load up with a Kepler-based GPU, the game was very dark and unplayable, which would suggest it\\'s a driver-related issue with the game\\'s lighting. We\\'re sure if GeForce 600/700 owners make enough noise Nvidia will address the bug.Wrap UpFor those of you targeting 1080p gameplay with all the bells and whistles, youâ\\x80\\x99ll want a graphics card with at least 4GB of memory, though ideally 6GB+ is the way to go. From the newer GPUs youâ\\x80\\x99ll want at least a 6GB GTX 1060 or GTX 980 from Nvidia or an RX 570/R9 390 from AMD.As it\\'s often the case, the RX 570 4GB represented exceptional value, but for the max preset you will want to ensure you get an 8GB version. And while you\\'re already there maybe just get an 8GB RX 580. The 8GB RX 580 was the pound for pound champ at 1080p.For those gaming at 1440p there are a few good options. Vega 56 is certainly one of them, as is the new RTX 2060. That said if youâ\\x80\\x99ve got a Pascal GTX 1070, 1070 Ti or 1080 then youâ\\x80\\x99ll have no issues playing Resident Evil 2 at 1440p.As for 4K, no surprises here. Youâ\\x80\\x99ll ideally want an RTX 2080 Ti, but if your not drowning in coin then the GTX 1080 Ti or RTX 2080 will get the job done. Of course, you can always tweak the quality settings for better performance and tomorrow Tim will be doing just that on a HUB video, so thatâ\\x80\\x99ll be worth checking out.Resident Evil 2 looks great. Tell you the truth, it looks quite amazing. If youâ\\x80\\x99re into survival horror games then Iâ\\x80\\x99m confident youâ\\x80\\x99ll enjoy it. Steam\\'s user reviews are overwhelmingly positive, so it looks like we have one of those rare titles that delivers. Weâ\\x80\\x99ll be adding this one to our full time roster of games that we benchmark with and it even looks like it might be a decent CPU benchmark as well, though we\\'ll need to play it more to see where the demanding sections are. If you enjoyed this feature, please share it, and let us know what future game releases you would like us to benchmark.'],\n",
       "  ['Gaming BenchmarksDue to time and how much we already have to cover, weâ\\x80\\x99re going to show the gaming performance of just half a dozen titles. Starting with Assassinâ\\x80\\x99s Creed Odyssey the 9900K boasts frame time performance by just 4% at 1080p when compared to the 8700K while the 9700K was slightly better providing a 7% increase. Sometimes we see CPUs performing better with SMT disabled if they have more cores than the game requires.The Ryzen processors get mugged at 1080p when looking at the average frame rate, here the 9900K was 23% faster, though it was just 10% faster for the 1% low result. Again we are using an RTX 2080 Ti and we see when moving to the more appropriate 1440p resolution the margins start to really shrink.Here the new 8-core models offer basically no performance advantage over the 8700K. That said the performance advantage over Ryzen was still reasonably significant and itâ\\x80\\x99s not until we hit the 4K resolution that the margin is almost entirely eliminated. Still this isnâ\\x80\\x99t a very good title for Ryzen so letâ\\x80\\x99s move on to Star Wars Battlefront II.Here we see much more respectable performance from the Ryzen processors and even at 1080p the Intel Core i7 and i9 processors donâ\\x80\\x99t offer noticeable performance gains over the 2700X. The 9900K was just 5% faster than the 870K while the 9700K was 7% faster.Then at 1440p we see no real difference between the Intel processors and now Ryzen is less than 10% slower. Finally at the 4K resolution we see the playing field netrelized and now all 5 tested CPUs enabled the same 76 fps on average.Next up we have Forza Horizon 4 which isnâ\\x80\\x99t a particularly CPU demanding title so unsurprisingly all CPUs enabled a similar level of performance. This is a good example of how most games will behave, as most games are indeed GPU bound.Like Assassinâ\\x80\\x99s Creed Odyssey, Hitman is a title that plays a little oddly with Ryzen and this gave the Intel CPUs a serious performance advantage, particularly at 1080p and 1440p. The 9900K was also up to 12% faster than the 8700K which can be seen when comparing the 1% low results at 1080p.Rainbow Six Siege shows little to no performance gains for the 8-core 9th gen processors over the 8700K, even at 1080p. Meanwhile the Intel processors did offer a performance boost over the Ryzen 7 2700X, even at 1440p, though the gains wonâ\\x80\\x99t exactly be noticable.The last game weâ\\x80\\x99re going to look at is Shadow of the Tomb Raider and here the 2700X gets trounced at 1080p, limiting the RTX 2080 Ti to around 90 fps on average while the Intel CPUs pushed on to hit around 120 fps. That said for the most part the 9900K and 9700K offered no real performance improvement over the 8700K. The 9900K did allow for a 10% boost to the 1% low result at 1080p but other than that, not much to report here.I also took the time to test gaming performance with the 9900K and 9700K overclocked to 5.1 GHz. Interestingly we see no performance gain when testing with Assassinâ\\x80\\x99s Creed Odyssey, even at 1080p. Weâ\\x80\\x99ve found in the past overclocking the 8700K lead to almost no performance gains at 1080p with the 1080 Ti and the same appears true with the RTX 2080 Ti.Again testing with Star Wars Battlefront II shows no real advantage to overclocking these CPUs, a mere 3% performance boost is seen for the 9900K and even less for the 9700K.That said Hitman is a title that did see double digit performance gains from the overclock, here the 9th gen 8-core processors were up to 12% faster at 1080p and 1440p once overclocked.'],\n",
       "  [\"Nvidia unveiled RTX laptops early 2019 and the first models sporting the GPUs are making it to market just now. Today we're checking out Nvidia's new GeForce RTX GPUs for laptops, starting with the RTX 2070. Hopefully in the near future weâ\\x80\\x99ll get to do a full performance breakdown of each RTX laptop offering, but we have to start somewhere, and thanks to Gigabyte weâ\\x80\\x99re kicking things off with the middle of the pack RTX 2070 Max-Q variant.Of course, the desktop RTX 2070 has been available for a while and we know exactly how it performs. However the laptop version of the chip is not an identical copy of the desktop card, which may lead to some confusion considering how Pascal (GTX 10 series) and now the GeForce 20 series are different on the mobile front.With Pascal there were 3 main offerings: the GTX 1060, GTX 1070 and GTX 1080. Excluding the Max-Q variants for a moment, the main regular laptop versions were very similar to their desktop counterparts. The GTX 1060, for example, had the same CUDA core count and the base clock was a little lower, but the boost clock was only 38 MHz lower than the desktop card. For the most part, in a good laptop design, the mobile GTX 1060 performed like a desktop GTX 1060.Then there was the GTX 1080, again same CUDA core count and even the same boost clock. The GTX 1070 was a little different in that the laptop variant had a slight increase to CUDA cores along with a slight decrease in clocks, but the end result was that a laptop GTX 1070 performed around the mark of a desktop GTX 1070. Times were simple.Then Nvidia introduced Max-Q variants, which were underclocked versions of the same GPUs designed to slot into thin and light systems that perhaps didnâ\\x80\\x99t have the cooling capabilities of the usual chunky gaming laptops. Many didn't like Max-Q, but in the practical sense it allowed for more performance in slimmer designs, which we approve of. It wasn't quite the full performance of the labelled GPU, but better than the tier below, and considering most laptops that had Max-Q GPUs were marketed as such, there wasnâ\\x80\\x99t much confusion.With the GeForce 20 series, Nvidia has taken things a step further.We still have Max-Q laptops, but the regular non Max-Q GPUs have also received significant clock speed cuts compared to the desktop versions. We wonâ\\x80\\x99t go through every variant, but focusing on the RTX 2070 we can see where the cuts have occurred.The 2070 GPU has the same CUDA core count, 2304 cores, which is why Nvidia can call this an RTX 2070. It is the same physical GPU. But clock speeds are significantly lower. The desktop card has a 1410 MHz base along with a 1620 MHz boost, which goes up to 1710 MHz for the Founders Edition card. Compare that to the laptop variant, which receives a 1215 MHz base and a max boost clock of 1440 MHz. And thatâ\\x80\\x99s the fully fledged mobile version.The 2070 Max-Q model sits at an 885 MHz base clock and 1185 MHz boost clock. Max-Q clocks have always been lower than the non Max-Q models, but now the gap has widened significantly compared to the last few years.There's a reason to all of this, and it relates to the TDP. Not a lot of people care about this metric in desktop cards, but for laptops itâ\\x80\\x99s a crucial figure as the laptop maker has to design a cooler that can handle the TDP of the GPU within tight space constraints. With Pascal, we had the GTX 1070 at 150W on the desktop, and 115W for full laptops. But with the RTX 2070, thatâ\\x80\\x99s now a 175W part, but still just 115W for laptops. The only way to hit that TDP with the same GPU is to put it on a leash and lower clocks.This also relates to Nvidiaâ\\x80\\x99s shifting up of product tiers. The GTX 1060 used to be 120W, but with the RTX 2060 itâ\\x80\\x99s now 160W which occupies the place where the GTX 1070 used to sit. The RTX 2070 is 175W, which is more like a GTX 1080. And then the RTX 2080 comes in at 215W, a place where laptops never used to go.With this predicament Nvidia could have gone in two directions: they could have made RTX laptop GPUs equivalent to RTX desktop GPUs, which would have meant the RTX 2070 replaces the GTX 1080 in designs capable of handling that TDP. The RTX 2060 would then replace the GTX 1070, and the RTX 2080 couldnâ\\x80\\x99t be used. But Nvidia didnâ\\x80\\x99t want to do that.Instead they chose option two: downclock the GPUs so that an RTX 2070 can replace a GTX 1070 and so on, creating a simple swap in generations. Even though it makes the laptop versions pretty different to the desktop chips.Weâ\\x80\\x99ve probably spent too much time talking about all of this, but our final point on the matter is that with Nvidia choosing the downclocking route, we feel using the name RTX 2070 for the laptop RTX 2070 isnâ\\x80\\x99t accurate anymore, and we feel this series should have added an M at the end of the label as we saw on generations prior, so this should be the RTX 2070M.Only a few more notes worth adding. Like the desktop card, the RTX 2070 for laptops uses 8GB of GDDR6 at â\\x80\\x9cup to 14 Gbpsâ\\x80\\x9d according to Nvidia specs. That means 14 Gbps for the fully fledged version, and 12 Gbps for Max-Q. Same 256-bit bus. RTX functionality also remains, so the Tensor and RT cores are present, but due to the GPU running slower, performance won't be the same. Nvidia cites up to 38T RTX OPS and 5 Gigarays per second for the laptop RTX 2070, compared to 42T RTX OPS and 6 Gigarays for the reference RTX 2070 desktop card.The focus of todayâ\\x80\\x99s review is the Max-Q variant of the RTX 2070, which belongs to the first RTX laptop we've got our hands on. Evidently, Max-Q GPUs are most popular in the thin and light/gaming ultraportable laptop category. This Max-Q laptops ship with an unchangeable power limit and they cannot be overclocked to the level of a full RTX 2070. You can do some basic core and memory clock overclocking, but the power limit is the main limitation here.Thanks go to Gigabyte for sending in their brand new Aero 15 X9 which packs the RTX 2070 Max-Q. We also have on hand the previous gen Aero 15X with the GTX 1070 Max-Q inside, so we can run some apples-to-apples comparisons. Because both laptops use the same design, as well as largely the same hardware and cooler, this makes comparing the RTX 2070 to the GTX 1070 pretty easy.The Gigabyte Aero 15 X9 packs Intelâ\\x80\\x99s six-core Core i7-8750H, the go-to choice for gaming laptops, which youâ\\x80\\x99ll also see used on most of the other laptops in our comparison. Our review unit was kitted out with 32GB of dual-channel DDR4, the same configuration in the Aero 15X with the GTX 1070. The testing was focused on 1080p gaming.Itâ\\x80\\x99s also key to note the Aero 15 X9 was tested with the fan mode set to â\\x80\\x98gamingâ\\x80\\x99. This delivers noticeably better performance than the quieter â\\x80\\x98balancedâ\\x80\\x99 mode. Both laptops were as loud as each other, which again cuts out a further variable when comparing the RTX 2070 with the GTX 1070.In terms of clock speeds, while the RTX 2070 Max-Q has a rated boost clock of just 1185 MHz, thanks to Nvidiaâ\\x80\\x99s GPU Boost, actual clock speeds in games were more around the 1250 to 1350 MHz mark. So typically at least 100 MHz lower than the fully fledged laptop RTX 2070â\\x80\\x99s boost clock, though the full version will also boost higher. This is basically the same behaviour as the GTX 1070 Max-Q, which has a rated boost of 1379 MHz but sits more around the 1500 to 1600 MHz mark in games.You should expect these results from any laptop with decent cooling, however those with poor cooling designs will downclock more aggressively and result in worse performance. Memory clocks donâ\\x80\\x99t change, and here the GDDR6 with the Max-Q variant is stuck at 12 Gbps.BenchmarksWeâ\\x80\\x99re going to kick things off with a look at Battlefield 1. This has been superseded by Battlefield V these days but a lot of our existing data was for BF1, so weâ\\x80\\x99ll start with that. Here the RTX 2070 Max-Q clocks in at just 7% faster than the GTX 1070 Max-Q, while offering a slight performance gain on the non-Max-Q variant. This is a decent start, but ideally youâ\\x80\\x99d want to see larger gains here.We know Wolfenstein II is one of the most favorable games to Nvidiaâ\\x80\\x99s new Turing architecture, at least when comparing to Pascal. Here the RTX 2070 Max-Q commands a decent 15% performance lead over the GTX 1070 Max-Q. However the game is also one that heavily favors the fully fledged GPUs for some reason, so the regular old GTX 1070 is still faster when looking at average performance.Far Cry 5 is a well optimized title and one that we like to benchmark with extensively. In this game, you can expect the RTX 2070 to come in with a 9 percent performance lead over the GTX 1070, when comparing Max-Q versions. That does increase to a 15% lead in 1% lows though, which is nice to see. When comparing the Max-Q 2070 to the full GTX 1070, performance is about equal.In Star Wars Battlefront II, the RTX 2070 is around 10 percent faster than the GTX 1070, again comparing Max-Q versions. The newer Turing GPU also holds a small advantage over the full GTX 1070, which seems to be typical for this new Max-Q model. Weâ\\x80\\x99ve only gone through four games so far, but in those games, the usual result is the Max-Q 2070 delivering the same performance as the non Max-Q 1070, within the same design constraints, so the same coolerDirt 4 is one of the most favourable games to the RTX 2070 Max-Q, the new GPU is 18 percent faster on average and 11% faster in 1% lows, relative to the GTX 1070 Max-Q. Itâ\\x80\\x99s also a small amount faster than the standard GTX 1070.Not every game youâ\\x80\\x99ll play on a laptop will be GPU limited though. At 1080p, a great example of that is Prey, which runs at around 100 FPS on modern laptops but thereâ\\x80\\x99s not a lot of difference between most higher-end GPUs. The RTX 2070 Max-Q is only 3 percent faster than the GTX 1070 Max-Q, and it holds one of the smallest margins over the GTX 1060 6GB, at 16 percent. We believe this is more of a memory bandwidth issue than a CPU bottleneck.Middle-earth Shadow of War is another game that is favourable to Turing. Here the RTX 2070 is 16% faster on average compared to the GTX 1070, looking at Max-Q variants. Itâ\\x80\\x99s also 6% faster than the regular GTX 1070, which is one of the larger margins for this GPU.Watch Dogs 2 is in a similar situation to Prey, in that the game is quite demanding on the CPU. We spotted a 4% gain to average performance when comparing the RTX 2070 Max-Q to the GTX 1070 Max-Q, but no gain in terms of 1% low performance. Unlike with desktops, where you might pair the RTX 2070 with a much more powerful CPU, the 45W CPUs youâ\\x80\\x99re stuck with in most laptops can limit the GPU, especially at 1080p.Couple more games we wanted to focus on here, Deus Ex Mankind Divided is an older title but is still very GPU limited. The RTX 2070 Max-Q is 18 percent faster than the GTX 1070 Max-Q, a big gain for Nvidiaâ\\x80\\x99s new Turing architecture. Compared to the average result this is a bit of an outlier but nonetheless itâ\\x80\\x99s impressive.Finally we have a game that not many people liked, Mass Effect Andromeda. A decent performance gain for the RTX 2070 Max-Q, coming in 13% faster than the GTX 1070 Max-Q. Itâ\\x80\\x99s also 7% faster than the fully fledged GTX 1070.Putting It All TogetherLooking at the performance summary, across 18 games we can see that on average the RTX 2070 Max-Q is 10 percent faster than the GTX 1070 Max-Q it replaces. You can spot a couple of newer games, such as Assassinâ\\x80\\x99s Creed Odyssey which performs poorly on laptop hardware, along with another largely CPU limited game in Hitman 2, plus two other new titles that deliver good gains in Battlefield V and Resident Evil 2.In general though, you can expect performance gains up to 18 percent, but as low as 3-4 percent in games that arenâ\\x80\\x99t GPU limited. And as weâ\\x80\\x99re talking about laptops, there are more instances where youâ\\x80\\x99ll run into bottlenecks than a typical gaming desktop build, and this is an important consideration when comparing desktop and laptop GPUs...Comparing the RTX 2070 Max-Q to the regular GTX 1070 is a bit of a mixed bag. This new Max-Q GPU is now delivering the performance of the last-gen fully fledged model, but in smaller, thinner and lighter systems with weaker cooling solutions. While not always the case, GTX 1070 laptops tended to be on the larger end of the scale, however with the RTX 2070 Max-Q we can expect to see that GPU in true thin and light gaming machines.On the less positive side, the desktop RTX 2070 is 27% faster than the GTX 1070 on average. So itâ\\x80\\x99s pretty obvious how far behind the RTX 2070 Max-Q for laptops is, in comparison to the desktop equivalent.We expect the full non Max-Q model to be faster, but it wonâ\\x80\\x99t close this gap entirely. The Max-Q to Max-Q margin was 10%, so weâ\\x80\\x99d expect the full versions will have a similar 10% margin, not the 27% we saw with the desktop cards.A couple of other comparisons to go, we have the RTX 2070 Max-Q compared to the GTX 1060 6GB, a very popular laptop GPU and a GPU that used to be the maximum for thin and light laptops. The RTX 2070 Max-Q is 35% faster on average, which shows the progress that has been made in this sort of portable form factor.Of course, RTX 2070 Max-Q laptops are a lot more expensive, but purely from a technical standpoint itâ\\x80\\x99s impressive to see this level of performance gain within the same available space for cooling and power delivery.The RTX 2070 Max-Q is still slower than the GTX 1080 Max-Q, to the tune of ~6%. The GTX 1080 Max-Q is the next step above the GTX 1070, and with the RTX 2070 Max-Q performing around GTX 1070 level, it makes sense the GTX 1080 Max-Q is still faster.Of course, youâ\\x80\\x99ll be wondering how ray tracing performs with the RTX 2070 Max-Q. And the good news is thatâ\\x80¦ it works. We actually werenâ\\x80\\x99t expecting to see ray tracing in the laptop versions due to the TDP constraints but itâ\\x80\\x99s there, even with the Max-Q variants. However, the 2070 Max-Q is only rated for 4 gigarays per second, lower than the desktop RTX 2060, so naturally it wonâ\\x80\\x99t be as good for ray tracing.In our standard Battlefield V benchmark run, which does have reflected surfaces that activate the gameâ\\x80\\x99s DXR reflections, using just the low ray tracing mode caused a pretty significant performance drop. Without DXR we were achieving an average of 86 FPS through the run, turning on DXR to Low dropped that to 54 FPS, so weâ\\x80\\x99re talking about a sub 60 FPS experience at 1080p. Turning RTX up to ultra andâ\\x80¦ yeah that drops even further to just a 44 FPS average. We donâ\\x80\\x99t think many people will want to play a game like BFV at below 60 FPS, however we're also waiting to see how true Nvidia's DLSS claims turn out to be.Closing ThoughtsWhile Nvidiaâ\\x80\\x99s RTX line has been largely disappointing on the desktop, laptops are an entirely different story with different price points, categories and considerations to make.We want to break this down into a few sections. First, the naming scheme is not good enough. With Nvidia downclocking all RTX GPUs for laptops, these are no longer direct equivalents to their desktop counterparts and we wish they used different names, even if slightly, to avoid confusion.On the performance front, weâ\\x80\\x99re getting 10% more out of the RTX 2070 Max-Q compared to the GTX 1070 Max-Q. For the same product category â\\x80\\x93 which means the same type of laptop, with a similar design, size, weight and cooler â\\x80\\x93 we can expect a modest 10% gain. Thatâ\\x80\\x99s ok. Itâ\\x80\\x99s fine. Itâ\\x80\\x99s nothing amazing, but it is faster in the same category.The good news is laptops are around the same price as well. Comparing launch prices, portable GTX 1070 Max-Q laptops launched in the $2,200 to $2,400 range, and thatâ\\x80\\x99s where RTX 2070 Max-Q laptops have launched with similar hardware configurations. If you're looking for new laptop options right now, you will also find GTX 1070 Max-Q laptops discounted to an appropriate price relative to newer RTX 2070 models, so you can save $100-300 depending on the model.For some people, RTX GPUs like the RTX 2070 Max-Q will bring additional benefits like ray tracing and DLSS depending on how much stock you place on their importance. Neither are widespread technologies right now, and in our opinion itâ\\x80\\x99s not something worth considering with your purchase.While on the desktop itâ\\x80\\x99s all about raw performance and bang for buck, with laptops thereâ\\x80\\x99s the added factor of performance per watt, or how much you can get out of a given cooler design. As Turing is built on 12nm, compared to 16nm for Pascal, there hasnâ\\x80\\x99t been a significant uptick in that department, so realistically we were never going to see a large jump in performance within the same given form factor.Laptop upgrade cycles are also different to the desktop. Most people buying an RTX laptop arenâ\\x80\\x99t just getting a new GPU, theyâ\\x80\\x99ll likely be upgrading the CPU, memory, storage, display and design, which can all add value to a purchase. So while the generation on generation gains are only about 10% of raw performance for the RTX 2070 Max-Q, a large number of buyers will see actual gains far higher than this with their upgrade once you factor in other components and overall system performance.\"]],\n",
       " 'tokenized': [],\n",
       " 'words': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora['hardware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "for corpus, infos in corpora.items():\n",
    "    sentences = []\n",
    "    corpora[corpus]['words'] = []\n",
    "    words = []\n",
    "    \n",
    "    for text in infos['raw']:\n",
    "        for line in text:\n",
    "            tmp_sentences = sent_tokenizer.tokenize(line.strip())\n",
    "            tokens = [nltk.tokenize.word_tokenize(sentence) for sentence in tmp_sentences]\n",
    "            sentences.append(tokens)\n",
    "            for doc in tokens:\n",
    "                words.extend([w for w in doc])\n",
    "                \n",
    "    corpora[corpus]['tokenized'] = sentences\n",
    "    corpora[corpus]['words'].extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ReviewsWhen',\n",
       " 'we',\n",
       " 'recently',\n",
       " 'updated',\n",
       " 'our',\n",
       " 'Best',\n",
       " 'CPUs',\n",
       " 'feature',\n",
       " ',',\n",
       " 'we',\n",
       " 'noticed',\n",
       " 'that',\n",
       " 'access',\n",
       " 'to',\n",
       " 'affordable',\n",
       " 'first-gen',\n",
       " 'Ryzen',\n",
       " 'processors',\n",
       " 'remains',\n",
       " 'an',\n",
       " 'attractive',\n",
       " 'option',\n",
       " 'for',\n",
       " 'many',\n",
       " '.',\n",
       " 'The',\n",
       " 'Ryzen',\n",
       " '7',\n",
       " '1700',\n",
       " 'is',\n",
       " 'a',\n",
       " 'standout',\n",
       " 'option',\n",
       " 'for',\n",
       " 'an',\n",
       " '8-core/16-thread',\n",
       " 'part',\n",
       " 'selling',\n",
       " 'for',\n",
       " '$',\n",
       " '160',\n",
       " ',',\n",
       " 'meaning',\n",
       " 'you',\n",
       " 'can',\n",
       " 'either',\n",
       " 'buy',\n",
       " 'the',\n",
       " 'R7']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora['hardware']['words'][1:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
